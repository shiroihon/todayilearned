
      
      
      
    
      
      
      
    
      
      
      
    
      
      
      
    
      
      
      
    
      
      
      
    
      
      
      
    
      
      
      
    
      
      
      
    
      
      
      
    
      
      
      
    
      
      
      
    
      
      
      
    
      
      
      
    
      
      
      
    
      
      
      
    
      
      
      
    
      
      
      
    
      
      
      
    
      
      
      
    
      
      
      
    
      
      
      
    
      
      
      
    
      
      
      
    
      
      
      
    
      
      
      
    
      
      
      
    
      
      
      
    
      
      
      
    
      
      
      
    
      
      
      
    
      
      
      
    
      
      
      
    
      
      
      
    
      
      
      
    
      
      
      
    
      
      
      
    
      
      
      
    
      
      
      
    
      
      
      
    
      
      
      
    
      
      
      
    [{"authors":null,"categories":[""],"content":" Why SSIA\nWhat 概要\nTableauは製品単位ではなく、ライセンス単位で購入できる。 Tableau 製品: ◆[データ分析] -----\u0026gt; ◼︎[結果を共有] 集約/加工/分析 レポート/ダッシュボード 位置 製品名 役割 ◆ Tableau DeskTop\nTableau Prep Builder 集約加工分析\n- Desktop: 主流製品\n- Prep Builder: 集約加工に特化(ETL処理に適任) ◼︎ Tableau Online\nTableau Server\n(addon:\n-Tableau Prep Conductor\n-Tableau Catalog) 結果共有に利用\n- Online: Tableau社サーバー流用\n- Server: 自前でｲﾝｽﾀﾝｽを立ちあげ ◆ (無料) Tableau Public ビジネスシーン不向き(∵Tableau社サーバーで全世界公開) ◼︎ (無料) Tableau Reader ファイル閲覧用 Tableau ライセンス: できることと金額\nライセンス名 集約 加工 分析 共有 金額(単/年)\n[s]Server\n[o]Online Tableau Creator ● ● ● ● [s/o]¥100,000- Tableau Explorer - ● ● - [s]¥50,000-\n[o]¥60,000- Tableau Viewer - - - ● [s]¥18,000-\n[o]¥22,000- Tableau 環境別導入: [#] 環境 規模 備考 1 サーバレス分析\nTableau Reader 少人数 導入コストや参入障壁低め 2 サーバ利用分析（自前）\nTableau Server - 導入コスト高め、しかしカスタマイズ性良い 3 サーバ利用分析（Tableau社）\nTableau Onlien - 運用保守がないのが良い cf.\nhttps://www.youtube.com/watch?v=051QSL1vn8Q Tableau 各種機能: 主要アクション（6つ）\n[ワークシート] -(trigger)-\u0026gt; [1.移動シート] -(t)-\u0026gt; [2.移動URL] -(t)-\u0026gt; [3.フィルター] -(t)-\u0026gt; [4.ハイライト] -(t)-\u0026gt; [5.セット] -(t)-\u0026gt; [6.パラメータ] cf.\nhttps://www.youtube.com/watch?v=3VsOBN3PjZI Tableau 拡張子: .twb: ワークブック .twbx: パッケージドワークブック .tds: データソース .hyper/(.tde): データを抽出した時にできるファイル cf.\n【Tableau】ワークブックの構成 .twbと.twbxの違いは？ ATARA https://note.com/sana626/n/n0ba8ea2a50c7#da1d4a9e-1053-4c20-950c-2ebc763603a0 https://community.tableau.com/s/question/0D54T00000C5ZbqSAF/whats-the-diff-between-twb-and-twbx Tablaeu 勉強: cf.\nhttps://www.tableau.com/ja-jp/learn/training https://www.youtube.com/@dviz_data/videos ","date":1748843168,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1751615720,"objectID":"ff30ce3721fc35572a02f1d359837eda","permalink":"https://kithub.f5.si/post/tableau_this_and_that/","publishdate":"2025-06-02T14:46:08+09:00","relpermalink":"/post/tableau_this_and_that/","section":"post","summary":"✨A little while ago, I'll be writing about my studies as an introduction to Tableau","tags":["Tableau","BI"],"title":"[Tableau] This and That","type":"post"},{"authors":null,"categories":["Data Analytics Engineer","Google"],"content":" Why # --------------------------------------------------------------------------------------- # ETL/ELT pipeline # --------------------------------------------------------------------------------------- Orchestration [Cloud Composer(Airflow)] [Kestra] [Argo Workflows(Kubernetesネイティブなワークフローエンジン)] DataSource DataLake DWH DataMart [DataSource] ---*---------------\u0026gt; [GCS] ---*-------------\u0026gt; [BQ] ---*-----------\u0026gt; [BQ] └(API) [Trocco(Embluk)] [Dataflow] [Dataform] └(CloudLogging) [DataProc(Spark)] [Argo Workflows] [dbt] └(GA4) [Cloud Functions] └バッチをKubernetes上の場合 └...etc. [Cloud Run ] (大規模)データ分析パイプライン: パイプライン構成とフロー：\nCloud Scheduler で日次トリガー Cloud Composer がワークフロー開始 Cloud Run で前処理（データ検証、分割） Dataproc でSparkジョブ実行（メイン処理） Cloud Functions で処理完了通知 BigQuery に結果ロード この構成は各サービスの強みを活かしながら、\nコスト効率の良いパイプラインを構築 Dataprocは一時クラスタを使用することで、大規模処理が必要な時だけリソースを消費するように設計 コスト比較の目安：\nサービス コスト特性 適したワークロード Cloud Functions 実行時間とメモリ使用量に比例 軽量で頻度の高いイベント処理 Cloud Run リクエスト数とCPU使用時間 中規模HTTPサービス/バッチ Dataproc クラスタ規模と実行時間 大規模データ(TB～PB級)処理バッチ Cloud Composer 固定費+環境規模 ワークフローオーケストレーション Composer + Cloud Run + Dataproc：\n# Cloud ComposerでのDataprocオペレーター使用例 from airflow.providers.google.cloud.operators.dataproc import ( DataprocCreateClusterOperator, DataprocSubmitJobOperator, DataprocDeleteClusterOperator ) with models.DAG(\u0026#39;data_pipeline\u0026#39;, schedule_interval=\u0026#39;@daily\u0026#39;) as dag: create_cluster = DataprocCreateClusterOperator( task_id=\u0026#34;create_cluster\u0026#34;, project_id=\u0026#34;your-project\u0026#34;, cluster_name=\u0026#34;ephemeral-spark-cluster\u0026#34;, region=\u0026#34;us-central1\u0026#34;, cluster_config={ \u0026#34;master_config\u0026#34;: {\u0026#34;num_instances\u0026#34;: 1, \u0026#34;machine_type_uri\u0026#34;: \u0026#34;n1-standard-4\u0026#34;}, \u0026#34;worker_config\u0026#34;: {\u0026#34;num_instances\u0026#34;: 2, \u0026#34;machine_type_uri\u0026#34;: \u0026#34;n1-standard-4\u0026#34;} } ) # 重いデータ処理はDataprocに委譲 spark_job = DataprocSubmitJobOperator( task_id=\u0026#34;run_spark_job\u0026#34;, job={ \u0026#34;reference\u0026#34;: {\u0026#34;project_id\u0026#34;: \u0026#34;your-project\u0026#34;}, \u0026#34;placement\u0026#34;: {\u0026#34;cluster_name\u0026#34;: \u0026#34;ephemeral-spark-cluster\u0026#34;}, \u0026#34;spark_job\u0026#34;: { \u0026#34;jar_file_uris\u0026#34;: [\u0026#34;gs://your-bucket/your-spark-job.jar\u0026#34;], \u0026#34;main_class\u0026#34;: \u0026#34;com.example.Main\u0026#34; } } ) delete_cluster = DataprocDeleteClusterOperator( task_id=\u0026#34;delete_cluster\u0026#34;, project_id=\u0026#34;your-project\u0026#34;, cluster_name=\u0026#34;ephemeral-spark-cluster\u0026#34;, region=\u0026#34;us-central1\u0026#34;, trigger_rule=\u0026#34;all_done\u0026#34; ) create_cluster \u0026gt;\u0026gt; spark_job \u0026gt;\u0026gt; delete_cluster What [Cloud Composer(Airflow)] でのデータ転送 [!NOTE]\nDWH（BigQuery）への定期的なデータロードができて、 「データ処理の自動化・スケジューリング・監視」 をコードベースで管理できるのが特徴！ ワークフローの自動化 -\u0026gt; e.g 「毎日深夜にBigQueryからデータを抽出 → 変換 → 別のテーブルにロード」といった処理を自動化 タスク依存関係の管理 -\u0026gt; 「Aの次にB実行」などをコードで記述でき、失敗やリトライも自動処理可能 マネージドサービスとしての利点 -\u0026gt; Airflow のインフラ（Webサーバー、スケジューラ、ワーカーなど）を Google Cloud が自動で構築・管理 Google Cloud とのネイティブ連携 -\u0026gt; BigQuery、Dataflow、Cloud Storage、Pub/Sub など GCP サービスと簡単に連携可能 監視とロギング -\u0026gt; 実行ログやタスクの状態は Cloud Logging / Monitoring と連携 され、可視化やアラート設定が容易 Cloud Composer (Airflow) でのCloud Runオペレーター使用例\nfrom airflow import models from airflow.providers.google.cloud.operators.cloud_run import CloudRunExecuteJobOperator with models.DAG( \u0026#39;cloudrun_pipeline\u0026#39;, schedule_interval=\u0026#39;@daily\u0026#39;, ) as dag: process_data = CloudRunExecuteJobOperator( task_id=\u0026#39;process_data\u0026#39;, project_id=\u0026#39;your-project\u0026#39;, region=\u0026#39;us-central1\u0026#39;, job_name=\u0026#39;data-processor\u0026#39;, dag=dag, ) send_report = CloudRunExecuteJobOperator( task_id=\u0026#39;send_report\u0026#39;, project_id=\u0026#39;your-project\u0026#39;, region=\u0026#39;us-central1\u0026#39;, job_name=\u0026#39;report-sender\u0026#39;, dag=dag, ) process_data \u0026gt;\u0026gt; send_report Ref.\nCloud Composer の概要 Googel Cloud Google Cloud でデータ分析 DAG を実行する Airflow コマンドライン インターフェースにアクセスする src: Source code for airflow.models.xcom Apache Airflow 【R\u0026amp;D DevOps通信】Cloud ComposerのDAGでデータ基盤の転送パイプラインを監視 Sansan Cloud Composerにデータマート集計基盤を移行しました zozo [Dataflow] でのデータ転送 DataLake DWH [GCS] ---*-----------\u0026gt; [BQ] [Dataflow] Ref.\n分析用ログをCloud Storageにアップロードして、Dataflow経由でBigQueryへ格納する Zenn [GCP入門] Cloud LoggingのログをCloud Storageへ転送する Zenn [Dataform] でのデータ転送 DWH DataMart [BQ] ---*-----------\u0026gt; [BQ] [Dataform] Dataform の概要\nSQL を使用して BigQuery でスケーラブルなデータ変換パイプラインを開発、運用します。\n☑️ BigQuery で、キュレートされた最新の信頼できる文書化されたテーブルを開発する ☑️ データ アナリストとデータ エンジニアが同じリポジトリでコラボレーションできるようにする ☑️ SQL を使用して BigQuery でスケーラブルなデータ パイプラインを構築する ☑️ GitHub および GitLab と統合する ☑️ インフラストラクチャの管理を必要とせずにテーブルを最新の状態に保つ “Dataform” Google Cloud\nDataform の概要 Google Cloud\n2020年8月 に Google Cloud に買収され、Google Cloud コンソールにも統合。ref\nOverview Pipline BigQuery 向けデータパイプラインサービス「Dataform」の基本的な使い方 Zenn Overview SQLX Dataformを徹底解説 Ggen Dataform コアの概要 Dataform は DataformCore 由来で、DatafromCore は以下より構成 --- Dataform core = SQLX + JavaScript --- Dataform core = オープンソースのメタ言語-\u0026gt; Dataform での SQL ワークフロー開発に利用 SQLX = オープンソースの、SQL の拡張言語 JavaScript = SQLX ファイルの中にブロック記述-\u0026gt; 再利用可能な関数を定義可能 SQLX JSON part\n/* JSON: config part */ config { type: \u0026#34;incremental\u0026#34;, schema : \u0026#34;reporting\u0026#34;, name: \u0026#34;service_log_install\u0026#34;, tags: [\u0026#34;service\u0026#34;, \u0026#34;log\u0026#34;, \u0026#34;sercice_log\u0026#34;, \u0026#34;daily\u0026#34;], description: \u0026#34;製品のインストールログテーブル\u0026#34;, columns: { user_id: docs.user_id, installed_at_jst: docs.installed_at_jst, tracking_key: \u0026#34;\u0026#34;, device_id: \u0026#34;デバイスID\u0026#34;, ip_address: \u0026#34;IPアドレス\u0026#34;, user_agent: \u0026#34;ユーザエージェント\u0026#34;, os_name: docs.os_name, country_cd: docs.country_cd, subregion_key: \u0026#34;\u0026#34;, language_key: \u0026#34;\u0026#34;, region: docs.region_jp_ww_kr, }, uniqueKey: [\u0026#34;user_id\u0026#34;, …","date":1743646304,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1752822167,"objectID":"3f19fba0e3ddfb68afd6f18c055d6c03","permalink":"https://kithub.f5.si/post/bigquery_about_various_data_transfer/","publishdate":"2025-04-03T11:11:44+09:00","relpermalink":"/post/bigquery_about_various_data_transfer/","section":"post","summary":"🔍 Find out various data transfer methods starting from BigQuery.","tags":["BigQuery","SQL","SQLX","dbt","DWH","DataLake","ETL/ELT"],"title":"[BigQuery] About Various Data Transfer","type":"post"},{"authors":null,"categories":["Google"],"content":" Why SSIA\nWhat Looker Studio / (Pro) Official:\nhttps://developers.google.com/looker-studio https://lookerstudio.google.com/navigation/reporting Looker Studio release notes 主な機能:\n容易に使用可能なウェブ インターフェース Looker Studio は直感的で容易に使用できるように設計されています。レポート エディタには、完全にカスタム化されたプロパティ パネルとグリッドに自動整列されたキャンバスによるシンプルなドラッグ＆ドロップ オブジェクト機能があります。 レポート テンプレート レポート テンプレートのしっかりしたライブラリを利用して、データを数分で可視化できます。データソースに接続し、ニーズに合わせてデザインをカスタマイズできます。 データコネクタ データソースは、Looker Studio レポートを、基となるデータに接続するためのパイプとして機能します。各ソースには、データに簡単にアクセスして使用できる、独自の事前構築されたコネクタが含まれています。 Looker Studio API Looker Studio API を使用すると、Google Workspace や Cloud Identity を利用している組織で Looker Studio アセットの管理と移行を自動化できます。アプリケーションを構成して、Looker Studio API を迅速かつ簡単に使用できできます。 レポートの埋め込み 埋め込みにより、任意のウェブページやイントラネットに Looker レポートを含めることができます。これにより、チームや世界中の人にデータ ストーリーを伝えやすくなります。 cf. https://cloud.google.com/looker-studio\nLooker Studio / Looker tools name service remarks Looker Studio SaaS (旧名: Data Studio/Data Portal) Looker Studio Pro SaaS Gemini Lookerが使える、通知ができる Looker SaaS ｱｸｾｽ権限など細かなｶﾞﾊﾞﾅﾝｽが効く cf.;\n【完全保存版】Looker Studio（旧データポータル）の使い方 | 機能やメリットを分かりやすく解説 Other BI tools other tools name service remarks Redash OSS UI/UXが無料の域を超えている! evidence OSS マークダウン上にSQLｸｴﾘをそのまま記述して、描画 light dash OSS Tableau SaaS Looker Studio CLI tools https://github.com/google/looker-studio-dashboard-cloner Looker Studio how to use [BIをSlackに通知] [Looker Studio] -\u0026gt; [Slack] ※ Pro版ならそのまま通知可能 なんとか無料で使いたい…\nLooker Studio -[Gmail][GAS]-\u0026gt; Slack const SLACK_TOKEN = \u0026#39;xoxb-000000000000-000000000000-xxxxxxxxxxxx\u0026#39;; const threads = GmailApp.search(\u0026#39;Quicksight Report\u0026#39;, 0, 2); const messages = GmailApp.getMessagesForThreads(threads); let text = \u0026#39;\u0026#39;; for (const i in messages) { for (const j in messages[i]) { if (!messageis[i][j].isStarred()) { for (const attachment of messages[i][j].getAttachments()) { if (attachment.getName() != \u0026#39;logo.png\u0026#39;) { const payload = { file: attachment, } const option = { method: \u0026#39;post\u0026#39;, payload: payload, headers: {\u0026#39;Authorization\u0026#39;: `Bearer ${SLACK_TOKEN}`}, } const upload = JSON.parse(UrlFetchApp.fetch(\u0026#39;https://slack.com/api/files.upload\u0026#39;, option).getContentText()); text += `${upload.file.permalink}\\n`; } } messages[i][j].star(); } } } const payload = { channel: \u0026#39;XXXXXXXXXXX\u0026#39;, text: text, } const option = { method: \u0026#39;post\u0026#39;, contentType: \u0026#39;application/json\u0026#39;, payload: JSON.stringify(payload), headers: {\u0026#39;Authorization\u0026#39;: `Bearer ${SLACK_TOKEN}`}, } if (text) UrlFetchApp.fetch(\u0026#39;https://slack.com/api/chat.postMessage\u0026#39;, option).getContentText(); cf. Gmailに届いた添付ファイルをGASでSlackに流す Zenn Google Apps Script（GAS）を使って、Looker Studioで作成したレポートをSlackに定期配信する Zenn Looker Studio -[Gmail][Zapier]-\u0026gt; Slack https://github.com/zapier https://zapier.com/ | doc. cf. Slack にメールを送信する Slack [Looker Studio内で加工] [BigQuery] - custom query -\u0026gt; [Looker Studio] [BigQuery] ----------------\u0026gt; [Looker Studio] └ 計算ﾌｨｰﾙﾄﾞ custom query：\nLooker Studio内部でSQLを記述してクエリする Looker Studioに接続 「空のレポート」からレポートを作成 「データに接続」内でBigQueryを選択してデータのレポートに追加 「カスタムクエリ」を選択 ※ 課金プロジェクトには、料金を算出したいプロジェクトを入力 「カスタムクエリを入力」のコンソール画面にクエリを記述 select -- ユーザー名 user_email -- 課金額($) = 課金対象の参照データ量(B) × 料金体系($6/TB) , total_bytes_billed / pow(1024, 4) * 6 as price -- クエリ実行時間 , creation_time from -- プロジェクト単位のジョブのメタデータを取得 `region-US`.INFORMATION_SCHEMA.JOBS_BY_PROJECT where -- 対象ジョブを実行済みのクエリだけにフィルタ (job_type = \u0026#34;QUERY\u0026#34; and state = \u0026#34;DONE\u0026#34;) 計算ﾌｨｰﾙﾄﾞ：\n既存データソースを用いて、新たなメトリクスを作成 Looker Studioに接続しデータソースを開く 「フィールドを追加」を選択 「計算フィールドを追加」を選択 「フィールド名」「計算式」を入力し、保存 cf.;\nTag/Looker Studioの関数 [Looker Studio] 計算フィールドの使い方 | コピペで使える便利な関数リスト付き BigQueryのINFORMATION_SCHEMAで、ユーザー毎の料金を算出し、可視化する Zenn ","date":1742782212,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1751615720,"objectID":"2dc4b49fada9b79aef7cc39706cada1e","permalink":"https://kithub.f5.si/post/looker_studio_this_and_that/","publishdate":"2025-03-24T11:10:12+09:00","relpermalink":"/post/looker_studio_this_and_that/","section":"post","summary":"🫧 How to use Google's SaaS about BI.","tags":["Looker Studio","BI","Looker"],"title":"[Looker Studio] This and That","type":"post"},{"authors":null,"categories":["Data Analytics Engineer","Google"],"content":" Why SSIA\nWhat | BQML (AutoML) create or replace model `{my_project.my_dataset}`.{モデル名} options( model_type = \u0026#39;AUTOML_CLASSIFIER\u0026#39; --( ※ )使用アルゴリズム /* 使用モデル別にオプションを適時変更 */ -- , budget_hours = 1.0 --(e.g.)時間制限 -- , num_clusters=5 --(e.g.)分割数（クラスタリング時） -- , standardize_features = true --(e.g.) ) as {クエリ} -- 使用するデータセット ; モデリング #standardSQL /***** cf. カリフォルニア大学アーバイン校 https://academic-accelerator.com/Manuscript-Generator/jp/California-Irvine Machine Learning データセットのリポジトリから『顧客解約予測データセット』（公開) *****/ /* MODEL_DEVLOP */ create or replace model `my_dataset`.AUTO_ML_1 -- モデル名 options( model_type = \u0026#39;AUTOML_CLASSIFIER\u0026#39; -- 使用アルゴリズム -- , input_label_cols = [\u0026#39;\u0026#39;] -- ターゲット名（カラム） , budget_hours = 1.0 -- 時間制限 ) as /* クエリ（学習に使用するデータを抽出する） */ select State , Account_Length , Area_Code , Total_Charge / Account_Length as avg_daily_spend , CustServ_Calls / Account_Length as avg_daily_cases , Churn_ as label -- 推論するラベル from `my_dataset.CSV_CUSTOMER_ACTIVITY` where date(2020, 1, 1) \u0026lt;= Record_Date ; cf. オープンデータ 人工知能研究センター AutoML Tables が BigQuery ML で一般提供に GoogleCloud 評価・推論 #standardSQL /***** ◇ BigQuery ML 先に、「MODEL_DEVLOP」を実行。実行後、コメントアウトしてView化して同一ファイルで扱うのもあり 次に、「MODEL_EVALUATE」「MODEL_PREDICT」を実行。 *****/ with /* MODEL_EVALUATE */ evaluation as ( select * from ml.evaluate(model `my_dataset`.AUTO_ML_1, ( /* サブクエリ: モデル作成に使用した特徴量を抽出するクエリ */ select State , Account_Length , Area_Code , Total_Charge / Account_Length as avg_daily_spend , CustServ_Calls / Account_Length as avg_daily_cases , Churn_ as label from `my_dataset.CSV_CUSTOMER_ACTIVITY` where date(2020, 1, 1) \u0026lt;= Record_Date ) ) ) /* MODEL_PREDICT */ , predict as ( select predicted_label , prob , State , Account_Length , avg_daily_spend , avg_daily_cases from ml.predict(model `my_dataset`.AUTO_ML_1, ( /* サブクエリ: 予測するのに必要な特徴量を抽出するクエリ */ select State , Account_Length , Area_Code , Total_Charge / Account_Length as avg_daily_spend , CustServ_Calls / Account_Length as avg_daily_cases -- , Churn_ as label --∵ 推論時不要 from `my_dataset.CSV_CUSTOMER_ACTIVITY` where date(2020, 1, 1) \u0026lt;= Record_Date ) ), unnest(predicted_label_probs) where predicted_label = \u0026#39;True.\u0026#39; --∵ 2値分類の為 ) /* OUTPUT */ select * from evaluation; -- モデル評価 -- select * from predict; -- 推論 実施時に出たエラー文 (error) Unable to identify the label column in query statement. Either specify the label column using OPTIONS(input_label_cols=[\u0026#39;your_label_col\u0026#39;]) or name the label column in the data as \u0026#39;label\u0026#39;. BigQuery ML unable to identify label column in data stack overflow feedbackThe CREATE MODEL statement Google Cloud cf. 「BigQueryML」でSQLを書いて機械学習モデルを構築\u0026amp;予測できる！ Qiita SQLだけでモデルが作れる！BigQuery ML による自動モデル作成と Tableau による可視化 SkillUpAI AutoML Tables が BigQuery ML で一般提供に GoogleCloud BigQuery上のGA4データを元にしたGoogle Auto ML Tablesの試用レビュー PRINCIPLE 「BigQueryML」でSQLを書いて機械学習モデルを構築\u0026amp;予測できる！ Qiita BigQuery MLでスロット使用量が急増しているプロジェクトやユーザーを異常検知する yasuhisa’s blog Taitaicコンペ（Kaggle） #standardSQL begin /* モデル作成 */ create model `Kaggle_titanic`.MODEL_TAITANIC options (model_type = \u0026#39;logistic_reg\u0026#39;) as select Pclass , title , is_female , family_size , is_alone , age , embarked , fare , class_age , Survived as label from `Kaggle_titanic.preprocessed_data` --（前処理終わりのテーブル） left join `Kaggle_titanic.label_train` using(PassengerId) -- ラベル付与 where train_flag = \u0026#39;train\u0026#39; ; /* モデル評価 */ select * from ml.evaluate(model `Kaggle_titanic`.MODEL_TAITANIC, ( select Pclass, title, is_female, family_size, is_alone, age, embarked, fare, class_age, Survived as label from `Kaggle_titanic.preprocessed_data` left join `Kaggle_titanic.label_train` using(PassengerId) where train_flag = \u0026#39;train\u0026#39; ) ); /* モデル推論（テストデータに対して） */ select * from ml.predict(model `Kaggle_titanic`.MODEL_TAITANIC, ( select Pclass, title, is_female, family_size, is_alone, age, embarked, fare, class_age, Survived as label from `Kaggle_titanic.preprocessed_data` where train_flag = \u0026#39;test\u0026#39; ) ); /* 係数算出（特徴量の重み） */ select processed_input , weight from ml.weights(model `Kaggle_titanic.model_titanic`) order by abs(weight) desc ; /* サブミット作成 (※ Kaggle用) */ select PassengerID , predicted_label as Survived from ml.predict(model `Kaggle_titanic`.model_titanic, ( select PassengerID, Pclass, title, is_female, family_size, is_alone, age, embarked, fare, class_age, Survived as label from `Kaggle_titanic.preprocessed_data` where train_flag = \u0026#39;test\u0026#39; ) ); end | BQML ﾊｲﾊﾟﾗﾁｭｰﾆﾝｸﾞ（hyper parameters tuning） create or repalce model taxi.TOTAL_AMOUNT_MODEL options ( model_type=\u0026#39;linear_reg\u0026#39; -- 線形回帰 , input_label_cols = [\u0026#39;total_amount\u0026#39;] -- ラベル名 ) as select * from `taxi.sample_data` ; create or repalce model taxi.TOTAL_AMOUNT_MODEL options ( model_type=\u0026#39;linear_reg\u0026#39; -- 線形回帰 , input_label_cols = [\u0026#39;total_amount\u0026#39;] -- ラベル名 , num_trials = 10 , hparam_tuning_objectives = [\u0026#39;mean_squared_error\u0026#39;] -- 平均二乗誤差(評価指標) , max_parallel_trials = 5 , enable_global_explain = TRUE -- 特徴量寄与度を確認の為（Explainable AI機能を用いる場合） ) as select * from `taxi.sample_data` ; なんでこんなことするの？ 推論の制度が向上するから。（予測精度を上げるため） cf. BigQuery MLを利用した予測モデル構築〜パラメータチューニングから評価まで〜 DATUM STUDIO Hyperparameters and Objectives – …","date":1741941104,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1751615720,"objectID":"c7deb73f9755a9cef2ec8219b2779ee3","permalink":"https://kithub.f5.si/post/bigquery_about_machine_learning/","publishdate":"2025-03-14T17:31:44+09:00","relpermalink":"/post/bigquery_about_machine_learning/","section":"post","summary":"🔍 Completed within DWH, allowing modeling and analysis through queries","tags":["BigQuery","BQML","AI"],"title":"[BigQuery] About Machine Learning","type":"post"},{"authors":null,"categories":["Data Reliability Engineer","Google"],"content":" Why SSIA\nWhat 概要 -- データセット内のすべてのテーブル一覧を取得 select table_name , table_type , creation_time from `your-project.your-dataset.INFORMATION_SCHEMA.TABLES` ; -- 特定のテーブルのカラム情報を取得 select column_name , data_type , is_nullable from `your-project.your-dataset.INFORMATION_SCHEMA.COLUMNS` where table_name = \u0026#39;your_table\u0026#39; ; ▼ テーブル関連の階層：\nTABLES: 基本的なテーブル情報 COLUMNS: TABLESの各テーブルに紐づくカラム情報 VIEWS: TABLESの特殊なタイプ（ビュー）の詳細情報 TABLE_STORAGE: テーブルの物理的なストレージ情報 ▼ ジョブ関連の階層：\nJOBS: データセットレベルのジョブ情報 JOBS_BY_USER: プロジェクトレベルの詳細なジョブ情報 JOBS_BY_PROJECT: プロジェクトレベルの詳細なジョブ情報 JOBS_BY_ORGANIZATION: 組織レベルの最も包括的なジョブ情報 階層別ベン図： [INFORMATION_SCHEMA Ecosystem] ┌─────────────────────────────────────────────────────────────┐ │ │ │ [TABLES] [COLUMNS] │ │ ┌──────────────┐ ┌──────────────┐ │ │ │- table_name │──────────────│- table_name │ │ │ │- table_type │ │ │- column_name │ │ │ │- ddl │ │ │- data_type │ │ │ └──────────────┘ │ │- is_nullable │ │ │ : │ └──────────────┘ │ │ : │ │ │ : │ │ │ [VIEWS] │ [TABLE_STORAGE] │ │ ┌────────────┐ │ ┌───────────────┐ │ │ │- view_name │ └──────│- table_name │ │ │ │- view_ddl │ │- total_rows │ │ │ │- base_table│ │- total_bytes │ │ │ └────────────┘ └───────────────┘ │ │ │ └─────────────────────────────────────────────────────────────┘ │ - JOBS ⊂ JOBS_BY_* │ │ JOBS_BY_USER ⊂ JOBS_BY_PROJECT ⊂ JOBS_BY_ORGANIZATION │ │ │ │ [JOBS] [JOBS_BY_USER] │ │ ┌────────────────┐ ┌────────────────┐ │ │ │- job_id │ │- user_email │ │ │ │- statement_type│ │- job_id │ │ │ │- start_time │ │- total_bytes │ │ │ └────────────────┘ │- state │ │ │ └────────────────┘ │ │ [JOBS_BY_PROJECT] │ │ ┌─────────────────────┐ │ │ │ - project_id │┐ │ │ │ - job_id ││ │ │ │ - job_type ││ │ │ │ - state ││ │ │ │ - start_time ││ │ │ │ - end_time ││ │ │ │ - query ││ │ │ │ - user_email ││ │ │ └─────────────────────┘│ │ │ └─────────────────────┘ │ │ │ │ │ │ │ │ [JOBS_BY_ORGANIZATION] │ │ ┌────────────────────────────────┐ │ │ │ + organization_id │ │ │ │ + project_number │ │ │ │ + project_id │ │ │ │ + reservation_id │ │ │ │ + bi_engine_statistics │ │ │ │ + session_info │ │ │ │ + total_bytes_processed │ │ │ │ + total_slot_ms │ │ │ └────────────────────────────────┘ │ └─────────────────────────────────────────────────────────────┘ スコープの違い:\nlevel INFORMATION_SCHEMA i.e. roles/permission データセットレベル TABLES, COLUMNS, VIEWS, TABLE_STORAGE, JOBS 個人レベル JOBS_BY_USER 単一プロジェクト内で実行ユーザーのジョブ情報のみ（180日間データ保持） bigquery.jobs.list プロジェクトレベル JOBS_BY_PROJECT 単一プロジェクト内の全てのジョブ情報のみ（180日間データ保持） bigquery.jobs.listALL 組織レベル JOBS_BY_ORGANIZATION 組織全体のジョブ情報（複数プロジェクトを含む）（180日間データ保持） bigquery.jobs.listALL rore/ :roles/bigquery.resourceViewer ▼ cf.\n公式ドキュメント（基本情報） INFORMATION_SCHEMA の概要 INFORMATION_SCHEMA を使用したメタデータのクエリ ジョブ関連 INFORMATION_SCHEMA.JOBS INFORMATION_SCHEMA.JOBS_BY_PROJECT INFORMATION_SCHEMA.JOBS_BY_ORGANIZATION テーブル・カラム関連 INFORMATION_SCHEMA.TABLES INFORMATION_SCHEMA.COLUMNS INFORMATION_SCHEMA.VIEWS INFORMATION_SCHEMA.TABLE_STORAGE 実践的な使用例 一般的なINFORMATION_SCHEMAクエリ例 ジョブメタデータのクエリ 権限とセキュリティ 必要な権限 IAMロールとINFORMATION_SCHEMA その他の関連リソース BigQueryのメタデータ管理ベストプラクティス BigQueryモニタリングとログ記録 勘所 ▼ 使用ケース：\nJOBS_BY_PROJECT: 単一プロジェクトの監査 プロジェクト固有の分析 JOBS_BY_ORGANIZATION: 組織全体の使用状況の分析 クロスプロジェクトの監査 リソース使用の最適化 コスト分析 ▼ cf.\nBigQuery MLでスロット使用量が急増しているプロジェクトやユーザーを異常検知する INFORMATION_SCHEMA.JOBS_BY_* で BigQuery ジョブ警察🚨 Qiita Calculating GCP Query Costs Google Cloud Community ","date":1741250371,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1751615720,"objectID":"04c67c78a9d66a39a0f52dc9662d0dac","permalink":"https://kithub.f5.si/post/bigquery_about_information_schema/","publishdate":"2025-03-06T17:39:31+09:00","relpermalink":"/post/bigquery_about_information_schema/","section":"post","summary":"🔍 It's not good to know about data governance.","tags":["BigQuery","Google Cloud"],"title":"[BigQuery] About INFORMATION_SCHEMA","type":"post"},{"authors":null,"categories":["Data Analytics Engineer","Know-how","Google"],"content":" Why モチベーション データ分析における下準備（前処理）を BigQuery 内部で GoogleSQL を用いて行いたい。\nWhat テーブルのカラム確認 INFORMATION_SCHEMA select column_name , date_type , is_nukkable , column_default from `project_name.dataset_name.INFORMATION_SCHEMA.COLUMNS` table_name` limit 0 where table_name = \u0026#39;omae_no_tabel\u0026#39; ; bq コマンド $ bq show --schema project_name:dataset_name.table_name LIMIT句 select * from `project_name.dataset_name.table_name` limit 0 様々なアクセス方法（構造化テーブル） Cannot access field project_id on a value with type ARRAY\u0026lt;STRUCT\u0026lt;project_id STRING, dataset_id STRING, table_id STRING\u0026gt;\u0026gt; at [99:99] このエラーは、BigQueryのSQLクエリで、ARRAY\u0026lt;STRUCT\u0026lt;project_id STRING, dataset_id STRING, table_id STRING\u0026gt;\u0026gt;型のフィールドに直接アクセスしようとした際に発生。 ARRAY型のフィールドには直接フィールドアクセス（例: project_id）を行うことはできない。 代わりに、ARRAY内の各要素（STRUCT）に対してアクセスする必要がある。\n以下は、この問題を解決するためのいくつかの方法。\nARRAY内の要素展開\nARRAY内の各要素に対してアクセスするには、UNNEST関数を使用してARRAYを展開し、その後でSTRUCTのフィールドにアクセス SELECT element.project_id, element.dataset_id, element.table_id FROM your_table, UNNEST(your_array_column) AS element your_array_columnというARRAY型のカラムをUNNESTで展開し、各要素（element）のフィールドにアクセス 特定要素にアクセス\nARRAY内の特定の要素にアクセスしたい場合は、インデックスを使用してアクセス SELECT your_array_column[OFFSET(0)].project_id, your_array_column[OFFSET(0)].dataset_id, your_array_column[OFFSET(0)].table_id FROM your_table ARRAYの最初の要素（インデックス0）にアクセス ARRAY内の要素をフィルタリングする\nARRAY内の特定の条件を満たす要素にアクセスしたい場合は、WHERE句を使用してフィルタリングできます。 SELECT element.project_id, element.dataset_id, element.table_id FROM your_table, UNNEST(your_array_column) AS element WHERE element.project_id = \u0026#39;your_project_id\u0026#39; このクエリでは、project_idが特定の値と一致する要素のみを取得 ARRAY内の要素を集計する\nARRAY内の要素を集計したい場合は、ARRAY_AGG や ARRAY_LENGTH などの関数を使用 SELECT ARRAY_LENGTH(your_array_column) AS array_length, ARRAY_AGG(element.project_id) AS project_ids FROM your_table, UNNEST(your_array_column) AS element このクエリでは、ARRAYの長さと、project_idのリストを取得 cf.\nRef. ","date":1741008200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1751615720,"objectID":"2776466c67d2189cc59d2c2fc5de89e0","permalink":"https://kithub.f5.si/post/bigquery_data_analysis_on_preparation/","publishdate":"2025-03-03T22:23:20+09:00","relpermalink":"/post/bigquery_data_analysis_on_preparation/","section":"post","summary":"🔍 I would like to prepare for data analysis using GoogleSQL inside BigQuery","tags":["BigQuery","SQL"],"title":"[BigQuery] Data analysis on preparation","type":"post"},{"authors":null,"categories":["Google"],"content":" Why よく使用するブラウザーなので便利機能の覚書\nWhat https://www.google.com/chrome/ https://www.google.com/chrome/canary/ https://www.chromium.org/\nhttps://chromium.googlesource.com/chromium/src.git\ncf.\nWhat is Google Chrome Google Chrome vs Chromium - What’s the Difference? リンク作成（Text-fragments） #:~:text= 上記をURL末尾に付加して、該当テキスト文を添付\ncf.\nLink to Text Fragment\nテキストフラグメントで特定テキストにジャンプ＆ハイライトさせるURLを作る\nページ全体スクショ 該当ページへ遷移 ⌘ + option + i ⌘ + shift + p 「Run \u0026gt; Capture full size screenshot」 cf. Webページの全画面をスクリーンショットする方法【Chrome版＆Edge版】※拡張機能なし note ","date":1740458758,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1751615720,"objectID":"89afe13f73ca271da3e54ba8994a0da3","permalink":"https://kithub.f5.si/post/google_chrome_this_and_that/","publishdate":"2025-02-25T13:45:58+09:00","relpermalink":"/post/google_chrome_this_and_that/","section":"post","summary":"⚪️ ","tags":["Google Chrome","Chrome Canary","Chromium"],"title":"[Google Chrome] This \u0026 That","type":"post"},{"authors":null,"categories":["Data Reliability Engineer"],"content":" Why SSIA\nWhat Infrastructure as Code（IaC） とは 一言でいうと、「インフラ構築を手動ではなく、コードで管理しよう」という思想で、これにより以下のような恩恵が受けられる\nインフラ構築・変更時、コードレビューによるダブルチェックが可能になる 各個人へ権限付与する必要がなくなり、以下のリスクを軽減できる 環境の再構築、削除、複製が容易になる（費用対効果、再現性の向上。ナレッジのコード化） Terraform GitHub https://github.com/hashicorp/terraform Comunity https://discuss.hashicorp.com/c/terraform-core/27 Certifications https://developer.hashicorp.com/certifications/infrastructure-automation Terraform フロー cf. https://developer.hashicorp.com/terraform/tutorials/docker-get-started/infrastructure-as-code\n[!NOTE] 1〜3: 事前準備\n4〜5: 実運用系\nTerraformをインストール https://developer.hashicorp.com/terraform $brew tap hashicorp/tap $brew install hashicorp/tap/terraform 定義ファイルの作成・配置 e.g VMware vSphere に構築したい場合、provider にインフラ名 ”vsphere” を定義。(もしもAWSの場合、”aws”を定義) provider \u0026#34;vsphere\u0026#34; { user = var.vsphere_user #vsphereのユーザー名 password = var.vsphere_password #vsphereのパスワード vsphere_server = var.vcenter_server #vCenterのFQDN/IPアドレス allow_unverified_ssl = true #SSL証明書の検証無効 } [省略] #Resource resource \u0026#34;vsphere_virtual_machine\u0026#34; \u0026#34;vm\u0026#34; { count = var.prov_vm_num #仮想マシンの数 name = \u0026#34;${var.prov_vmname_prefix}${format(\u0026#34;%03d\u0026#34;,count.index+1)}\u0026#34; #仮想マシンの名前”接頭語”+”001” [省略] #Resource for VM Specs num_cpus = var.prov_cpu_num #仮想マシンのCPUの数 memory = var.prov_mem_num #仮想マシンのメモリのMB ※変数の定義は別途必要 ※ Terraformでは、拡張子.tf のファイル(テキストデータ)に、リソースの定義を記述 ※ なお、Terraformのコードは、HCL という独自言語により記述されているため、構文には注意 Terraformプロバイダを設定($terraform init) 定義ファイルに基づいて、各製品のＡＰＩに対応したコードを自動生成($terraform plan) コード適用($terraform apply) (余談) 様々なIaC\nTool 設定ファイル ref Chef recipe（ﾚｼﾋﾟ） ref Ansible playbook IaSQL SQLでインフラを管理しようの試み\nRef. Terraform連載 Terraform連載 第1回：いまさら聞けない、IaCってなに？～Terraform、IaSQLの紹介～ Terraform連載 第2回：Terraform v1.5の紹介＆活用方法について考えてみた Terraform連載 第3回：Terraform v1.6のtestコマンドについてご紹介 Terraform連載 第4回：for_eachの使い方 Terraform連載 第5回：module(モジュール)の紹介 Terraform連載 第6回：Terraform v1.7 removedブロックの紹介 Terraform連載 第7回：importブロックはmoduleのリソースも取り込めるか？ Terraform連載 第8回：Terraformの管理スコープとしてiDRACをimportしてみた Terraform連載 第9回：比較で広がるIaCの世界！ ～TerraformとAnsible～ Terraformでのloop処理の書き方（for, for_each, count） Zenn\nTerraformでネストしたloopを書く Zenn\n","date":1740044946,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1751863902,"objectID":"dbc626abcbbdb0d142873a47aa8bcb41","permalink":"https://kithub.f5.si/post/terraform_catchup/","publishdate":"2025-02-20T18:49:06+09:00","relpermalink":"/post/terraform_catchup/","section":"post","summary":"✝️ A little while ago, I'll be writing about my studies as an introduction to Terraform.","tags":["Terraform","IaC"],"title":"[Terraform] Catchup","type":"post"},{"authors":null,"categories":["Data Analytics Engineer"],"content":" Why SSIA\nWhat || NumPy https://github.com/shiroimon/ml_dic/blob/master/pre_learning/week0/dic_exam.ipynb https://github.com/shiroimon/ml_academic/blob/master/various/online/01_cdx/cdx_numpy.ipynb https://github.com/shiroimon/ml_academic/blob/master/various/online/02_kame/ds2_numpy.ipynb Ref. ","date":1739418332,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1751615720,"objectID":"75f2df285d5470658cc277fa29c89f39","permalink":"https://kithub.f5.si/post/python_basicly_datahandling_at_numpy/","publishdate":"2025-02-13T12:45:32+09:00","relpermalink":"/post/python_basicly_datahandling_at_numpy/","section":"post","summary":"🔷 ","tags":["Python","NumPy"],"title":"[Python] Basicly data-handling \u003cNumpy\u003e","type":"post"},{"authors":null,"categories":["Data Analytics Engineer"],"content":" Why SSIA\nWhat || Pandas https://github.com/shiroimon/ml_academic/blob/master/various/online/01_cdx/cdx_pandas.ipynb https://github.com/shiroimon/ml_academic/blob/master/various/online/02_kame/ds3_pandas.ipynb Ref. ","date":1739418332,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1751615720,"objectID":"a7ba7af45bc96f87fbf5221db34670a5","permalink":"https://kithub.f5.si/post/python_basicly_datahandling_at_pandas/","publishdate":"2025-02-13T12:45:32+09:00","relpermalink":"/post/python_basicly_datahandling_at_pandas/","section":"post","summary":"🐼 ","tags":["Python","Pandas"],"title":"[Python] Basicly data-handling \u003cPandas\u003e","type":"post"},{"authors":null,"categories":[""],"content":" Why SSIA\nWhat # System info. sw_vers ProductName: Mac OS X ProductVersion: 10.13.2 BuildVersion: 17C88 # Hardware info. system_profiler SPHardwareDataType # Disk info. df -h # Browser open. open https://www.google.com open -a \u0026#34;Google Chrome\u0026#34; https://www.google.com # Show hidden files. defaults write com.apple.finder AppleShowAllFiles TRUE # Hide hidden files. defaults write com.apple.finder AppleShowAllFiles FALSE # Restart Finder. killall Finder # Show desktop. osascript -e \u0026#39;tell application \u0026#34;Finder\u0026#34; to set visible of every process whose visible is true and name is not \u0026#34;Finder\u0026#34; to false\u0026#39; # Hide desktop. osascript -e \u0026#39;tell application \u0026#34;Finder\u0026#34; to set visible of every process whose visible is false and name is not \u0026#34;Finder\u0026#34; to true\u0026#39; # Memory usage. top -o MEM # One process done. kill -9 [プロセスID] # All process done. (Dangerous) killall [プロセス名] # Disk usage. du -sh [ディレクトリ名] # Disk usage list. du -h [ディレクトリ名] | sort -h # Disk usage list with hidden files. du -h -d 1 [ディレクトリ名] | sort -h # 1GB以上のファイルを検索 find . -type f -size +1G -print0 | xargs -0 ls -lh # 特定の拡張子（例：.log）の大きなファイルを検索 find . -type f -name \u0026#34;*.log\u0026#34; -size +100M -print0 | xargs -0 ls -lh # 最近変更されたファイル検索（48時間以内） find . -type f -mtime -2 # 物理コア数 sysctl -n hw.physicalcpu_max # 論理コア数 sysctl -n hw.logicalcpu_max # ﾒﾓﾘ (byte) sysctl hw.memsize # PC userid id -u # PC groupid id -g [ZIP] # ZIP化（鍵あり） zip -e new.zip old. # ZIP化（鍵なし） zip new.zip old. # ZIP開封 un zip new.zip [Launchpad] # Icons Size (eg. 4× 4) defaults write com.apple.dock springboard-rows -int 4 defaults write com.apple.dock springboard-columns -int 4;killall Dock [Wi-Fi] # [1] SSID一覧 networksetup -listpreferredwirelessnetworks en0 # より詳細を取得 networksetup -listallhardwareports # [2] パスワード確認 # （mySSID を任意の SSID に置き換える-\u0026gt; UserName と password が求められる) security find-generic-password -wa {mySSID} cf.\n過去に接続したことがあるWi-Fiのパスワードをターミナルで表示させる。 Qiita 「Mac」や「Windows」PCでWi-Fiのパスワードを表示するには CNETJapan Ref. MACのコマンドラインをど忘れしない用のコマンド集 Zenn Macターミナルでの圧縮・解凍コマンド備忘録 Qiita How to Increase or Decrease Launchpad Icons Size in macOS? WebNote ","date":1737993032,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1751615720,"objectID":"63e03be58b75dd9ec53047f5dbd58bdb","permalink":"https://kithub.f5.si/post/macos_commands_memorandum/","publishdate":"2025-01-28T00:50:32+09:00","relpermalink":"/post/macos_commands_memorandum/","section":"post","summary":"🍏 macOS useful commands and tips.","tags":["macOS","CLI"],"title":"[macOS] Commands memorandum","type":"post"},{"authors":null,"categories":["Certifications","Google"],"content":" Why 数多に存在する Google Cloud ツール\nDAE/DRE で良く用いるツール群を整理したい\nWhat [GCP] Certification [GCP] Certification https://cloud.google.com/learn/certification?hl=en\nFoundational certification Associate certification Professional certification [GCP] Tools Cloud AI (LLM / ML) REPL (対話型実行環境) DWH (EDH) BI CI/CD DAG (Directed Acyclic Graph) Infrastructure Storage Others Office Browser Data Source (Analytics) Script Language SEM (SEO / PPC) Competition Ref. Google Cloud products at a glance Google Cloud Explore over 150+ Google Cloud products Google Cloud List of all Google Cloud Platform (GCP) Services economize 50 Google Cloud Products List in One Page Medium ","date":1737444283,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1751615720,"objectID":"3e5ebbedb0064f9a30f4048ec1002889","permalink":"https://kithub.f5.si/post/google_cloud_tools_memorandum/","publishdate":"2025-01-21T16:24:43+09:00","relpermalink":"/post/google_cloud_tools_memorandum/","section":"post","summary":"☁️ I would like to summarize it to understand Cloud tools.","tags":["Google Cloud"],"title":"[Google Cloud] Tools memorandum","type":"post"},{"authors":null,"categories":["Data Analytics Engineer"],"content":" Why SSIA\nWhat :%CSVArrangeColumn: 表形式描画 :CSVTranspose：転置行列 [!NOTE] 主な操作コマンドが、列指向（縦持ち）が強いので、\n仮に、元データが行指向（横持ち）なら、先に変換をかましたほうが分析しやすいかもしれない。\n分析系 コマンドモード 内容 （基礎集計） :CSVSumRow 行の合計を出力 :CSVSumCol 現在の列の合計を出力 :CSVSumCol 列番号 指定した列の合計を出力 :CSVCountCol 現在の列内の値の数を出力 :CSVCountCol 列番号 指定した列内の値の数を出力 :CSVColumnWidth 列ごとの最大文字数を出力 （統計データ） :CSVMaxCol 現在の列内の最大値を出力 :CSVMaxCol 列番号 指定した列内の最大値を出力 :CSVMaxMin 現在の列内の最小値を出力 :CSVMaxMin 列番号 指定した列内の最小値を出力 :CSVAvgCol 現在の列内のデータの平均値を出力 :CSVAvgCol 列番号 指定した列内のデータの平均値を出力 :PopVarCol 現在の列の母集団分散を出力 :PopVarCol 列番号 指定した列の母集団分散を出力 :SmplVarCol 現在の列の標本分散を出力 :SmplVarCol 列番号 指定した列の標本分散を出力 :PopStdCol 現在の列の母標準偏差を出力 :PopStdCol 列番号 指定した列の母標準偏差を出力 :SmplStdCol 現在の列の標本標準偏差を出力 :SmplStdCol 列番号 指定した列の標本標準偏差を出力 :CSVAnalyze 現在の列を分析する（値とその数、割合など） :CSVAnalyze 列番号 指定した列を分析する（値とその数、割合など） 編集系 コマンドモード 内容 （行操作） :CSVNewRecord 新しい行を作成 （列操作） :CSVAddColumn 現在の列の右側に新しい列を追加 :CSVAddColumn 列番号 指定した列の右側に新しい列を追加 :CSVColumn 現在の列をコピー :列番号CSVColumn 指定した列をコピー :CSVDeleteColumn 現在の列を削除 :CSVDeleteColumn 列番号 指定した列を削除 :CSVDupColumn 現在の列を右側に複製 :CSVDupColumn 列番号 指定した列を右側に複製 :CSVMoveColumn 列番号 列番号 最初に指定した列を、2つ目に指定した列の右側へ移動 :CSVMoveColumnor 現在の列を最後の列の右側へ移動 :CSVDuplicates 列番号 指定した列で重複している行を出力 :CSVSubstitute 列番号/パターン/文字列/ 指定した列で置換 :CSVSort 現在の列でファイルをソート :CSVSort 列番号 現在の列でファイルをソート :CSVSort! 現在の列でファイルを逆順にソート :CSVSort! 列番号 現在の列でファイルを逆順にソート :CSVConvertData データをほかの形式に変換 :CSVTranspose 列と行を入れ替える（転置） :CSVNewDelimiter デリミタ 区切り文字を変更 表示系 コマンドモード 内容 :CSVTabularize テーブル形式でのプレビュー表示 :CSVArrangeColumn テーブル形式での表示へ切り替え（実験的機能） :CSVWhatColumn カーソルが何列目にあるか :CSVWhatColumn! 同列1行目の内容を表示 :CSVNrColumns 最大列数を表示（先頭から10行で判断） :CSVSearchInColumn/パターン/ パターン検索（現在列） :CSVSearchInColumn 列番号 /パターン/ パターン検索（指定番号列） :CSVHiColumn 強調表示（現在の列） :CSVHiColumn 列番号 強調表示（指定番号の列） :CSVHiColumn! 強調表示解除 :CSVHeader 別ウィンドウで表示（１行目） :CSVHeader 行数 別ウィンドウで表示（１〜指定行数分） :CSVHeader! 開いた行ヘッダウィンドウを閉じる :CSVVHeader 別ウィンドウで表示（１列目） :CSVVHeader 列番号 別ウィンドウで表示（１〜指定列数分） :CSVVHeader! 開いた列ヘッダウィンドウを閉じる :CSVVertFold 列を折りたたむ（１〜現在列まで） :CSVVertFold 列番号 列を折りたたむ（１〜指定列数まで） :CSVVertFold! 列の折りたたみを解除 cf.\n第239回 にわか管理者のためのLinux運用入門 Vimを使う - CSVを使いこなす「chrisbra/csv.vim」 第240回 にわか管理者のためのLinux運用入門 Vimを使う - CSVを使いこなす（閲覧編） 第241回 にわか管理者のためのLinux運用入門 Vimを使う - CSVを使いこなす(移動編) 第242回 にわか管理者のためのLinux運用入門 Vimを使う - CSVを使いこなす(分析編) 第243回 にわか管理者のためのLinux運用入門 Vimを使う - CSVを使いこなす(比較編) 第244回 にわか管理者のためのLinux運用入門 Vimを使う - CSVを使いこなす(編集編) 第245回 にわか管理者のためのLinux運用入門 Vimを使う - CSVを使いこなす(フィルタ編) 第246回 にわか管理者のためのLinux運用入門 Vimを使う - CSVを使いこなす(フォーマット変換編) 第247回 にわか管理者のためのLinux運用入門 Vimを使う - CSVを使いこなす(設定編) 番外編 揃えることに特化した最強ツール\ncf. 第248回 にわか管理者のためのLinux運用入門 Vimを使う - 「良い具合」にそろえるプラグイン「vim-easy-align」\n","date":1735216027,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1751615720,"objectID":"f44f8ae89f66b7228ab6cbcdffc72558","permalink":"https://kithub.f5.si/post/vim_csv_control/","publishdate":"2024-12-26T21:27:07+09:00","relpermalink":"/post/vim_csv_control/","section":"post","summary":"✅ God's tool that lets you edit CSVs like Excel in Vim.","tags":["Vim","CSV"],"title":"[Vim] CSV Control","type":"post"},{"authors":null,"categories":["Udemy","Hands-on"],"content":" Why SSIA\nWhat YAML = “YAML Ain’t Markup Language” （YAMLはマークアップ言語ではない） cf. YAMLの歴史と語源 Qiita YAML 【YAML Ain’t Markup Language】 .ymlファイル IT用語辞典 Udemy 実例で学ぶ！DevOps基礎力としてのYAML入門\n対象 DevOpsツールを用いているが半信半疑で書いているﾆｷ (e.g. Ansible, K8s, Docker…etc.) Handsonしたいﾆｷ ふんわり文法（List, dictionary…etc.）理解しているがもう少し踏み込んで書けるようになりたいﾆｷ ゴール YAMLの概要、特徴を言語化できる YAMLの文法エラーに対処できる YAMLを自信持って書けるようになる [1] YAML概要（理論編） # An employee record # コメントアウト --- # セパレータ ※先頭行に記述 name: shiro job: Data engineer EMPLOYED: TRUE languages: # Dict記述方法 python: middle shellscript: row sql: high foods: # List記述方法 - apple - banana YAMLとは設定ファイル\nシリアライズ（直列化）が容易 # 直列化/逆直列化図 「\u0026lt;プログラム\u0026gt; -\u0026gt; 直列化 -\u0026gt;「\u0026lt;バイト列\u0026gt; \u0026lt;-\u0026gt; データ 「\u0026lt;変換先\u0026gt; object 00001010 形式変換 \u0026lt;-\u0026gt; fail, DB, NW 」\u0026lt;- 逆直列化 \u0026lt;- 」 」 直列化の目的は「データの形式変換」 事前に\u0026lt;バイト列\u0026gt;にしておき、様々なデータ形式へ変換やネットワーク経由送信を行えるようになる [2] YAMLハンズオン（実践編） Overview\n# ※ ディレクトリの構成には特に意味はないとのこと development/ └ yaml_project/ └ handson1.yaml └ ~ HandsOn [3] YAMLサンプル（実例編） DevOps GirHub Actions Kubernetes Docker Compose Ref. ","date":1734442788,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1751615720,"objectID":"822fb8aa7b359d1f4d73e073bb85467a","permalink":"https://kithub.f5.si/post/yaml_catchup/","publishdate":"2024-12-17T22:39:48+09:00","relpermalink":"/post/yaml_catchup/","section":"post","summary":"🚫 Understand Yaml, the configuratio file familiar to DevOps.","tags":["YAML","XML","JSON","TOML"],"title":"[YAML] Catchup","type":"post"},{"authors":null,"categories":[""],"content":" Why SSIA\nWhat cf. https://github.com/topics/shell\nhttps://www.zsh.org/ ZSH Documentation https://zsh.sourceforge.io/Doc/ https://zsh.sourceforge.io/Doc/Release/zsh_toc.html Zsh Extentions https://github.com/ohmyzsh/ohmyzsh # 現在仕様Shell $ echo $SHELL $ echo $0 # Zshのマニュアル $ man zsh # ---------- # 基本 # ---------- $ ls $ grep word # \u0026#34;word\u0026#34;の文字列検索 $ wc -l # word counts: 単語数 $ ls | grep conf | wc -l 17 # ---------- # 移動 # ---------- $ cd $ pwd ~/sandbox/prodctions/dev/user $ cd dev prod # 特定のディレクトリ構造であればジャンプできる ~/sandbox/prodctions/prod/user # Directory Hash (Zsh特有) # 移動先ルートパスをメモ書きできる $ hash -d api=/tmp/src/main/go/api $ cd ~api \u0026amp;\u0026amp; pwd ~/tmp/src/main/go/api # ---------- # 操作系 # ---------- # カーソル移動 (Emacs key風は macOS 特有) # # ^p # ^b | ^f # ^n # # ^d :delete with Ctrl + d # ^h :backspace with Ctrl + h # ^a :行頭 # ^e :行末 # ^k :カーソルより後ろ（右方向）削除 # ^w :カーソルより前（左方向）の単語消す # ---------- # 履歴 # ---------- $ history # 過去のgit コマンド列挙して、ナンバー実行 $ history | grep git $ !{number} # ---------- # 展開 # ---------- # - Brace Expansion (zsh特有) :{,}で総当り展開 # - Glob Expansion (zsh特集) :Tabで確認展開（^xu :で戻せる） $ echo {shop_a,shop_b}.csv shop_a.csv shop_b.csv $ diff -u {shop_a,shop_b}.csv $ cp -p httpd.conf htppdd.conf.buckup $ cp -p htppd.conf{, .buckup} # 一気にで構造化ディレクトリが作れる # src/ # └main/ # └script/ # └api/ # └user/ # └task/ # └db/ # └user/ # └task/ # └test/ # └script/ # └api/ # └user/ # └task/ # └db/ # └user/ # └task/ $ mkdir -p src/{main,test}/script/{api,db}/{user,task} # ---------- # Mac特有 # ---------- $ pbcopy # QuickLook :Finderで見ることが $ qlook # Double click $ open $ open https://google.com Ref. 【Shell入門】一生触るターミナル操作を強化しておこうぜ！ ","date":1732602180,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1751615720,"objectID":"9cdbda76da579750c0e7d30b74966cd2","permalink":"https://kithub.f5.si/post/shell_commands_memorandum/","publishdate":"2024-11-26T15:23:00+09:00","relpermalink":"/post/shell_commands_memorandum/","section":"post","summary":"🐚 ","tags":["Shell","Zsh","CLI"],"title":"[Zsh] Commands memorandum","type":"post"},{"authors":null,"categories":["Know-how"],"content":" Why SSIA\nWhat || OSS https://github.com/github/opensource.guide https://opensource.guide/ || LICENSE https://choosealicense.com/\nGitHub社がライセンス選定指針のサイトを作成。 GitHub上のライセンスは、リポジトリの利用条件を明確にし、他の開発者がそのコードをどのように使用、変更、配布できるかを定義するために重要。 ライセンスを適切に設定することで、著作権侵害のリスクを軽減し、 オープンソースコミュニティの一員 としてのルールを守ることができる。 | メインのオープンソースライセンス ライセンス名 特徴 条件 MIT License 非常に寛容なライセンス で、ほぼ無制限にコードを使用、コピー、変更、合併、公開、配布、サブライセンス、販売することができる。 ※ ただし、 元の著作権表示 と ライセンス通知 を全てのコピーや著作物に含める必要がある。 GNU GPL (General Public License) コピーや配布、変更の自由を保障しつつも、変更したバージョンも同じライセンスで配布することを求めている。 変更したコードを配布する際に、元の著作権表示、ライセンス通知、およびソースコードを提供する義務がある。 Apache License 2.0 商用利用、特許権の明示的な許諾、改変後のコードの配布を許可している。 著作権表示、ライセンス通知、変更点を明示することを求めている。 BSD License (3-clause BSD License) MITライセンスと似ているが、追加の条件として 広告材料に貢献者の名前を使わない ことを求めている。 著作権表示、ライセンス通知を含めること。 その他 cf. たくさんあるオープンソースライセンスのそれぞれの特徴のまとめ coliss 主要ライセンス以外では上記の記事がより詳細！ | 利用方法（MIT） リポジトリにMITライセンスを適用\nライセンスファイルの作成\n$ touch LICENSE MIT License Copyright (c) [year] [fullname] Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \u0026#34;Software\u0026#34;), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \u0026#34;AS IS\u0026#34;, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE. e.g. https://github.com/hubotio/hubot/blob/main/LICENSE.md README.mdにライセンス情報を追加\n## License This project is licensed under the MIT License - see the [LICENSE](license-link) file for details. cf. https://docs.github.com/ja/repositories/managing-your-repositorys-settings-and-features/customizing-your-repository/licensing-a-repository || Comunication GitHubでのコメントは、オープンソース（OSS）のプロジェクト管理やチーム内の協力を円滑に進めるために非常に重要。 詰まるところプロジェクトの成功に欠かせない要素だとも思う。 丁寧で建設的なコメントは、コミュニケーションを円滑にし、プロジェクトの質向上の助けになると信じ、敬意を持ち、具体的で明確なフィードバックを心がけることで、良好なコラボレーション環境を築いていきたい。ref\nGitHubでのコメントの際に注意すべきポイントや、適切なマナーについてや、マネしたいと思う事、素晴らしいと思ったことを振り替えれるように箇条書きしておきたい。（いつかの自分の為にも）\n| マナー・エチケット編 社会人ならば \u0026#34;こんなこと\u0026#34;、と軽視しがちな当たり前を言語化を目指したい。 敬意を持つ: 他の貢献者に対して敬意を持ってコメントをする。礼儀正しい言葉遣いを心がけ、攻撃的な言葉やトーンは避ける。 建設的なフィードバック: 批判する際には、具体的な問題点と改善提案を含めるようにする。単なる否定的なコメントは避け、建設的なアプローチを心掛けたい。 感謝の気持ちを表す: 貢献や助けてくれたことに対して感謝の意を示すことは、コミュニティのモチベーションを高めるため。善意ある行動は当たり前ではなく、感謝すべき点である。 具体的で明確に: コメントは具体的で明確に。曖昧な表現は避け、__誰でも理解できる__ようにする。 関連性のある情報を提供: 問題や提案に関連する情報を提供。コードの一部やリンクを含めると、理解しやすく優しい世界。 質問を明確に: 問題点や疑問点を明確にし、具体的な質問をすることで、迅速で適切な回答を得られる。 | 思想編 思想的なところは、どちらかというと付加価値を獲るためのものと考える。 協力的な姿勢: コミュニティは協力によって成り立っている事は間違いない。問題解決や改善に向けて協力する姿勢を持ちたい。 透明性: 意見や決定は透明性を持って行いたい。背後での議論は避け、オープンな場でのコミュニケーションを心がけたい。 学習と成長: 自分の意見が常に正しいとは限らない。全ては 学習の機会 と捉え、他者の意見にもしっかりと傾聴したい。 || アプローチとテクニック編 適切なスレッド: 適切なスレッドやプルリクエスト、イシューでコメントを行う。関係のない場所でのコメントは避ける。 FMTの活用: GitHubのマークダウン機能を活用し、コードブロックやリスト、リンクを使ってコメントを見やすく整理。 TMPの活用: 特定の形式でコメントを求める場合、テンプレートを用意しておくと効率的。 語学力（適切な文法）: 会話ならばボディランゲージや表情が助けになる。しかし、ドキュメントのみというとそうはいかない。適切な言葉遣い、読解力等は避けて通れない。（特にOSSに参加するなら尚のこと…翻訳ばかりに頼り切らず頑張らねば！きっと楽しい） | Suggestion ```suggestion [訂正文] ``` 効果：コードの一部分を改変依頼を出したい時に見やすく分かり易く依頼を投げることができる機能。 手順：レビューワーが±マークのボタン「Add a suggestion」を押し、選択したコードに対して修正案を記載。 cf. [GitHub] Multi-line code suggestionsでコード提案機能が便利になりました DevelopersIO Githubのレビューの際に「こやつできるな」と思わせることができる小技、テクニック GitHub コードレビューでサジェスション機能を使って差分を提案しよう | diff ```diff [main.py] - print(\u0026#39;hoge\u0026#39;) + print(\u0026#39;huga\u0026#39;) ``` 効果：Markdown で修正前後を記述することができる。 勘所：isuue をpostしたり、ドキュメントを記述する際に用いる。 | Hide comment “Choose a reason” ・Spam 👎スパム ・Abuse 🤬侵害・雑言 ・Off topic 👋スレ違い ・Outdated 😇古い・期限切れ ・Resolved 💮解決済み せっかくいただいた、コメントに対して、真摯にステータスを振って非表示へとクロージング。ref | “Pending” mark 問題 (Pending) マークがついていると記述者は見えているが、他の人には見えない状態なので回避したい。ref 解決 FileChanged \u0026gt; Review changes \u0026gt; Submit review 課題 基本的には、[Conversation]タブから返信していれば、pending状態にはならないよう。 一度でも[FileChanged]タブから[Start a review]ボタンでコメント返信したあとに、 [Conversation]タブ上から返信するとすべてpending状態になってしまうよう。 cf. プルリクエストへのコメント GitHub 【GitHub】プルリクにコメント返信した\u0026#34;つもり\u0026#34;になってた話 Qiita | Commit Message コミットメッセージの規則：\nfeat: 新機能 fix: バグ修正 docs: ドキュメントのみの変更 style: コードの意味に影響を与えない変更（空白、フォーマット、セミコロンの欠落など） refactor: バグを修正したり機能を追加したりしないコードの変更 test: テストの追加・修正 chore: ビルドプロセスやドキュメント生成などの補助ツールやライブラリの変更 上記のような接頭辞（プレフィックス）をつけると親切!!! eg. git commit [!NOTE] とある機能開発でのコミットの打ち方例\nデータ構造の追加 $ git commit -m \u0026#34;feat: Add data classes for baseline slot adjustment - Add BaselineSlotAdjustmentConfig - Add BaselineSlotAdjustmentResult - Add BaselineSlotAdjustmentInfo\u0026#34; 基本機能の実装 $ git commit -m \u0026#34;feat: Implement baseline slot …","date":1732158368,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1751615720,"objectID":"fa5c429b47bef5a9988e53ea4938c9cb","permalink":"https://kithub.f5.si/post/github_oss_wana_be_join/","publishdate":"2024-11-21T12:06:08+09:00","relpermalink":"/post/github_oss_wana_be_join/","section":"post","summary":"📍 About etiquette and the meaning of licenses when paticipating in OSS.","tags":["GitHub","OSS"],"title":"[GitHub] Wana be join OSS","type":"post"},{"authors":null,"categories":["Udemy","Know-how","Data Reliability Engineer"],"content":" Why SSIA\nWhat What is Kubernetes (K8s) https://kubernetes.io/ | ja | doc\nKubernetes is a portable, extensible, open-source platform for managing containerized workloads and services, that facilitates both declarative c- onfiguration and automation. It has a large, rapidly growing ecosystem. Kubernetes services, support, and tools are widely available. ref\nKubernetes, also known as k8s, can automate many of the manual processes involved in deploying, managing, and scaling containerized applications.\nSo what does Kubernetes actually provide?\nService discovery and load balancing - Kubernetes can expose a container using their domain name or using their own IP address. If traffic to a container is high, Kubernetes is able to load balance and distribute the network traffic so that the deployment is stable. Storage orchestration - Kubernetes allows you to automatically mount a storage system of your choice, such as local storages, public cloud providers, and more. Automated rollouts and rollbacks - You can describe the desired state for your deployed containers using Kubernetes, and it can change the actual state to the desired state at a controlled rate. For example, you can automate Kubernetes to create new containers for your deployment, remove existing containers and adopt all their resources to the new container. Automatic bin packing - You provide Kubernetes with a cluster of nodes that it can use to run containerized tasks. You tell Kubernetes how much CPU and memory (RAM) each container needs. Kubernetes can fit containers onto your nodes to make the best use of your resources. Self-healing - Kubernetes restarts containers that fail, replaces containers, kills containers that don’t respond to your user-defined health check, and doesn’t advertise them to clients until they are ready to serve. Secret and configuration management - Kubernetes lets you store and manage sensitive information, such as passwords, OAuth tokens, and SSH keys. You can deploy and update secrets and application configuration without rebuilding your container images, and without exposing secrets in your stack configuration. Hands-on Tutorial\nPlease check the following tutorial made by the Kubernetes official site. It provides a walkthrough of the basics of the Kubernetes cluster orchestration system. Each module contains some background information on major Kubernetes features and concepts with interactive tutorials. You will be albe to have basic ideas by managing a simple cluster and its containerized applications for yourself. Learn Kubernetes Basics Kubernetes Terminology and Concepts Before starting to use Kubernetes, we need to know some basic terminology and concepts in Kubernetes. Pod Pods are the smallest deployable units of computing that you can create and manage in Kubernetes. Each Pod is meant to run a single instance of a given application, like one API server. Each pod has its own yaml file which is the definition of this pod. A Pod can have one or more containers, with shared storage and network resources, and a specification for how to run the containers. Worker node The worker node(s) host the Pods that are the components of the application workload. Every node has three components: kubelet,kube-proxy and Container Runtime. kubelet - The primary “node agent” that runs on each node.It ensures all pods of the node are running and healthy. kube-proxy - A network proxy that runs on each node in your cluster which maintains network rules on nodes. These network rules allow network communication to your Pods from network sessions inside or outside of your cluster. Container runtime - The software that is responsible for running containers. Control Plane The control plane manages the worker nodes and the Pods in the cluster. kube-apiserver - The API server is a component of the Kubernetes control plane that exposes the Kubernetes API. The API server is the front end for the Kubernetes control plane. etcd - Consistent and highly-available key value store used as Kubernetes’ backing store for all cluster data. kube-scheduler - The scheduler monitors the newly created Pods that have not yet been specified on which Node to run, and coordinates the most suitable node to run the pod according to the resource regulations and hardware constraints on each node. kube-controller-manager - Responsible for managing and running the components of the Kubernetes controller. Simply put, the controller is the process in Kubernetes that is responsible for monitoring the status of the Cluster, such as Node Controller, Replication Controller. Cluster A Kubernetes cluster is made up of one control plane and several worker nodes. Networking Kubernetes defines a network model that helps provide simplicity and consistency across a range of networking environments and network implementations. The Kubernetes network model specifies: Every pod gets its own IP address Containers within a pod share the pod IP address and can communicate freely with …","date":1731904173,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1751615720,"objectID":"3d308ac0ceb409410ed6e2e6eb122360","permalink":"https://kithub.f5.si/post/kubernetes_catchup/","publishdate":"2024-11-18T13:29:33+09:00","relpermalink":"/post/kubernetes_catchup/","section":"post","summary":"⚓️ About container orchestration tecnology.","tags":["Kubernetes","Docker","CUE","YAML"],"title":"[Kubernetes] Catchup","type":"post"},{"authors":null,"categories":["Udemy","Hands-on","Data Analytics Engineer","Google"],"content":" Why SSIA\nWhat Udemy講座 BigQuery で学ぶ非エンジニアのための SQL データ分析入門 Udemy 当時の所感: 2021-09-10 はじめに この記事は、Udemyにて学習をした際のメモです。 コース受講したい場合は、ページ最下部に参考リンクとして掲載いたします。 学習メモ 次章からは、完全個人メモです。（後に自分で見返すようになっております） どんな考え方（ロジック）でクエリを書き上げたのか、どこで躓いたのか、 後に俯瞰したいためにお恥ずかしながら演習問題で間違えてしまったクエリはそのまま（コメントアウト）記載してます。 悪しからず... おわりに はじめて BigQueryに触れましたが、 標準SQLの奥の深さ、データを抽出することの難しさに新たな課題感を感じております。 ですが、本講座を通して、BigQueryの仕様や、SQLの基本的な文法は体得したのではないのかな？と思います。 （自分でも手が動くのに驚きを隠せませんでした。） 前半は基礎文法と地道ではありましたが、後半はパズルゲームを解く感覚に近くとても楽しんで取り組むことができました。 SECTION2~3：歴史と基本設定 データベース\nOSS （Open Source Softwear） MySQL、PostgreSQL、SQLitte …etc. PaaS （） Google BigQuery、Amazon Web Server、TreasureData …etc. 2種類のサービス形態がある。 平たくまとめるとお金を取るか、取らないか。無料で利用できるのがOSS。 BigQueryは無料枠もあるが、処理に応じて従量課金型。 SQLとは\n-- {テーブル名}から、{カラム名}に記載の列を取得してきて！って意味 SELECT {カラム名} FROM {テーブル名}; SQL生みの親 SQL（Structured Query Language：構造化された検索言語）の略で、古くはIBM社が1970年台に開発していたデータベース管理システムの制御用の言語、SEQUEL（Structured English Query Language）が元になっている。\ncf. The Relational Model Content based on Chapter 3 SlideToDoc\nSQLは、関係データベース管理システム において、データの操作や定義を行うためのデータベース言語、ドメイン固有言語である。プログラミングにおいてデータベースへのアクセスのために、他のプログラミング言語と併用される。\ncf. SQL Wekipedia\nGoogle BigQueryとは？\n-\u0026gt; ざっくり、Googleが提供するPaaS（Platform as a Service）のこと。 BigQuery（ビッグクエリ）は、ペタバイト単位のデータに対するスケーラブルな分析を可能にする、フルマネージドのサーバーレスのデータウェアハウスである。ANSI SQLを使用したクエリをサポートするPlatform as a Service（PaaS）としてGoogle Cloud Platformにより提供されている。また、機械学習の機能も組み込まれている。BigQueryは2010年5月に発表され、2011年11月に一般提供（GA）となった。\ncf. BigQuery Wikipedia\n適切なハードウェアとインフラストラクチャを用意せずに大規模なデータセットを保存し、それに対してクエリを実行すると、時間と費用がかかってしまいます。エンタープライズ データ ウェアハウスである Google BigQuery は、Google のインフラストラクチャの処理能力を活用して SQL クエリを超高速で実行し、こうした問題を解決します。データを BigQuery に読み込んだら、後の処理は Google にお任せください。また、データの表示やクエリの実行権限を自分以外のユーザーに付与できるため、業務上の必要に応じてプロジェクトや自分のデータに対するアクセスを制御することも可能です。\ncf. BigQuery とは GoogleCloud\nBigQuery環境構築\nBigQueryにアクセス https://cloud.google.com/bigquery/ 完成イメージとしては以下のような図。 外枠にプロジェクトがあり、その中にデータセットがあり、更にその中に各テーブル（Excelでいうタブ）がある。 入れ子（ネスト）構造。 プロジェクトを作成 データセットを作成 テーブルを作成 SECTION4 ：基本文法 記述順序\n1. SELECT ：取得する列の指定 2.（集計関数） 3. FROM ：取得するテーブルの指定 4. JOIN ：テーブルの結合の処理 5. ON / USING ：結合に利用するキー指定 6. WHERE ：絞り込み条件の指定 7. GROUP BY ：グループ化項目の指定 8. HAVING ：グループ化された集計結果に対して絞り込み条件の指定 9. ORDER BY ：並び替え条件 10. LIMIT ：表示する行数の指定 実行順序\n1. FROM 2. ON / USING 3. JOIN 4. WHERE 5. GROUP BY 6. (集計関数) 7. HAVING 8. SELECT 9. ORDER BY 10. LIMIT SELECT句 - かラム指定\n-- shop_purchasesテーブルからpurchase_id,user_id,dateを取り出してください。 SELECT purchase_id, user_id, date FROM bq_sample.shop_purchases ; -- | |purchase_id|user_id|date | -- |1| 22| 956425|2018-01-05| -- |2| 388| 937162|2018-04-30| DISTINCT句 - 重複除外\n-- shop_purchasesを利用して、固有のユーザーが何人いるかを取得。 SELECT DISTINCT user_id FROM bq_sample.shop_purchases ; -- | |user_id| -- |1| 956425| -- |2| 937162| EXCEPT() - かラム除外\n-- shop_purchasesテーブルから全カラムを取得。 SELECT * FROM bq_sample.shop_purchases; --70.1KB -- shop_purchasesテーブルからshop_id、product_idを除くカラムを取得。 -- ie. 抽出時に除外したいカラム名を記載。 SELECT * EXCEPT(shop_id, product_id) FROM bq_sample.shop_purchases; --50.1KB ORDERBY句 - ソート機能 e.g.\nORDER BY {カラム番号,カラム} (ASC:昇順 DESC:降順） … [1]indexに注意 -- shop_purchasesテーブルから、user_id、quantityを取得し、 -- quantityの大きい順に並び替える。 SELECT user_id, quantity FROM bq_sample.shop_purchases ORDER BY quantity DESC -- ORDER BY 2 DESC; ; -- | | user_id| quantity| -- |1| 971235| 5| -- |2| 992842| 5| -- shop_purchasesテーブルの全列を取得し、dateの古い順(日付順)に並べる。 -- もし、同じ日に複数行ある場合、sales_amountの大きい順に並べる。 -- さらに、sales_amountも同額であった場合には、quantityの大きい順に並べる。 SELECT * FROM bq_sample.shop_purchases --日付順(昇順) \u0026amp; 同日の場合売上高降順 \u0026amp; 数量降順 ORDER BY date, sales_amount DESC, quantity DESC ; -- [ASC] [DESC] [DESC] -- | |purchase_id|user_id|date |shop_id|product_id|quantity|sales_amount| -- |1| 6| 954830|2018-01-01| 1| 17| 3| 15488| -- |2| 1| 733995|2018-01-01| 2| 10| 1| 12775| -- shop_purchasesテーブルの全列を取得し、dateの新しい順（日付逆順）に並べる。 -- もし、同日複数のレコードがある場合には、sales_amountの小さい順に並べる。 -- 但、並べ替えには列の順序番号を利用すること。 SELECT * FROM bq_sample.shop_purchases ORDER BY 3 DESC, 7 ASC ; -- [DESC] [ASC] -- | |purchase_id|user_id|date |shop_id|product_id|quantity|sales_amount| -- |1| 1281| 873505|2018-12-31| 2| 7| 1| 7527| -- |2| 1280| 840135|2018-12-31| 1| 11| 2| 7527| -- |3| 1282|1118265|2018-12-31| 1| 15| 4| 16000| LIMIT句 - レコード制限\n※ 表示レコード数を制限するだけで、後ろの計算は全行行われている。 そのため発生料金に対して節約できるわけではない。 ex.【4.5 演習問題1 (0:55)】\nshop_purchasesテーブルから、全カラムを5行だけ取得してください。\nSELECT * FROM bq_sample.shop_purchases LIMIT 5; ex.【4.5 演習問題2 (2:23)】\nshop_purchasesテーブルから、date大きい順（＝新しい順）に、全カラムを10行だけ取得してください。\nSELECT * FROM `prj-test3.bq_sample.shop_purchases` ORDER BY date DESC LIMIT 10; OFFSET e.g.\nLIMIT [取得したい行数] OFFSET [取得をずらす行数]; ※ ORDERBY を使用前提 ex.【4.5 演習問題3 (:)】取得制限\nshop_purchasesテーブルから、purchase_id、sales_amountを取得。その際sales_amont降順に並べた上で11位から5レコードだけを取得。\nSELECT purchase_id, sales_amount FROM `prj-test3.bq_sample.shop_purchases` ORDER BY sales_amount DESC LIMIT 5 OFFSET 10; 行 purchase_id sales_amont 1 233 73000 2 286 72568 ※ 上位10行から、上位5行（＝5位）まで取得している | WHERE句 - 絞り込み条件 e.g.\nshop_purchasesテーブルからquantity列の３より大きい行を取得。\nSELECT * FROM \u0026#39;bq_sample.shop_purchases\u0026#39; WHERE quantity \u0026gt; 3; ※ WHERE句はFROM句の直後に記述する。 ※ 絞り込みをして …","date":1729869725,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1751615720,"objectID":"60adb75b3635cad149b9d42f36158445","permalink":"https://kithub.f5.si/post/bigquery_data_analysis_catchup/","publishdate":"2024-10-26T00:22:05+09:00","relpermalink":"/post/bigquery_data_analysis_catchup/","section":"post","summary":"🔍 Re-summarizeing the trajectory I learned as an introductory.","tags":["BigQuery","SQL","DWH","Data Mart"],"title":"[BigQuery] Data analyize catchup","type":"post"},{"authors":null,"categories":[""],"content":" Why SSIA\nWhat https://wezfurlong.org/wezterm/ | dw (macOS) | overview ~/ | (XDGベースのディレクトリ構造を利用) └ .config/ └ wezterm/ └ wezterm.lua \u0026lt;-推奨！ └ .wezterm.lua \u0026lt;-互換性のためにサポートされている場所 i.e. ベストプラクティスは、~/.config/wezterm/wezterm.lua $HOME 直下に配置されているwezterm.luaは不可視にしてもしなくてもOK UXの観点で、ファイル名を.(ドット)で始めると、UNIX系システムではファイルが不可視（隠しファイル）として扱われる。 -- Pull in the wezterm API local wezterm = require \u0026#39;wezterm\u0026#39; -- This table will hold the configuration. local config = {} -- In newer versions of wezterm, use the config_builder which will -- help provide clearer error messages if wezterm.config_builder then config = wezterm.config_builder() end -- Apply your config choices here -- Notification when config is reloaded wezterm.on(\u0026#39;window-config-reloaded\u0026#39;, function(window, pane) wezterm.log_info \u0026#39;the config was reloaded for this window!\u0026#39; end) -- Alt + 矢印キーをVimに送信する config.keys = { { key = \u0026#34;DownArrow\u0026#34;, mods = \u0026#34;ALT\u0026#34;, action = wezterm.action.SendKey { key = \u0026#34;DownArrow\u0026#34;, mods = \u0026#34;ALT\u0026#34; } }, { key = \u0026#34;UpArrow\u0026#34;, mods = \u0026#34;ALT\u0026#34;, action = wezterm.action.SendKey { key = \u0026#34;UpArrow\u0026#34;, mods = \u0026#34;ALT\u0026#34; } }, } -- フルスクリーン切り替えやペイン作成 config.keys = { { key = \u0026#39;t\u0026#39;, mods = \u0026#39;SHIFT|CTRL\u0026#39;, action = wezterm.action.SpawnTab \u0026#39;CurrentPaneDomain\u0026#39;, }, { key = \u0026#39;d\u0026#39;, mods = \u0026#39;SHIFT|CTRL\u0026#39;, action = wezterm.action.SplitHorizontal { domain = \u0026#39;CurrentPaneDomain\u0026#39; }, }, } -- Color scheme config.color_scheme = \u0026#39;Builtin Dark\u0026#39; -- Background opacity and blur config.window_background_opacity = 0.85 config.macos_window_background_blur = 15 -- Font config.font = wezterm.font(\u0026#34;HackGen\u0026#34;, {weight=\u0026#34;Medium\u0026#34;, stretch=\u0026#34;Normal\u0026#34;, style=\u0026#34;Normal\u0026#34;}) config.font_size = 14 -- Finally, return the config return config WezTerm Setup HandsOn WezTerm インストール（CLI） $ brew install --cask wezterm $ wezterm --version i.e. wezterm コマンドを利用できるようにする方法（以下いずれか） 環境変数（PATH）設定 $ echo export PATH=\u0026#34;/Applications/WezTerm.app/Contents/MacOS:$PATH\u0026#34; \u0026gt;\u0026gt; ~/.zshrc $ source ~/.zshrc シンボリックリンク設定 $ sudo ln -s /Applications/WezTerm.app/Contents/MacOS/wezterm /usr/local/bin/wezterm WezTerm.app がアプリケーションフォルダに存在する場合、（WebUIでダウンロードした場合） -\u0026gt; /Applications/WezTerm.app/Contents/MacOS/WezTerm にあるWezTermのバイナリのシンボリックリンクを作成。 設定ファイルの配置場所の確認 $ ls -l ~/.wezterm.lua 設定ファイルの構文エラーの確認 $ wezterm start --verbose 再起動 $ wezterm cli reload 更に良くしたいなら weztermのGitHub Discussionが盛んで、カスタマイズを参考にできる。 https://github.com/wez/wezterm/discussions/628\nRef. モテるターミナルにカスタマイズしよう（wezterm） Zenn 最高のターミナル環境を手に入れろ！WezTermに入門してみた。 DevelopersIO 君もイカしたTerminal環境で開発してみなイカ？(wezterm, zsh, starship, neovim) Zenn ","date":1729734032,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1751615720,"objectID":"55be86c38e349255ab1e39fc87b0a613","permalink":"https://kithub.f5.si/post/wezterm_setup/","publishdate":"2024-10-24T10:40:32+09:00","relpermalink":"/post/wezterm_setup/","section":"post","summary":"🟣 Would like to use the most powerful terminal that rumored on the street.","tags":["WezTerm","Lua","Terminal","CLI"],"title":"[WezTerm] Setup","type":"post"},{"authors":null,"categories":["Google"],"content":" Why SSIA\nWhat https://github.com/topics/google-sheets | doc https://github.com/topics/google-apps-script | doc Google Spread Sheets (以下：GSS) [GSS] 一括翻訳 googletranslate() = arrayformula( trim( transpose( split( googletranslate(textjoin(\u0026#34;. \u0026#34;, 1, $B$2:$B$1000), \u0026#34;en\u0026#34;, \u0026#34;ja\u0026#34;), \u0026#34;。 \u0026#34;, true ) ) ) ) ref\nNo Phrases and vocabulary Meaning in japanese 1 excess permissions f() 2 inventory confirmation deadline 在庫確認期限 3 escrow payment methods エスクロー支払い方法 cf. How can I use ARRAYFORMULA (or something similar) with GOOGLETRANSLATE? StackExchange\n[GSS] 他シートから取得 filter() Vlookup()のようにKeyを用いて、他シートのValueを取得したい。\n= iferror( filter( \u0026#39;ログ用Create文(サーバー)\u0026#39;!$A:$A, regexextract( \u0026#39;ログ用Create文(サーバー)\u0026#39;!$A:$A, \u0026#34;CREATE OR REPLACE TABLE\\s+`[^`]+\\.([^`]+)`\u0026#34; ) = $A1 ), \u0026#34;\u0026#34; ) [管理表]シート データ名 クエリ login f() registration [ログ用Create文(サーバー)]シート CREATE OR REPLACE TABLE pj.datamart.login (… CREATE OR REPLACE TABLE loud-dev-45a0b.datamart.registration (… … [GSS] 複数対象の文字置換 カラムの表記揺れを直している時、不要文字列を除外したい時に使える。 例えば、\nold acc mo ji -\u0026gt; oji o mji -\u0026gt; oji 1列目、1~2行いずれも「m」「space」を除外したい。 substituto() や replace() を連続使用すると割とゴチャにゴチャになる… そこで、\n=substitute(substitute(A1, \u0026#34;m\u0026#34;, \u0026#34;\u0026#34;), \u0026#34; \u0026#34;, \u0026#34;\u0026#34;) ↓\n= reduce( A1, {\u0026#34;m\u0026#34;, \u0026#34; \u0026#34;}, lambda(acc, old, substitute(acc, old, \u0026#34;\u0026#34;)) ) cf. SUBSTITUTEをネストせずに複数置換するには スプレッドシートで空白を詰める便利関数はある？ 使い方と事例を紹介 OneChatMedia [GSS] 複数ノイズを加味してカテゴライズ ifs() =if(or(O3=\u0026#34;\u0026#34;, O3=\u0026#34;対象外\u0026#34;, O3=\u0026#34;※データソースなし\u0026#34;, regexmatch(O3, \u0026#34;（※エラーのため*\u0026#34;)), \u0026#34;Null\u0026#34;, if(regexmatch(O3, \u0026#34;csv|xls*|excel*\u0026#34;), \u0026#34;serialize\u0026#34;, \u0026#34;database\u0026#34;)) ↓\n= ifs( isblank(O2), \u0026#34;Null\u0026#34;, O2=\u0026#34;※データソースなし\u0026#34;, \u0026#34;Null\u0026#34;, O2=\u0026#34;対象外\u0026#34;, \u0026#34;Null\u0026#34;, regexmatch(O2, \u0026#34;（※エラーのため\u0026#34;), \u0026#34;Null\u0026#34;, regexmatch(O2, \u0026#34;(?i)csv|xls|excel\u0026#34;), \u0026#34;serialize\u0026#34;, TRUE, \u0026#34;database\u0026#34; ) [GSS] 複数行を単一行へ flatten() =flatten(A1:B) - ds: dataset_name_item sa: aq-sq-000@item-attributes-jp-prod.iam.gserviceaccount.com - ds: dataset_name_shop sa: aq-sq-000@item-attributes-jp-prod.iam.gserviceaccount.com ↓\n- ds: dataset_name_item sa: aq-sq-000@item-attributes-jp-prod.iam.gserviceaccount.com - ds: dataset_name_shop sa: aq-sq-000@item-attributes-jp-prod.iam.gserviceaccount.com [GSS] 歯抜けの連番 = arrayformula( if(B2:B=\u0026#34;\u0026#34;, \u0026#34;\u0026#34;, scan(0, B2:B, lambda(acc, val, if(val\u0026lt;\u0026gt;\u0026#34;\u0026#34;, acc+1, acc)) ) ) ) # Labels Data source 1 AM_Daily_KPI KPI_ItemName_1.csv _ KPI_ItemName_2.csv _ KPI_ItemName_3.csv _ crn_d.crn_basic_kpi _ crn_d.m_game_series_titles 2 AM_Monthly_KPI KPI_ItemName_1.csv _ KPI_ItemName_4.csv 3 AM_Yearly_KPI … [WIP][GSS] 複数のファイル内特定シート統合 「A」 「B」 「C」 ↓ ↓ ↓ └--「D」--┘ ※　「」= GSSのファイルの意（便宜的） 前提として「A~C」ファイル内部シートのフォーマットは、統一していること （SQLの UNION ALL 縦型結合方式をとりたい為） [WIP][GSS] テーブル化（like Excel, like Pandas df） cf. Google スプレッドシートでテーブルを使用する Google Excel のアレ がキター！ 【超速報】Googleスプレッドシート テーブル機能まとめ note 「Google スプレッドシート」にテーブル機能が追加 窓の杜 Google Apps Script (以下：GAS) [GAS] 転置行列 /***** cf. https://docs.google.com/spreadsheets/d/1fQl5PQPaGcX4z9VP6gGkRHcBOku5Bn9_XfBpIzMPwXE/edit?gid=1244521797#gid=1244521797 *****/ function rearrangeData() { const sheet = SpreadsheetApp.getActiveSpreadsheet().getActiveSheet(); // アクティブなシートを取得 const data = sheet.getDataRange().getValues(); // シート内の全データを2次元配列として取得 let results = []; for (let i = 0; i \u0026lt; data.length; i += 2) { // データを2行ずつ処理 const code201 = data[i][0]; // 奇数行にある[201(found)]を取得 const code300 = data[i + 1] ? data[i + 1][0] : \u0026#39;\u0026#39;; // 偶数行にある[300(extracted)]を取得 results.push([code201, code300]); // [201][300]のペアを配列として保存 } // 結果をシートに書き込む（C列から） const startRow = 1; // 書き込み開始行 const startCol = 3; // 書き込み開始列（C列） sheet.getRange(startRow, startCol, results.length, 2).setValues(results); // 書き込み範囲に結果をセット } befor run no results 1 [201] 2 [300] 3 [201] 4 [300] after run no results results 1 [201] [300] 2 [201] [300] [GAS] 指定FMTでのクエリ自動生成 /***** cf. https://script.google.com/u/0/home/projects/1nQSocTOFaD_rYKe4kZWl9HJGHnQnH1LVxjv8bzsAUzB20EgbD3LiWrbf/edit?pli=1 *****/ const PROJECT_ID = \u0026#39;dev-project-XXXX\u0026#39;; const DATASET_ID = \u0026#39;datamart\u0026#39;; const TABLE_SUFFIX = \u0026#39;_20240801\u0026#39;; // メニュー追加 function onOpen() { let sheet = SpreadsheetApp.getActiveSpreadsheet(); let entries = [{ name: \u0026#34;Create文生成\u0026#34;, functionName: \u0026#34;generateBigQueryCreateStatements\u0026#34; }]; sheet.addMenu(\u0026#34;クエリ生成\u0026#34;, entries); } // メイン処理 function generateBigQueryCreateStatements() { try { let spreadsheet = SpreadsheetApp.openById(\u0026#39;~youer_sheet_id~\u0026#39;) let listSheet = spreadsheet.getSheetByName(\u0026#39;一覧\u0026#39;); let data = getSheetData(listSheet); // 対象テーブルごとに処理 let sqlStatements = data .filter(row =\u0026gt; isTableReady(row)) .map(row =\u0026gt; generateTableCreateStatement(spreadsheet, row)); // SQL文を別シートに出力 outputSqlStatements(sqlStatements); } catch (error) { Logger.log(`エラーが発生しました: ${error.message}`); } } // 一覧シートのデータを取得 function getSheetData(sheet) { let lastRow = sheet.getLastRow(); return sheet.getRange(2, 1, lastRow - 1, sheet.getLastColumn()).getValues(); } // 対象テーブルが対応済か確認 function isTableReady(row) { let statusColumnIndex = 12; // M列（実装ステータス） let tableNameColumnIndex = 6; // G列（テーブル物理名） let status = row[statusColumnIndex]; let tableName = row[tableNameColumnIndex]; return status === \u0026#39;対応済\u0026#39; \u0026amp;\u0026amp; tableName; } // 各テーブルのCREATE文を生成 function generateTableCreateStatement(spreadsheet, row) { let …","date":1729569382,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1751615720,"objectID":"712947b18106a91f889c16e9e46efff2","permalink":"https://kithub.f5.si/post/google_sheets_used_functions/","publishdate":"2024-10-22T12:56:22+09:00","relpermalink":"/post/google_sheets_used_functions/","section":"post","summary":"📗 Make a memorandum of what you used with the most powerful free tool that anyone can use.","tags":["Google Sheets","Connected Sheets","Google Apps Script"],"title":"[Google Sheets] Used in functions","type":"post"},{"authors":null,"categories":["Data Reliability Engineer","Google"],"content":" Why 実務をしていると様々な依頼がくる。\n品質について 「ある時点（eg.10/14）のデータが欠損しているように思われる。 原因究明してほしいのと、 できたら分析で使うから戻して（データ復旧して）ほしい！」\nガバナンスについて 「権限付与」\nデータ分析とはまた違う、データの品質やガバナンスを救う技術のｱﾚｺﾚをまとめたい。\nWhat 手法 用途 backfill 埋める timetravel 戻れる || 品質 | Backfill Backfillとは、データの過去分を遡って埋める作業を指す。\neg. 新しいデータパイプラインを導入した後に、そのパイプラインを使って過去のデータを処理・格納するために行われる。 データ欠損が発生した場合や、データが遅延している場合にもbackfillが使われる。 【4選】BigQueryでbackfillを行う:\nクエリベースで再計算: 過去のデータを対象とするクエリを実行し、その結果をテーブルに追加挿入（INSERT INTO）。 スケジューラの活用: DataflowやAirflowなどのワークフローオーケストレーションツールを使い、特定の過去の期間のデータを処理してBigQueryに取り込む。 Partitioned Tables: BigQueryのパーティションテーブルを使用して、特定の時間範囲のデータだけを効率よく処理。パーティションキー（通常は日時）を指定し、そのパーティション内のデータをbackfill。 Cloud FunctionsやPub/Sub: 外部のクラウドサービスやトリガーを使って、BigQueryにデータをbackfillも可能。 | Timetravel timetravelは、BigQueryの「過去のデータにアクセスする機能」。 BigQueryでは、データを挿入・更新・削除した後も、　最大7日間の履歴にアクセスできる機能が提供されている。 これにより、過去の状態のテーブルをクエリして復元•分析ができる。\n-- 1時間前の状態のデータにアクセス select * from `project.dataset.table` FOR SYSTEM TIME AS OF timestamp_sub(current_timestamp(), interval 1 hour) 期間制限: 過去7日間のみの履歴が保持されているため、それより前のデータにアクセスすることはできない。 クエリコスト: 通常のクエリと同様に、タイムトラベルクエリもデータのスキャンにコストがかかるため、注意が必要。 | CDC (Change Data Capture) CDCは、データベースの変更をリアルタイムでキャプチャし、他のシステムやデータストアに反映させる技術。 backfillとは異なり、データの変更を逐次反映するため、データの欠損や遅延が少なく、リアルタイム性が必要な場面に適している。\nbackfillの補完的: backfillでは過去のデータを埋めることが主目的、CDCは継続的にデータを同期が目的。両者を組み合わせることで、リアルタイムのデータストリームに欠損があった場合に後からbackfillで補完する、といった運用が可能。 運用の複雑さ: CDCを正確に実装するには、データの一貫性やレイテンシーを管理するための仕組みが必要。 | Snapshot スナップショットは、データベースやテーブルのある時点での状態を保存しておく方法。\n定期的にスナップショットを取得することで、timetravelのように過去のデータを参照できるが注意点がある。\ntimetravelの代替: BigQueryのtimetravelは最大7日間の履歴しか保持できないが、スナップショットはもっと長期間保存可能。定期的なスナップショットを取得しておけば、timetravel期間外の過去データにもアクセス可能。 ストレージコスト: スナップショットを頻繁に取得するとストレージのコストが増大！！！… 適切な保存期間と頻度の設計が重要。 | Partitioning パーティショニングは、テーブルを特定のカラム（日時など）に基づいて分割し、データ管理を効率化する手法。\n| Data Validation データの官署🚪 データバリデーションは、データの一貫性や正確性を検証するプロセス。 データパイプラインの各段階でバリデーションを行うことで、異常なデータが流入することを防ぐ。\n| Data Lineage データの探偵🕵️ データリンネージは、データの出所や変更履歴を追跡する仕組み。 どのようなプロセスを経てデータが現在の状態になったのかを把握できるため、データの透明性や信頼性を確保。\ntimetravelの補完: timetravelが特定の時点のデータを参照するのに対して、データリンネージはデータがどのように加工され、変わってきたかを追跡するため、過去の変更の影響を評価できる。 エラートラッキング: backfillやデータパイプラインで異常が発生した際、データリンネージを使用して、どの処理が原因だったのかを特定することが可能。 | Audit Logging (監査ログ) データの警察👮 監査ログは、データアクセスやクエリ実行の履歴を記録する機能。 BigQueryや他のクラウドサービスでは、どのユーザーがいつどのデータにアクセスしたかを追跡できるため、セキュリティやコンプライアンスの観点で重要。\n| ETL/ELT Pipeline データの抽出、変換、ロードのプロセス。\n用途：backfill時や、定期的なデータ更新 過去データの抽出と変換を実施し、適切な形でDWHにロードする一連の処理を自動化。 大規模なデータ処理が必要な場合、効率的にbackfillやデータ管理できるETLツールやデータパイプライン管理ツール ツール： Airflow, Dataflow, dbt …etc. || ガバナンス Ref. BigQuery の承認済みビューの権限を Terraform で管理して、優先順位の競合状態を回避 Google Cloud blog Authorized views and materialized views Google Cloud ","date":1729054194,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1751615720,"objectID":"e7f0031aaca6b617445c0ff4b8a31d80","permalink":"https://kithub.f5.si/post/bigquery_about_data_quality/","publishdate":"2024-10-16T13:49:54+09:00","relpermalink":"/post/bigquery_about_data_quality/","section":"post","summary":"🔍 Different perspective from data analysis. About data quality.","tags":["BigQuery","SQL","DWH"],"title":"[BigQuery] About data quality","type":"post"},{"authors":null,"categories":[""],"content":" Why macOS が大きな影響を受けているとのことでちょっときなるのと、\n以下の投稿で面白そうと思ったから。\nhttps://x.com/ko1nksm/status/1836280017816273303?s=46\u0026amp;t=RSRnjQg5szbGQ19iUHwMlg https://x.com/ko1nksm/status/1836371691402072560 Unixの起源の歴史につながる伝説のゲーム「スペース・トラベル」で遊んでみよう！ https://t.co/gb6zMpogWV #Qiita @ko1nksmより #UNIX #ゲーム #devsumi #Unix哲学 #SpaceTravel Space Travel\n— Koichi Nakashima (@ko1nksm) September 18, 2024 What Unixとは Profile 開発者：ケン・トンプソン、デニス・リッチー 特徴： cf. UNIX wikipedia Unix系OSの進化と派生 cf. UNIX Unixはオープンソースプロジェクトの発展に大きく寄与し派生している。\nLinux系: Unix互換のオープンソースOS。サーバーやデスクトップ、組み込みシステムで広く使用されている BSD系OS: Berkeley Software Distribution (BSD) から派生したOS群。FreeBSD、OpenBSD、NetBSD、Debian…etc. macOS: AppleのmacOSはUnixに基づいており、POSIX互換性を持つ一方で独自のGUIとエコシステムを備えている UnixからmacOSへ Unixよりも前のOS Unixが登場する以前には、いくつかのオペレーティングシステム (OS) が存在しており、コンピュータの発展に重要な役割を果たしていたそうで 以下はUnix以前に影響力を持っていて、 これらのシステムは、コンピュータの進化において重要なステップであり、特にタイムシェアリングや仮想メモリ、バッチ処理などの概念がUnixの設計に大きな影響を与えました。\nGM-NAA I/O (1956) 開発者: General MotorsとNorth American Aviation 特徴: 初期のバッチ処理OSで、IBM 704コンピュータ向けに開発されました。バッチ処理システムは、ジョブを自動的に連続して処理するもので、入力と出力を効率化しました。 Atlas Supervisor (1962) 開発者: マンチェスター大学 特徴: Atlasコンピュータ向けに開発され、世界で初めて仮想メモリの概念を導入したOS。仮想メモリ技術により、物理メモリ容量を超えるプログラムの実行が可能になりました。 CTSS (Compatible Time-Sharing System) (1961) 開発者: MIT（マサチューセッツ工科大学） 特徴: 世界初のタイムシェアリングシステムの1つ。タイムシェアリングとは、複数のユーザーが同時にシステムを使用できる機能で、CPU時間を分割して利用者間で共有します。Unixのタイムシェアリングにも影響を与えました。 Multics (1965) 開発者: MIT、ベル研究所、GE（General Electric） 特徴: Unixに直接的な影響を与えたOSで、当時の最も野心的なOSプロジェクトの1つ。セキュリティやファイルシステム、仮想メモリ管理における革新的な技術を導入しました。Multicsは非常に複雑であったため、後にUnixがそれを簡素化した設計で作られることになります。 IBM OS/360 (1964) 開発者: IBM 特徴: メインフレームコンピュータ向けの商業用OS。IBM System/360用に開発され、非常に広範囲に使用されました。バッチ処理だけでなく、タイムシェアリングやリアルタイム処理もサポートしました。OS/360は業務用コンピュータシステムの基盤となりました。 BESYS (1950年代) 開発者: ベル研究所 特徴: ベル研究所の内部で使用されたバッチ処理システム。Unixを開発する研究者たちがBESYSを利用していたため、Unixの誕生に間接的に影響を与えました。 The Exec系列 (1950年代後半 - 1960年代) 開発者: UNIVAC（Universal Automatic Computer） 特徴: タイムシェアリングおよびバッチ処理をサポートするOS。Exec IIやExec 8は、初期のメインフレームで使われました。 Ref. Unixの歴史の起源を伝説のゲーム「スペース・トラベル」で遊んで学ぼう！ Qiita プログラミング言語の歴史【訂正版作成予定】 Youtube UNIX UNIXとは？その特徴やLinuxとの違いを解説 ﾚﾊﾞﾃｯｸｷｬﾘｱ Ref. Books UNIXという考え方 ［改訂第4版］シェルスクリプト基本リファレンス ――#!/bin/shで、ここまでできる (Tech × Books plusシリーズ) ","date":1727706484,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1751615720,"objectID":"780c1a62d636cf80ccdbb5d82c5c6df8","permalink":"https://kithub.f5.si/post/macos_unix_is_ancestor/","publishdate":"2024-09-30T23:28:04+09:00","relpermalink":"/post/macos_unix_is_ancestor/","section":"post","summary":"🩶 About the legacy of inventions from a previous era that heavily influenced macOS.","tags":["Unix","macOS","Linux"],"title":"[macOS] UNIX is ancestor","type":"post"},{"authors":null,"categories":[""],"content":" Why remote -[main] repository ----------------------------------------------------\u0026gt; | ❶/fork ↓ remote -[main] repository ----------------------------------------------------\u0026gt; | ❶/clone　↑ ⑤/push | ↓ ↑ ③/commit -- ↑ ④/commit -┘ ↓ ❻/pull local [main] branch ----------|------------- |----------------------\u0026gt; | ❷/chackout　| /add | /add └-\u0026gt; Aさん[feature_A] branch ------┘ What Official: https://git-scm.com | doc | com | dw GitHub: https://github.com/GIT ワークフローという哲学 ワークフロー名 種類別ブランチ名 対応規模 GitHub flow 2種(master/main, feature） 個人 ~ 小規模 Git-flow 5種(master/main, develop, feature, release, hot-fix) 大規模 企業規模や、OSS単位ごとで使用される粒度が異なる。（独自なところもあるが上記2つが主流の流派）\n# 現在`master`ブランチに居ることを確認して $ git branch * master $ git cheackout -b feature/edit_hoge ie.\nfeature/{任意}} ブランチ名を切り出す。 わざわざ明記しなくとも動く。ただ、このように明示的にしていることで開発にかかる他の方に迷惑を被ることを回避できる。 cf.\nGitHub flow GitHub 【入門】Github Flowとは？使い方の基本 カゴヤのサーバー研究室 What is Gitflow? Atlassian Git-flowをざっと整理してみた DevoloperIO 手元に幹を（植え込み🪴） remote repository -[main] -------\u0026gt; | /fork | /clone ↓ | remote repository [main] ---|---\u0026gt; | /clone | ↓ ↓ local repository [main] -------\u0026gt; cf. リポジトリをフォークする GitHub リポジトリのcloneとforkの違い Qiita Githubリポジトリのfork（フォーク/派生）の使い方・clone（クローン）との使い分け カゴヤのサーバー研究室 幹から枝を生やす # main/master branch から新たにブランチを切り出したい。 local -[main] branch ------------------------\u0026gt; └├--\u0026gt; Aさん[feature_A] branch ---\u0026gt; └--\u0026gt; Bさん[feature_B] branch ---\u0026gt; $ git checkout -b feature_B 他の人の枝を手元にも # remote -[main]-------------------------✨---------------------\u0026gt; | |Aさんの[feature_A] branch [A] local -[main] branch -----------------|-\u0026gt; | └-\u0026gt; Aさん[feature_A] branch -----┘ | | [B] local -[main] branch ---------------------|------------------\u0026gt; └-\u0026gt; Bさん[feature_A] branch # 他の人のブランチの最新の変更を取得して、リモート上のブランチ名を一覧確認 $ git fetch origin $ git branch -r # feature_Aブランチを自分のローカルにチェックアウト $ git checkout -b feature_A origin/feature_A $ git branch # 変更を加え、コミット＆プッシュ $ git add . $ git commit -m \u0026#34;プルリクエストのための変更\u0026#34; $ git push origin feature 枝から枝を # main/master branch に merge されていない 他の人の branch に対してプルリクエストを作成したい。 local -[main] branch ------------------------------------------\u0026gt; └-\u0026gt; Aさん[faeture_A] branch -----------------------\u0026gt; └-\u0026gt; Bさん[feature_B] branch -┘ # 他の人のブランチの最新の変更を取得するためにリモートリポジトリからフェッチ $ git fetch origin \u0026lt;その他の人のブランチ名\u0026gt; # 他の人のブランチをローカルに持ってくるために、チェックアウト $ git checkout -b \u0026lt;新しいブランチ名\u0026gt; origin/\u0026lt;その他の人のブランチ名\u0026gt; # 変更を加え、コミット＆プッシュee $ git add . $ git commit -m \u0026#34;プルリクエストのための変更\u0026#34; $ git push origin \u0026lt;新しいブランチ名\u0026gt; 枝から枝を（名前変更） # branch名を[feature_B] → [feature_C] に変えたい local -[main] branch -------------------------------------------------\u0026gt; └-\u0026gt; Aさん[faeture_A] branch ------------------------------\u0026gt; └-\u0026gt; Bさん[feature_B] branch -\u0026gt; [feature_C] branch ----\u0026gt; # 現在ブランチ確認 $ git branch * feature_B main # 現在ブランチ名をリネーム $ git branch -m feature_C # 新しいブランチ名をリモートにプッシュ反映 $ git push origin feature_C # 古いブランチ名はリモートから削除（後片付け） $ git push origin --delete feature_B # 他の開発者への通知 $ git fetch origin 大きく育った枝を剪定 # [feature_A] の修正ファイル数が膨大になり分割して、切り出しor並び替えしたい -[main] branch -------------------------------------------\u0026gt; └-\u0026gt; Aさん[faeture_A] branch　| | | |/rebase | | | └-\u0026gt; [new-branch-1] branch -┘ | | └-\u0026gt; [new-branch-2] branch ---|-┘ └-\u0026gt; [new-branch-3] branch ---┘ └-\u0026gt; [new-branch-4] branch ------× コミット分割（※ 必要に応じて） # コミットをいくつかに分割したい場合、インタラクティブリベースでコミットを分割 # （ n は分割したいコミットの数） $ git rebase -i HEAD~n 分割したいコミットの行を edit に変更。(エディタが開き、コミット履歴が表示されます。) pick e7f6c98 First commit edit d4f8a7b Second commit pick 7a7f6e3 Third commit # editにしたコミットが表示されたら、そのコミットを分割 # コミットを一旦解除 $ git reset HEAD^ $ git add \u0026lt;ファイル名\u0026gt; $ git commit -m \u0026#34;細かく分けたコミット1\u0026#34; $ git add \u0026lt;別のファイル名\u0026gt; $ git commit -m \u0026#34;細かく分けたコミット2\u0026#34; すべてのコミットを分割した後、リベースを続行 $ git rebase --continue ブランチ分割 異なるPRを作成するために、複数のブランチに変更を振り分け。 新しいブランチを作成 # 現在ブランチから分割する新しいブランチを作成。 $ git checkout -b new-branch-1 特定のコミットを含むようにブランチを操作 新しいブランチに特定のコミットだけを残すために、git resetやgit cherry-pickを使う。 eg. # 新しいブランチに特定のコミットだけを残したい場合 $ git reset --hard \u0026lt;特定のコミットハッシュ\u0026gt; # 不要なコミットを削除したい場合（新しいブランチに残したくないコミットがあれば） $ git rebase -i 各ブランチでPR作成 それぞれのブランチで、GitHubにプッシュして新しいPRを作成。 $ git push origin new-branch-1 GitHub上で、この新しいブランチに基づいたPRを作成します。 次に、他のブランチも同様にプッシュし、それぞれにPRを作成します。 古くなった枝を最新に remote -[main] branch -----------------------------------------\u0026gt; |/pull |/pull local -[main] branch ---------------------|------------|------\u0026gt; | | | | ↓/merge ↓/rebase └\u0026gt; Aさん[faeture_A] branch ---✨-----------✨------\u0026gt; [Merge] 簡単かつ安全（でも履歴汚ねぇ…）\n最新のmain（またはdevelop）ブランチを取得 $ git checkout main $ git pull origin main 作業ブランチに移動し、マージ $ git checkout feature_A $ git merge main (if) マージコンフリクトの解消 $ git add \u0026lt;conflicted-files\u0026gt; $ git commit -S -m \u0026#34;memo\u0026#34; 変更をリモートにプッシュ $ git push origin feature_A [Rebase] 綺麗な履歴\nmainブランチの最新のコミットの後に来るように並べ替えることができる。 これにより、コミット履歴が綺麗な状態で最新コードを取り込むことができる。 一方で、特に共有ブランチに対して行う場合は強制プッシュが必要になる。（他の開発者に影響が出る可能性あり）\n$ git checkout main $ git pull origin main $ git checkout feature_A $ git rebase main (if) リベース中にコンフリクトが発生した場合 $ git add \u0026lt;conflicted-files\u0026gt; $ git rebase --continue # 中断したい場合 $ git rebase --abort リモートに強制プッシュ $ git push --force-with-lease origin feature_A 空コミットの要請 $ git commit --allow-empty -m \u0026#34;メッセージ\u0026#34; cf. git commit –allow-empty で空コミットを作成し …","date":1727168888,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1751615720,"objectID":"2b59ee12ceec1c29f251377ab4d31dbb","permalink":"https://kithub.f5.si/post/github_branches_pruning/","publishdate":"2024-09-24T18:08:08+09:00","relpermalink":"/post/github_branches_pruning/","section":"post","summary":"📍When developing on GitHub, I often switch branches, just like pruning.","tags":["Git","GitHub","CLI"],"title":"[Git] Branches pruning","type":"post"},{"authors":null,"categories":[""],"content":" Why 自作プラグインを作っってみたい。\nWhat Howto Build Vimのプラグインは自作可能です。 Vimスクリプトを使えば、Vimの機能を拡張したり、自分好みの機能を追加することができます。 作成手順としては、以下の流れで進められます。\nVimスクリプトの知識 基本的なVimのコマンドや設定（:set ,:map …etc.）の理解 Vimスクリプトの構文（if, function, autocmd…etc.）の理解 ie. Vimプラグイン開発入門 Zenn プラグインのディレクトリ構成 一般的なVimプラグインは以下のような構成で作成される。 (eg. GitHub上で管理する場合、プラグイン名でディレクトリを作り、以下のようなサブディレクトリを配置) myplugin/ ├── autoload/ \u0026lt;-関数が初めて呼び出されたときにロードされるスクリプト ├── doc/ \u0026lt;-プラグインのメイン機能を定義するスクリプト ├── plugin/ \u0026lt;-プラグインのメイン機能を定義するスクリプト ├── syntax/ \u0026lt;-プラグインのメイン機能を定義するスクリプト ├── README.md └── LICENSE プラグインの機能定義 基本的な設定やコマンドを定義。 function! MyPluginFunction() echo \u0026#34;Hello from my plugin!\u0026#34; endfunction command! MyPlugin call MyPluginFunction() :MyPlugin と入力することで関数が実行され、「Hello from my plugin!」と表示される。 プラグインの公開 作成したプラグインはGitHubなどに公開して、他のユーザーがインストールして使えるようにすることができます。 README.mdにインストール手順や使い方を記載しておくと良いです。 Vimプラグインの作成は、最初はシンプルな機能から始めて、徐々に機能を拡張していくのが良いでしょう。 自作している方々 vim-textmanip Winresizer nin-english.vim 便利なプラグイン一覧 dein.vim unite.vim Molokai Indent Guides NERDTree fugitive.vim Search Multiple Search vim-quickhl vim-visualstar ctrlp.vim 開発補助 emmet-vim neocomplete.vim Neosnippet closetag.vim tcomment surround.vim vim-textmanip DrawIt vim-prettyprint html5validator syntastic watchdogs.vim vim-operator-sort vim-operator-comment Ref. 機能別おすすめVimプラグイン25選！入れ方も解説 【Vim/NeoVimマクロ】作成/保存/追記/定義/格納場所など使用例も解説 ","date":1726051189,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1751615720,"objectID":"356e3db96dfecea55ac51c3c87092ef7","permalink":"https://kithub.f5.si/post/vim_building_plugin/","publishdate":"2024-09-11T19:39:49+09:00","relpermalink":"/post/vim_building_plugin/","section":"post","summary":"✅ Once I gain a deeper understanding, what may be able to develop general-purpose automated tools.","tags":["Vim","Vim script","CLI"],"title":"[Vim] Building plugin","type":"post"},{"authors":null,"categories":["Hands-on","Data Pipline"],"content":" Why SSIA\nWhat Argo は「K8s上での(バッチ)ジョブ実行」に特化 短期・高負荷ジョブ（数分～数時間の処理）に最適 YAMLベースのワークフロー定義 Airflowより軽量だが、スケジューリング機能は弱い Argo Workflows が向くケース Kubernetes上で短期・高負荷ジョブを実行（例: 「100並列で画像処理」） 機械学習パイプライン（TensorFlow/PyTorchジョブのオーケストレーション） CI/CDパイプライン（テスト→ビルド→デプロイの自動化） Argo Projects Argo(アルゴ）は、2017年にApplatixで開発された。 その後、Applatixは2018年にIntuitによって買収された。 2020年4月にCNCFのインキューベーション・プロジェクト（支援）になった。\nアルゴプロジェクトは、ワークフロー、デプロイ、ロールアウト、およびイベントなど、 ジョブやアプリケーションをデプロイし、実行するためのKubernetes-ネイティブツールのセットを提供。 継続的デリバリー、累進的デリバリーなどの GitOps パラダイムによって、KubernetesでMLOps つまり機械学習のDevOps基盤を推進。\nTekton Pipeline や Trigger のタスクの１ステップに、アルゴを組み込むことができるので、 Tektonのトリガーで受けた git push の イベントから、Argo CDにつないでアプリケーションの状態管理など連携運用も可能。\nこの活動は複数あるが、主要な4つのグループをまとめると以下\nArgo Workflows Kubernetesクラスター上でワークフローを定義・実行できるオープンソースのシステム。（ワークフローは、複数のタスクが順序や依存関係に基づいて実行されるプロセスのこと） 各タスクはPod（Kubernetes上のコンテナ）として実行されるため、分散処理が簡単に実現でき、大規模なデータ処理や複雑な計算フローにも対応可能。 メリット コンテナベース 各ステップが独立したコンテナとして実行されるため、タスクの依存関係を管理しやすい リソースの分離も可能 スケーラビリティ 負荷に応じてワークフローのタスクを自動的にスケールさせることができる（Kubernetesのスケーリング機能活用） 再現性 ワークフロー定義はYAMLファイルで記述され、バージョン管理が可能 同じワークフローを再実行したり、環境間での移植が容易 可視化とモニタリング ワークフローの状態や進行状況をWeb UIで可視化 失敗したタスクの再実行も Web UIで簡単に行える 依存関係の管理 タスク同士の依存関係をDAG（Directed Acyclic Graph）で表現でき、順次処理や並列処理を柔軟に設定可能 ワークフロー構造（サンプル）ref apiVersion: argoproj.io/v1alpha1 kind: Workflow metadata: generateName: hello-world- spec: entrypoint: main templates: - name: main steps: - - name: print-hello template: echo - name: echo container: image: alpine:latest command: [echo, \u0026#34;Hello, Argo!\u0026#34;] cf. https://argo-workflows.readthedocs.io/en/latest/ Artifacts Management in Container-Native Workflows (Part 1) Quick Start Argo CD(Continuous Development) cf. Getting Started Argo Rollouts cf. Getting Started demo Argo Events cf. Introduction cf. Argo とはなんだ？ Qiita\n^ IBMの方がより簡潔にまとめられている! Local Hans-on Goal 「embulkでローカルにあるcsvファイルを読み取って、内容をフィルターして標準出力に出すところまで」 Prep kubernetesクラスタのセットアップ(ローカル) $ brew install kubectl $ kubectl version --client Client Version: v1.29.2 Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3 cf. kubectlの基本操作をまとめてみた。 Qiita Argo Workflowsのインストール ref # Namespaceの作成 $ kubectl create ns argo # 作成したNamespaceにArgoをインストールする $ kubectl apply -n argo -f https://raw.githubusercontent.com/argoproj/argo/stable/manifests/quick-start-postgres.yaml NAME READY STATUS RESTARTS AGE argo-server-74d5fbc96f-9gc2n 1/1 Running 0 2m2s minio-58977b4b48-m9c5l 1/1 Running 0 2m1s postgres-6b5c55f477-pbbv8 1/1 Running 0 2m1s workflow-controller-7fb47d49bb-t2mrb 1/1 Running 0 2m Argo CLIのインストール $ argo version argo: v3.3.6 BuildDate: 2022-05-26T01:07:24Z GitCommit: 2b428be8001a9d5d232dbd52d7e902812107eb28 GitTreeState: clean GitTag: v3.3.6 GoVersion: go1.17.10 Compiler: gc Platform: darwin/amd64 Implementation（実装） Overview . ├── Dockerfile ├── argo/ │ ├── workflow.yaml │ └── workflow_template.yaml └── data/ └── customer.csv.gz DockerfileにEmbulkの実行環境、フィルタリングに必要なpluginも同時にインストールするよう記述。 FROM openjdk:8-jre-alpine ARG VERSION RUN mkdir -p /root/.embulk/bin \\ \u0026amp;\u0026amp; wget -q https://dl.embulk.org/embulk-${VERSION}.jar -O /root/.embulk/bin/embulk \\ \u0026amp;\u0026amp; chmod +x /root/.embulk/bin/embulk ENV PATH=$PATH:/root/.embulk/bin RUN apk add --no-cache libc6-compat \u0026amp;\u0026amp; embulk gem install embulk-filter-column Embulkを実行する Templateは下記のように記述。 (※ Argo Workflows はこのように処理をテンプレ化させておいて、各所で再利用することが可能らしい。) apiVersion: argoproj.io/v1alpha1 kind: WorkflowTemplate metadata: name: embulk-template spec: entrypoint: embulk templates: - name: embulk inputs: artifacts: - name: embulk-config path: /input/config.yml container: image: embulk:v0.1 command: [java] args: [\u0026#34;-jar\u0026#34;, \u0026#34;/root/.embulk/bin/embulk\u0026#34;, \u0026#34;run\u0026#34;, \u0026#34;/input/config.yml\u0026#34;] volumeMounts: - name: data mountPath: /data volumes: - name: data configmap: name: customer-data embulkを実行する本体の処理は以下。embulkのconfigはとりあえずartifactsとして渡す。 apiVersion: argoproj.io/v1alpha1 kind: Workflow metadata: name: csv-to-stdout-sample spec: entrypoint: embulk arguments: artifacts: - name: embulk-config raw: data: | in: type: file path_prefix: \u0026#39;data/\u0026#39; decoders: - {type: gzip} parser: charset: UTF-8 newline: CRLF type: csv delimiter: \u0026#39;,\u0026#39; quote: \u0026#39;\u0026#34;\u0026#39; escape: \u0026#39;\u0026#34;\u0026#39; null_string: \u0026#39;NULL\u0026#39; skip_header_lines: 1 columns: - {name: customer_id, type: string} - {name: customer_name, type: string} - {name: gender_cd, type: string} - {name: gender, type: string} - {name: birth_day, type: timestamp, format: \u0026#39;%Y-%m-%d\u0026#39;} - {name: age, type: long} - {name: postal_cd, type: string} - {name: address, type: string} - {name: application_store_cd, type: string} - {name: application_date, type: string} - {name: status_cd, type: string} filters: - type: column columns: - {name: customer_id, type: string} - {name: gender_cd, type: string} - {name: gender, type: string} - {name: birth_day, type: timestamp, format: \u0026#39;%Y-%m-%d\u0026#39;} - {name: age, type: long} - {name: postal_cd, type: string} - {name: application_store_cd, type: string} - {name: application_date, type: string} - {name: status_cd, type: string} out: type: stdout workflowTemplateRef: name: embulk-template Execution DockerfileはイメージとしてArgo …","date":1725853259,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1751615720,"objectID":"411a4d24f5ca2c42e9d63c2cd8112075","permalink":"https://kithub.f5.si/post/argo_workflows_catchup/","publishdate":"2024-09-09T12:40:59+09:00","relpermalink":"/post/argo_workflows_catchup/","section":"post","summary":"🐙 Understanding and implementing Argo projects and workflows.","tags":["DAG","Argo workflows","Kubernetes","YAML","ETL/ELT"],"title":"[Argo Workflows] Catchup","type":"post"},{"authors":null,"categories":["Data Analytics Engineer","Google"],"content":" Why 分析関係でお仕事をしていたり、最近では営業担当や、マーケティング担当の方々も容易にBigQueryで用いるのがDQL。 他にも様々な区分のなコマンドが存在している。\nコマンド区分の理解と、用いてきた関数等を覚書していきたい。\nWhat || SQL Commands Classification DDL: Data Define Language CREAT ALTER DROP RENAME TRUNCATE DML: Data Manipulation Language INSERT UPDATE DEFELR MERGE DCL: Data Control Language GRANT REVOKE DQL: Data Query Language SELECT || SQL Types Type SQL 文 句 SELECT, FROM, WHERE, 式 CASE, || Used Functions | [小技] テーブルフラット化 #standardSQL /* cf. * GA4/Firebaseのログをフラット化する汎用クエリ * https://www.marketechlabo.com/ga4-firebase-log-preprocessing/ */ with #importTable GA4_EVENTS as (select * from `bigquery-public-data.ga4_obfuscated_sample_ecommerce.events_*`) #preprocessTable --ログフラット化 , GA4_LOG_FLAT as ( select user_pseudo_id , event_name as EVENT_NAME , (select -- 如何なる型を、文字列に纏め上げ case when p.value.string_value is not NULL then safe_cast(p.value.string_value as string) when p.value.int_value is not NULL then safe_cast(p.value.int_value as string) when p.value.double_value is not NULL then safe_cast(p.value.double_value as string) else NULL end from unnest(event_params) p where p.key = \u0026#39;page_title\u0026#39;) as PAGE_TITLE from GA4_EVENTS ) #outputTable select * from GA4_LOG_FLAT; フラット化を動的処理 #standardSQL /* cf. * GA4/Firebaseのログをフラット化する汎用クエリ * https://www.marketechlabo.com/ga4-firebase-log-preprocessing/ */ #config declare str_ep_columns string; declare str_up_columns string; declare str_dynamic_columns string; --event_params set str_ep_columns = ( with #importTable GA4_EVENTS as (select * from `bigquery-public-data.ga4_obfuscated_sample_ecommerce.events_*`) #preprocess , TYPE_CHACK as ( select KEY , case when 0 \u0026lt; CNT_STRING then \u0026#39;string\u0026#39; when 0 \u0026lt; CNT_INT64 and 0 \u0026lt; CNT_FLOAT64 then \u0026#39;numeric\u0026#39; when 0 \u0026lt; CNT_INT64 then \u0026#39;int64\u0026#39; when 0 \u0026lt; CNT_FLOAT64 then \u0026#39;float64\u0026#39; else \u0026#39;string\u0026#39; end TYPE --型 from ( select p.key as KEY , sum(case when p.value.string_value is not null then 1 else 0 end) as CNT_STRING , sum(case when p.value.int_value is not null then 1 else 0 end) as CNT_INT64 , sum(case when p.value.double_value is not null then 1 else 0 end) as CNT_FLOAT64 from GA4_EVENTS, unnest(event_params) as p group by 1 ) ) , GET_LOG_FLAT as ( select /* --下記のクエリを「KEY」の数だけ生成 * (select * case * when p.value.string_value is not null then safe_cast(p.value.string_value as string) * when p.value.int_value is not null then safe_cast(p.value.int_value as string) * when p.value.double_value is not null then safe_cast(p.value.double_value as string) * else null * end * from unnest(event_params) p where p.key = \u0026#34;all_data\u0026#34;) as all_data */ string_agg( \u0026#39;(select case when p.value.string_value is not null then safe_cast(p.value.string_value as \u0026#39; || TYPE || \u0026#39;) when p.value.int_value is not null then safe_cast(p.value.int_value as \u0026#39; || TYPE || \u0026#39;) when p.value.double_value is not null then safe_cast(p.value.double_value as \u0026#39; || TYPE || \u0026#39;) else null end from unnest(event_params) p where p.key = \u0026#34;\u0026#39; || KEY || \u0026#39;\u0026#34;) as \u0026#39; || KEY order by KEY --※順序規定 ) from TYPE_CHACK ) select * from GET_LOG_FLAT ); --user_properties set str_up_columns = ( with #impoerTable GA4_EVENTS as (select * from `bigquery-public-data.ga4_obfuscated_sample_ecommerce.events_*`) #preprocess , TYPE_CHACK as ( select KEY , case when 0 \u0026lt; CNT_STRING then \u0026#39;string\u0026#39; when 0 \u0026lt; CNT_INT64 and 0 \u0026lt; CNT_FLOAT64 then \u0026#39;numeric\u0026#39; when 0 \u0026lt; CNT_INT64 then \u0026#39;int64\u0026#39; when 0 \u0026lt; CNT_FLOAT64 then \u0026#39;float64\u0026#39; else \u0026#39;string\u0026#39; end TYPE --型 from ( select p.key as KEY , sum(case when p.value.string_value is not null then 1 else 0 end) as CNT_STRING , sum(case when p.value.int_value is not null then 1 else 0 end) as CNT_INT64 , sum(case when p.value.double_value is not null then 1 else 0 end) as CNT_FLOAT64 from GA4_EVENTS, unnest(user_properties) p group by 1 ) ) , GET_LOG_FLAT as ( select /* --下記のクエリを「KEY」の数だけ生成 * (select * case * when p.value.string_value is not null then safe_cast(p.value.string_value as string) * when p.value.int_value is not null then safe_cast(p.value.int_value as string) * when p.value.double_value is not null then safe_cast(p.value.double_value as string) * when p.value.set_timestamp_micros is not null then safe_cast(p.value.set_timestamp_micros as string) * else null * end * from unnest(user_properties) p where p.key = \u0026#34;all_data\u0026#34;) as all_data */ string_agg( \u0026#39;(select case when p.value.string_value is not null then safe_cast(p.value.string_value as \u0026#39; || type || \u0026#39;) when p.value.int_value is not null then safe_cast(p.value.int_value as \u0026#39; || type || \u0026#39;) when p.value.double_value is not null then safe_cast(p.value.double_value as \u0026#39; || type || \u0026#39;) when p.value.set_timestamp_micros is not null then safe_cast(p.value.set_timestamp_micros as \u0026#39; || type || \u0026#39;) else null end from unnest(user_properties) p where p.key = \u0026#34;\u0026#39; || key || \u0026#39;\u0026#34;) u_\u0026#39; || key order by KEY --順序規定 ) from TYPE_CHACK ) select * from GET_LOG_FLAT ); #aggregation if 0 \u0026lt; length(str_up_columns) then set str_dynamic_columns = str_ep_columns || \u0026#39;, \u0026#39; || str_up_columns; else set str_dynamic_columns = str_ep_columns; end if; #output execute immediate format(\u0026#34;\u0026#34;\u0026#34; …","date":1724928464,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1751615720,"objectID":"e44ae9e80925024cbf85f170637acfdd","permalink":"https://kithub.f5.si/post/bigquery_used_functions/","publishdate":"2024-08-29T19:47:44+09:00","relpermalink":"/post/bigquery_used_functions/","section":"post","summary":"🔍 When I getting from my daily work. Added as need. I keep writing various tips.","tags":["BigQuery","SQL"],"title":"[BigQuery] Used in functions","type":"post"},{"authors":null,"categories":["Data Reliability Engineer","Data Pipline"],"content":" Why SSIA\nWhat Trocco とは Official: https://trocco.io/ LP: https://trocco.io/lp/index.html GitHub: https://github.com/trocco-io Trocco 転送ログ eg.\n[Spanner] ------\u0026gt; 「 Troccco -\u0026gt; [BigQuery] [DataSource] ---\u0026gt; 」 2024-08-20 15:06:00.298 +0000 Preparing your trocco environment... 2024-08-20 15:06:00.347 +0000 cpu_request=3.5, memory_request=15.0Gi, cpu_limit=3.5, memory_limit=15.0Gi, disk=200.0Gi Successfully created your environment Loading Java Agent version 1 (using ASM9). 2024-08-20 15:07:27.489 +0000: Embulk v0.9.26 2024-08-20 15:07:27.971 +0000 [WARN] (main): DEPRECATION: JRuby org.jruby.embed.ScriptingContainer is directly injected. 2024-08-20 15:07:29.165 +0000 [INFO] (main): BUNDLE_GEMFILE is being set: \u0026#34;/work/embulk_bundle/Gemfile\u0026#34; 2024-08-20 15:07:29.166 +0000 [INFO] (main): Gem\u0026#39;s home and path are being cleared. 2024-08-20 15:07:30.636 +0000 [INFO] (main): Started Embulk v0.9.26 2024-08-20 15:07:30.808 +0000 [INFO] (0001:transaction): Loaded plugin embulk-input-gcs (0.3.4) 2024-08-20 15:07:32.458 +0000 [INFO] (0001:transaction): Loaded plugin embulk-output-bigquery (0.6.9.trocco.0.1.0) 2024-08-20 15:07:32.470 +0000 [INFO] (0001:transaction): Loaded plugin embulk-filter-speedometer (0.3.6) 2024-08-20 15:07:32.482 +0000 [INFO] (0001:transaction): Loaded plugin embulk-filter-column (0.7.1) 2024-08-20 15:07:32.492 +0000 [INFO] (0001:transaction): Loaded plugin embulk-filter-typecast (0.2.2) 2024-08-20 15:07:33.305 +0000 [INFO] (0001:transaction): Using local thread executor with max_threads=8 / output tasks 4 = input tasks 1 * 4 2024-08-20 15:07:33.327 +0000 [INFO] (0001:transaction): embulk-output-bigquery: Get dataset... sample-dev-45a0b:amount_master_data 2024-08-20 15:07:33.329 +0000 [WARN] (0001:transaction): embulk-output-bigquery: timeout_sec is deprecated in google-api-ruby-client \u0026gt;= v0.11.0. Use read_timeout_sec instead 2024-08-20 15:07:34.085 +0000 [INFO] (0001:transaction): embulk-output-bigquery: Create table... sample-dev-45a0b:amount_master_data.LOAD_TEMP_32196187_5779_43e4_a4e5_9caa550aa7cb_act_present_item 2024-08-20 15:07:34.257 +0000 [INFO] (0001:transaction): embulk-output-bigquery: Create table... sample-dev-45a0b:amount_master_data.act_present_item 2024-08-20 15:07:34.380 +0000 [INFO] (0001:transaction): {done: 0 / 1, running: 0} 2024-08-20 15:07:34.423 +0000 [INFO] (0020:task-0000): {speedometer: {active: 0, total: 0.0b, sec: 0.00, speed: 0.0b/s, records: 0, record-speed: 0/s}} 2024-08-20 15:07:34.633 +0000 [INFO] (0020:task-0000): embulk-output-bigquery: create /tmp/bigquery_output_option_20240820-1-exorlf.16.2000.jsonl.gz 2024-08-20 15:07:34.642 +0000 [INFO] (0020:task-0000): {speedometer: {active: 0, total: 30.0b, sec: 0.22, speed: 137b/s, records: 1, record-speed: 4/s}} 2024-08-20 15:07:34.643 +0000 [INFO] (0001:transaction): {done: 1 / 1, running: 0} 2024-08-20 15:07:34.650 +0000 [INFO] (Ruby-0-Thread-1: /work/embulk_bundle/jruby/2.3.0/gems/embulk-output-bigquery-0.6.9.trocco.0.1.0/lib/embulk/output/bigquery/bigquery_client.rb:153): embulk-output-bigquery: Load job starting... job_id:[embulk_load_job_a2b456a3-99f7-4152-8a66-8bc28e994f79] /tmp/bigquery_output_option_20240820-1-exorlf.16.2000.jsonl.gz =\u0026gt; sample-dev-45a0b:amount_master_data.LOAD_TEMP_32196187_5779_43e4_a4e5_9caa550aa7cb_act_present_item in asia-northeast1 2024-08-20 15:07:34.651 +0000 [WARN] (Ruby-0-Thread-1: /work/embulk_bundle/jruby/2.3.0/gems/embulk-output-bigquery-0.6.9.trocco.0.1.0/lib/embulk/output/bigquery/bigquery_client.rb:153): embulk-output-bigquery: timeout_sec is deprecated in google-api-ruby-client \u0026gt;= v0.11.0. Use read_timeout_sec instead 2024-08-20 15:07:35.845 +0000 [INFO] (Ruby-0-Thread-1: /work/embulk_bundle/jruby/2.3.0/gems/embulk-output-bigquery-0.6.9.trocco.0.1.0/lib/embulk/output/bigquery/bigquery_client.rb:153): embulk-output-bigquery: Load job checking... job_id:[embulk_load_job_a2b456a3-99f7-4152-8a66-8bc28e994f79] elapsed_time:7.4e-05sec status:[RUNNING] 2024-08-20 15:07:45.899 +0000 [INFO] (Ruby-0-Thread-1: /work/embulk_bundle/jruby/2.3.0/gems/embulk-output-bigquery-0.6.9.trocco.0.1.0/lib/embulk/output/bigquery/bigquery_client.rb:153): embulk-output-bigquery: Load job completed... job_id:[embulk_load_job_a2b456a3-99f7-4152-8a66-8bc28e994f79] elapsed_time:10.056669999999999sec status:[DONE] 2024-08-20 15:07:45.900 +0000 [INFO] (Ruby-0-Thread-1: /work/embulk_bundle/jruby/2.3.0/gems/embulk-output-bigquery-0.6.9.trocco.0.1.0/lib/embulk/output/bigquery/bigquery_client.rb:153): embulk-output-bigquery: Load job response... job_id:[embulk_load_job_a2b456a3-99f7-4152-8a66-8bc28e994f79] response.statistics:{:start_time=\u0026gt;1724166455760, :completion_ratio=\u0026gt;1, :creation_time=\u0026gt;1724166455640, :end_time=\u0026gt;1724166456927, :reservation_id=\u0026gt;\u0026#34;default-pipeline\u0026#34;, :total_slot_ms=\u0026gt;152, :load=\u0026gt;{:input_file_bytes=\u0026gt;116, :output_bytes=\u0026gt;73, :output_rows=\u0026gt;1, :bad_records=\u0026gt;0, :input_files=\u0026gt;1}} 2024-08-20 15:07:45.902 +0000 [INFO] (0001:transaction): embulk-output-bigquery: Get table... …","date":1724221846,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1751615720,"objectID":"268752e90b1614a28288874abfb0a7b1","permalink":"https://kithub.f5.si/post/trocco_and_embluk_catchup/","publishdate":"2024-08-21T15:30:46+09:00","relpermalink":"/post/trocco_and_embluk_catchup/","section":"post","summary":"🎢 Summarizing Trocco and Embluk to deepen understanding.","tags":["Trocco","SaaS","Embluk","OSS","ETL/ELT"],"title":"[Trocco][Embluk] Catchup","type":"post"},{"authors":null,"categories":["Hands-on"],"content":" Why SSIA\nWhat Docker Docker Hub Docker Desktop on Mac Setup $ brew install docker インストール cf.brew $ docker --version バージョン確認 $ docker login ログイン $ docker logout ログアウト Docker コマンド一覧 Docker File $ docker build . Dockerfileの格納されているディレクトリ上で実行（. はcdの意） $ docker build -t {name} {directory} 名前指定してビルド Docker Image $ docker images イメージ一覧 $ docker pull {image} イメージ取得（eg. DockerHub → DockerImage） $ docker build {directory} イメージ化（DockerFile → DockerImage） $ docker run -it {image} bash イメージ実行（コンテナ起動）　※「pull」「start」を内部的に実行 $ docker run -it --rm {image} bash コンテナ起動後に削除 $ docker run -it -v {host}:{container} {imege} ホストのファイルシステムをコンテナにマウント e.g. $ docker run -it -v ~/host/mounted_folder:/new_dir {image} bash docker run -u {UserId}:{UserGroup} ユーザーID、グループ名を指定してコンテナ作成 $ docker run -it -u $(id -u):$(id -g) -v ~/host/mounted_folder:/new_dir {image} bash $ id -u PCのユーザーID確認 $ id -g PCのグループ名確認 $ docker run -p {host_port}:{container_port} ホストのポートをコンテナポートに繋げる $ docker run -it -p 8888:8888 --rm jupyter/datascience-notebook bash $ docker --cpus {# of CPUs} コンテナがアクセスできるCPUI上限確認 $ docker --memory {byte} $ docker run -it --rm --cpus 2 --memory 2g ubuntu bash $ sysctl -n hw.physicalcpu_max 物理コア数 $ sysctl -n hw.logicalcpu_max 論理コア数 $ sysctl hw.memsize メモリ（byte） $ docker rmi {image} イメージ削除 $ docker rmi $(docker images -q) 全イメージ削除 Docker Container $ docker ps -a コンテナ一覧（ps=process status） $ docker inspect {container} コンテナのあらゆる情報確認 $ docker inspect 5f90be76cd31 | grep -i cpu CPU数やメモリ量等確認時 （|grep=抽出 -i=ignore大文字小文字問わず {検索語句} ） $ docker restart {container} コンテナ再起動 $ docker exec -it {container} bash コンテナ実行 $ docker stop {container} コンテナ停止 $ docker stop $(docker ps -q) 全コンテナ停止 $ docker rm {container} コンテナ削除 $ docker system prune、docker rm $(docker ps -q -a) 全コンテナ削除 $ docker run --name {name}{imagename} コンテナ名付け $ docker run -d {imagename} detached mode コンテナ起動後にdetachする（バックグラウンドで動かす） $ docker run --rm {imagename} foreground mode コンテナをExit後に削除する（使い捨てコンテナ用） $ docker commit {imageid/name} {new_imagename(:tag)} コンテナ更新 $ docker tag {new_imagename(:tag)} {target} コンテナ名変更 $ docker push {imagename} コンテナをDockerHubへ Dockerfile HandsOn Overview ローカルやクラウドで、データ分析環境を容易に構築するためのDockerfile\n# Host (Local or AWS/GCP) docker/ ┝ dsenv_build/ │ └ Dockerfile \u0026lt;- ここ作る └ ds_python/ \u0026lt;- 任意名 # Docker Containar (root)/ ┝ bin/ ~ ┝ opt/ │ └ anaconda3 └ work/ \u0026lt;-「ds_python」と紐づいている (eg.「test.ipnb」Jupyterlabなどで分析する場所） 分析環境構築 構築（ローカル） ディレクトリ作成 \u0026amp; 移動 $ mkdir docker \u0026amp;\u0026amp; cd docker Dockerイメージ作成してコンテナ起動 $ docker build . $ docker run -p 8888:8888 -v ~/docker/ds_python:work --name my_lab {image} 構築（クラウド: AWS） SSHでクラウドに接続（セキュアにログイン） $ ssh -i hoge.pem ubuntu@ec2-00-100-200-300.ap-northeast-1.compute.amazonaws.com nb. 予めAWSに（EC2サーバー）インスタンスを作成して、そこにログインしておく。 hoge.pem AWSのアクセス認証鍵で名前は任意。（鍵のあるディレクトリで実行） ec2-{~}.compute.amazonaws.com PublicDNSは都度取得。 クラウド上にDockerCE（ComunicatEdition）をインストール (aws)$ sudo apt-get update (aws)$ sudo apt-get install docker.io (aws)$ docker --version (aws)$ sudo gpasswd -a ubuntu docker #sudo無しでdockerコマンド使用する為 nb. DockerEE（EnterprizeEdition）はガチの奴だから今回はCE Dockerimageをクラウドに送る ※　ネットワークに繋がる状況下か否かでケース分している [case.1] クラウドがネットワークに繋がれるなら SFTPでローカルからクラウドへファイル転送（セキュアに） $ sftp -i hoge.pem ubuntu@ec2-00-100-200-300.ap-northeast-1.compute.amazonaws.com (aws)$ put ~/docker/dsenv_build/Dockerfile home/ubuntu buildcontextを作成して、ファイル移動 $ ssh -i hoge.pem ubuntu@ec2-00-100-200-300.ap-northeast-1.compute.amazonaws.com (aws)$ mkdir dsenv_build (aws)$ mv Dockerfile dsenv_build dockerイメージ作成 (aws)$ cd dsenv_build (aws)~/dsenv_build$ docker build . dockerコンテナ、JupyterLab起動。 (aws)~dsenv_build$ docker run -v ~:/work -p 8888:8888 {imageID} @ec2-00-100-200-300.ap-northeast-1.compute.amazonaws.com:8888 にブラウザーからアクセスする。\n[case.2] クラウドがネットワークに繋がれないなら ローカルでDockerimageをtarで圧縮 $ docker build . $ docker save {imageID} \u0026gt; myimage.tar nb. MacのM1チップ使用時は $ docker build --platform linux/amd64.\nSFTPでローカルからクラウドへファイル転送（セキュアに） $ sftp -i hoge.pem ubuntu@ec2-00-100-200-300.ap-northeast-1.compute.amazonaws.com (aws)$ put ~/docker/myimage.tar home/ubuntu (aws)$ docker load \u0026lt; myimage.tar #tarファイルを解凍 nb. $ docker load で解凍され、同時にイメージが作られる。\nクラウド上でDockerコンテナ起動 (aws)$ docker run -it {imageID} bash Dockerfile作成 FROM ubuntu:latest RUN apt-get update \u0026amp;\u0026amp; apt-get install -y \\ sudo \\ wget \\ vim WORKDIR /opt RUN wget https://repo.anaconda.com/archive/Anaconda3-2019.10-Linux-x86_64.sh \u0026amp;\u0026amp; \\ sh Anaconda3-2019.10-Linux-x86_64.sh -b -p /opt/anaconda3 \u0026amp;\u0026amp; \\ rm -f Anaconda3-2019.10-Linux-x86_64.sh ENV PATH /opt/anaconda3/bin:$PATH WORKDIR / RUN pip install --upgrade pip CMD [\u0026#34;jupyter\u0026#34;, \u0026#34;lab\u0026#34;, \u0026#34;--ip=0.0.0.0\u0026#34;, \u0026#34;--allow-root\u0026#34;, \u0026#34;--LabApp.token=\u0026#39;\u0026#39;\u0026#34;] nb. $ jupyter lab で起動 --ip 0.0.0.0 ローカルホストの意味 --allow-root ローカルで実行の為ルート権限（※ 別サーバーでならユーザー指定するべき） --LabApp.token トークン指定でセキュリティを高める。（ローカル上の為未指定） 書き方 FROM FROM {dockerimage} # OS等々指定 RUN RUN {linux comand} # やりたいこと eg. RUN touch test RUN echo `hello world` \u0026gt; test RUNの他に、COPY、ADDがレイヤーを作成するインストラクション \u0026amp;\u0026amp;: コマンド結合 \\: 改行 apt: ubuntuのパッケージ管理(パッケージインストール) RUN apt-get install {package} RUN apt-get update: 最新版取得 (注意) RUN を複数書き …","date":1724166034,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1752842406,"objectID":"9b211cb53baa9784a4127dc30f217ccf","permalink":"https://kithub.f5.si/post/docker_commands_memorandum/","publishdate":"2024-08-21T00:00:34+09:00","relpermalink":"/post/docker_commands_memorandum/","section":"post","summary":"🐋 Deepen my understanding of Docker's basic commands and mechanisms.","tags":["Docker","CLI","YAML"],"title":"[Docker] Commands memorandum","type":"post"},{"authors":null,"categories":[],"content":" Why SSIA\nWhat Command エディタ起動 $ vim ( $ view Viewモードを使って、ファイルの書き換えミスの保存やアクシデントなどを防ぎたい時に便利！） [V] ビジュアルモード v 複数文字や行を選択できるモード [I] インサートモード i: ファイルに書き込むためのモード a 次の文字からインサートモードに切替え o カーソルの下に空白を入れ、インサートモードに切替え 0 カーソル行に空白を入れ、インサートモードに切替え [:] コマンドモード : 保存 / 終了\n:w 上書き保存 :wq 保存して終了 :q 編集終了 :q! 保存をせずに終了 削除\n:%d: 一括削除（ファイルの中身を空の状態にする） ジャンプ（移動）\n:${数字} 指定した行数に移動 eg. :5 5行目に移動 :10 10行目に移動 :set number 行数を表示 :set nonumber 行数を非表示 コマンドを実行\n:!{$コマンド} eg. :!ls lsコマンド（一覧表示） :!python3 test.py Pythonファイルを実行 :!ruby test.rb Rubyファイルを実行 :!! 前のコマンドを実行 複数ファイルオープン -:e ファイルパス 指定したファイルを開く。:editの略。（eg.:e ~/.vimrc）\nバッファ（=開いたファイルのこと）の一覧から、オープン\n:ls バッファの一覧表示。 :bn 次のバッファを表示。:bnextの略 :bp 次のバッファを表示。:bpreviousの略 :b# 直前まで開いていたバッファを表示。 :b[N] 該当の番号のバッファを表示。番号はファイルを開いた順に振られる。（eg. :b2） 画面分割\n:sp 画面を上下に分割する。:splitの略 :vs 画面を左右に分割する。:vsplitの略 置換\n:s/old/new 該当行の最初の一致箇所を置換 :s/old/new/g 該当行の一致箇所を全置換 :1,10s/old/new/g 1〜10行目の一致箇所を全置換 :%s/old/new/g ファイル全体の一致箇所を全置換 :%s/${検索ワード}/${置換ワード}/g : 一括置換 :%s/${検索ワード}/${置換ワード}/gc: 確認しながら置換 :\u0026#39;\u0026lt;,\u0026#39;\u0026gt;s/{old}/{new}/g [V] visual-modeで選択内 一括置換 cf. Vimの置換コマンドの使い方 Memo on the web vim 文字列置換 基本的な事 Qiita [ ] ノーマルモード [esc]: 基本移動（カーソル移動）\nk ↑ 上に移動 j ↓ 下に移動 h ← 左に移動 l → 右に移動 特殊移動\n0 行頭に移動 ^ / $ インデントの先頭/行末に移動 { / } ひとつ上/下の段落に移動 [[ / ]] ひとつ上/下の空白行に移動 gg / G ファイルの先頭/最後に移動 ctrl + o 移動前に戻る ウィンドゥ間の移動\n\u0026lt;C-w\u0026gt;h カーソルを1つ左のwindowに移動 \u0026lt;C-w\u0026gt;j カーソルを1つ下のwindowに移動 \u0026lt;C-w\u0026gt;k カーソルを1つ上のwindowに移動 \u0026lt;C-w\u0026gt;l カーソルを1つ右のwindowに移動 \u0026lt;C-w\u0026gt;w カーソルを1つ前のwindowに移動 戻る(undo) \u0026amp; 進む(redo)\nu ひとつ前の状態に戻す = その行でインデント位置を自動修正 ctrl + r 直前の操作に進む 削除\nx 1文字削除 dd 1行削除 2dd 2行削除 3dd 3行削除 dw 単語ごとに削除 コピー(ヤンク) \u0026amp; ペースト(プット)\nyy（ヤンク）1行コピー {数字}yy （ヤンク）複数行コピー eg. 2yy 2行コピー 3yy 3行コピー p（プット）カーソルの下にペースト yyp コピペ ddp カット＆ペースト J カーソル行と下の行を連結する 検索 \u0026amp; 置換\n\\${ワード} 検索 n / N 次/前の検索結果に移動 R 置換 ? 逆方向検索 :set is 部分マッチ検索 :set hls 検索マッチする文字強調表示 画面分割\n\u0026lt;C-w\u0026gt;s 画面を上下に分割。ctrlを押しながらwと入力し、その後にsを入力、の意味 \u0026lt;C-w\u0026gt;v 画面を上下に分割。ctrlを押しながらwと入力し、その後にvを入力、の意味 折り畳みの操作方法\nza 現在の折り畳みを開閉（toggle）する zc 現在の折り畳みを閉じる zo 現在の折り畳みを開く zM 全ての折り畳みを閉じる zR 全ての折り畳みを開く Convenience 一括操作：コメントアウト Ctrl + v 矩形（くけい）選択モードで行をバッと選択 Shift + i コメントアウト文字を入力（e.g. #、--、//…etc.） Esc 押下後にバッと入力されるっ！ cf. vimを使わないviで一括コメントアウト Qiita 【Vim】複数行に一括でコメントアウト Qiita 一括操作：インデント削除 [:] コマンドモード :20,25\u0026lt; [v] ビジュアルモード Ctrl + v 矩形選択 5j 行選択(５行下方選択) \u0026lt;\u0026lt; 一括操作：文字サイズ変更 [:] コマンドモード :1,10gu 1~10行目 :%gu ファイル全体 [v] ビジュアルモード Ctrl + v 範囲指定 gu 小文字 gU 大文字 cf.\nVim Cheat Sheet Ref. Vimの基本的なコマンドリスト Qiita Vim幼稚園からVim小学校へ Qiita はじめてのVim 〜 Vimはいいぞ！ゴリラと学ぶVim講座(1) さくらのナレッジ テキストファイルに対してSQLを発行できるツール「q」\u0026amp; vimから使う「vimq.vim」の紹介 Zenn vimtutorで速習Vim 今更ながらvimチートシート Qiita 慣れてきた頃に知りたいVimの便利機能 Zenn Vim ( \u0026amp; ideavim ) キーマップ設定ガイド Qiita 【vimめも】 3. レジスタ Qiita テキストエディタ「vim」入門 envader ","date":1723824123,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1751615720,"objectID":"96d56c47329fe36e020af1afb4608450","permalink":"https://kithub.f5.si/post/vim_commands_memorandum/","publishdate":"2024-08-17T01:02:03+09:00","relpermalink":"/post/vim_commands_memorandum/","section":"post","summary":"✅ A list of vim commands for the TextEditor tool, along with various tips.","tags":["Vim","TextEditor","IDE","CLI"],"title":"[Vim] Commands memorandum","type":"post"},{"authors":null,"categories":["Data Reliability Engineer","Google"],"content":" Why 「タダ(無料)じゃないんだよ！…」お金のかかるお話はどうも苦手ですが、、 しかし、よく扱うからこそ BigQuery の課金の仕組み等をキチン🍗と知っておきたいとそう思う訳で。\nWhat Billing price List（料金表） Computing (分析料金) Billing Fee Free 1. Query(オンデマンド) $7.50/TB 1TB/mm 2. Query(月定額) $2,400/100slots - 3. Query(年定額) $2.040/100slots - 4. BQeditions(Standard) $0.051/slots(h) - 5. BQeditions(Enterprise) $0.076/slots(h) - 6. BQeditions(EnterprisePlus) $0.128/slots(h) - Strage (ストレージ料金) Billing Fee Free 1. Activ (論理) $0.023/GB 10GB/mm 2. Longterm (論理) $0.016/GB 10GB/mm 3. Active (物理) $0.052/GB 10GB/mm 4. Longterm (物理) $0.026/GB 10GB/mm cf. BigQuery の料金 GoogleCloud Slots / Reservation ザックリ買い占めたい人ならSLOT単位でまとめ買い オンデマンド料金モデル スロットと分析容量を明示的に制御不可能 専用 or 自動スケーリングされたクエリ処理容量に対して支払う。 ie. 一時的なバースト容量を備えたプロジェクトごとのスロットの割り当てが適用 アカウントで使用しているスロットの数を確認するには、BigQuery のモニタリング 容量ベースの料金モデル スロットと分析容量を明示的に制御可能 予約するスロット数を明示的に選択。クエリはその容量内で実行され、デプロイされる 1 秒ごとに容量に対して継続的に支払う eg. BigQueryスロットを 2,000個 購入した場合、 集計クエリは、任意の時点で 2,000個の仮想 CPU を使用する状態に制限。 削除するまで 2,000スロットに対して支払う。（この容量は削除するまで保持される） cf. Work with slot reservations (ja) GoogleCloud Understand slots (ja) GoogleCloud Slots and reservations BigQueryのスロットの購入・予約・割り当てをSQLだけで実施する DevelopersIO Ref. BigQuery の料金 GoogleCloud BigQuery エディションの概要 GoogleCloud クエリ費用を管理する GoogleCloud 費用の見積もりと管理 GoogleCloud Google BigQuery の料金体系を解説 電算ｼｽﾃﾑ ","date":1723808107,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1751615720,"objectID":"eb16b1effbc2438c182b051fd279dc9b","permalink":"https://kithub.f5.si/post/bigquery_billing_how_works/","publishdate":"2024-08-16T20:35:07+09:00","relpermalink":"/post/bigquery_billing_how_works/","section":"post","summary":"🔍 Summarize billing mechanism, fee structure, and money-saving tips.","tags":["BigQuery","Google Cloud"],"title":"[BigQuery] Billing how works","type":"post"},{"authors":null,"categories":["Free-lance","Know-how"],"content":"Why フリーランスはあまり関係ないかもだが、\n割と長いこと購入した書籍をPDF化（自炊）して処分・整理している。\n書籍の自炊をする際に使用している道具・アプリのまとめ的な。\nWhat 自炊\n自炊手助けツール 使用ツール 溶解機器（必要に応じて）: とじたくん amazon.co.jp 裁断機: CARL DC-210N amazon.co.jp スキャナー: ScanSnap iX1500 amazon.co.jp PDF管理アプリ: Booklover cf. だすまんちゃん直伝！本や教科書を裁断・自炊してPDF化する方法 本の自炊（PDF化）〜管理方法(iPhone, iPadユーザー向け) 様々な課題 どこで管理する？\ncf. 無期限にデータを保存するならドコ？オンラインストレージ比較 著作権侵害大丈夫？\n私的利用の範囲なら◯〜△ cf. 本の自炊iCloudなどで保存する場合の著作権侵害について。本を購入し、… AI活用 cf. 物理本を scan \u0026amp; OCR で NotebookLMに読み込ませる方法 Qiita NotebookLM×スキャンピーで学習を効率化！PDF化した本をフル活用 NotebookLM⑧　画像PDFから読み込む Note その他 https://github.com/zuiurs/jisui ","date":1723801317,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1752919461,"objectID":"2088fb3ffc1f6d2a57accbeb99ccb28d","permalink":"https://kithub.f5.si/post/other/freelance_how_to_jisui/","publishdate":"2024-08-16T18:41:57+09:00","relpermalink":"/post/other/freelance_how_to_jisui/","section":"post","summary":"📚 Converting cut-up books into digital data. Also, an article on how to cut books and store the data.","tags":["PDF","NotebookLM"],"title":"[Freelance] How to Jisui \u003c自炊\u003e","type":"post"},{"authors":null,"categories":["Know-how"],"content":" Why 普段ブラウザー（e.g. Google Chrome）を使用していて思うことがある。\nよく使うWebアプリケーションをタブ分割ではなく、ネイティブアプリケーションのように使用したいと。 せめてデスクトップ上だけでも。。\nWhat Google Chromeのデフォルト機能を用いて（エセ）ネイティブアプリをDockに追加していく。 PWAの技術とはまた違う。 How 想定環境\nGoogle Chrome（2024/08現在の最新版） MacBook (Chromeが動けばどのマシンでもいけると思う） 手順\nアプリケーション化 Google Chrome で任意のWebアプリケーションにアクセス。 [≡]右上設定ファイルが畳まれているボタン \u0026gt; [Save and Share] \u0026gt; [Install page as App…] \u0026gt; [Install] 格納先（e.g.）：$HOME/Applications/Chrome Apps/[page].app アイコン変更（任意） 作成された[page].app を右クリック \u0026gt; [Show Package Contents] Contents/Resources/app.icns を同名、同拡張子で置換。 いい感じにアイコン作りたい時は以下が便利 .png ↔︎.icns なら cloudconvert 何でもできる なら BANNER KOUBOU Ref. ","date":1722668658,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1751615720,"objectID":"3b0625f812618c7727223f88c9bb2620","permalink":"https://kithub.f5.si/post/macos_looks_like_native_application/","publishdate":"2024-08-03T16:04:18+09:00","relpermalink":"/post/macos_looks_like_native_application/","section":"post","summary":"🍏 A little trick to use it like a native laptop application.","tags":["macOS","Chrome"],"title":"[macOS] Looks like native application","type":"post"},{"authors":null,"categories":[""],"content":" Why 別に知らなくても生きていけるが、知っていると楽しいよ！ってことらしいので。\n（適時増やしていく）\nWhat cf.\n用語調べる e-Words IT用語辞典 Computing Abbreviations 省略形調べる Acronym Finder Top Acronyms and Abbreviations Dictionary 略語一覧 コーディングして、変数名決めたい時。 Readmeにドキュメント残したり読んだりしている時。 GitHubでプルリク出してコードレビューしたり、されたりラジ...ry ▼ よく見る\nPR (pull request) プルリク SSIA (subject says it all) 掲載(掲題)の通り LGTM (looks good to me) いい感じ PTAL (please take another look) 再度ご確認ください NB/N.B. (nota bene / note well) 特に注意、備考 i.e. (id est / that is) すなわち c.f./cf. (confer / compare) ○○を参照 e.g. (exempli gratia / for example) たとえば ex (extra) 特別な、元◯◯ tldr/tl;dr/tl/dr (Too long, didn’t read) 長い文を読みたくない人向けの要約 GCP (Google Cloud Platform) GKE (Google Kubernetes Engine) GA (General Availability)一般提供 TF (Terraform / Tensol Flow) K8S/K8s (Kubernetes) gr8 (Great) LOL (Laugh out loud) HTH (Hope this help) OSS (Open source soft) f/w (Framework) THX (Thanks) ▼ コミュニケーション（Slack, Gmail…etc.）\nRE: (regarding、reply) 返信: R.S.V.P (respondez s’ il vous plait) ご返信ください N.A. (not available / not applicable) 該当なし/使用不可 Pls (Please) ASAP (As soon as possible) なるはやで FYI (for your information) ご参考までに FYIG (for your information and guidance) 情報および指針として、ご参考までに IMO (in my opinion) 私の意見では EOB (end of business day) 終業時間までに COD (close of business）終業時間までに IAC (in any case) とにかく/いずれにしても WFH (work from home) 在宅勤務 OOO/OoO (out of office) 不在 AFK (away from keyboard) 離席します ⌨️ GJ (Good job) CTN (Can’t talk now) J/K (Just kidding) 冗談 w/e (Whatever) ▼ (対)経営層\n経営用語 1~4Q (Quoter) 四半期決算 経営指標 ROI () GMV (Gross Merchandise Value) 流通取引総額 ref KPI/KGI () 目標管理フレームワーク OKR (Objectives and Key Results) KPT () MBO () PDCA () ▼ （対）マーケター層\nマーケティング指標・用語 SEM (Search Engine Marketing) SEO (Search Engine Optimization) LPO (Landing Page Optimization) PPC () CV (Conversion) 獲得数 PV (Page View) 閲覧数 SS (Session) アクセス数 IMP (Impression) 表示回数 CVR (Conversio Rate) CPA () CTA () ASL (Age / Sex / Location) RFM (Recency / Frequency / Monetary) 最終購入日/購入頻度/購入金額 ref LTV (Life / Time / Value) GA (Google Analytics) GTM (Google Tag Manager) 略語分類（学術的） 分類 定義 例 アクロニム 複数の単語の頭文字を組み合わせた略語で、通常はそのまま単語のように発音される。 NATO（North Atlantic Treaty Organization）「ナトー」 イニシャリズム アクロニムと似ているが、頭文字を一文字ずつ発音する略語。 FBI（Federal Bureau of Investigation）「ｴﾌ・ﾋﾞｰ・ｱｲ」 バクロニム 元の略語に対して後から意味や言葉を当てはめたもので、後付けで作られた言葉の意味がある。 SOS（“Save Our Souls” とされるが、元は単に無線信号）「ｴｽ・ｵｰ・ｴｽ」 DAI語★ 日本のタレントが番組で披露してから、主に日本の若い世代に流行。(知らんけど) OYKK (「俺の嫁北川景子」) cf. 英語の略語、アクロニム＆イニシャリズム＋バクロニム おまけ cf. 急に仕事で英語を使うことになった社会人に贈るまとめ(便利ツール/コンテンツ) Qiita\nRef. 英語のコメントや issue で頻出する略語の意味 (FYI, AFAIK, …) Qiita 【変数】英単語の略し方や省略形を検索する【命名】 Qiita 商業略語表（1/3） PDF English Grammar Here “FYI”や“TBA”って？ビジネスシーンに登場する英語の便利な略語 OKRとは？Googleやメルカリも導入する目標管理手法を解説 【初心者向け】1Q・2Q・3Q・4Qとは？四半期決算の発表日の調べ方を解説 やさしい投資家の教科書 【ビジネス用語】FYIやTBDの略語から英語の専門用語までご紹介 Berlits “FYI”や“TBA”って？ビジネスシーンに登場する英語の便利な略語 KotsuKotsu ARCHIVED: What do BTW, FAQ, FYI, IMHO, RTFM, and other acronyms mean? University Information TL;DRの意味を勘違いして使っていたら顰蹙を買ったので気をつけて使おう！ Qiita ","date":1722311335,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1751615720,"objectID":"b70523ac2d0b99aaaabb766c48594c68","permalink":"https://kithub.f5.si/post/english_abbreviations/","publishdate":"2024-07-30T12:48:55+09:00","relpermalink":"/post/english_abbreviations/","section":"post","summary":"🗽 About abbreviations. They also appear in business and professional situations.","tags":["English","GitHub","Slack"],"title":"[English] Abbreviations","type":"post"},{"authors":null,"categories":["Hands-on","Data Reliability Engineer","Data Pipline"],"content":" Why SSIA\nWhat dbt とは What is dbt? dbt\ndbtとは data build tool の略で、データ統合を行う際のプロセスであるELT(抽出, 変換, 格納)のうち Transform(変換) の役割を担うツールです。 Transformのプロセスでは一般的にデータウェアハウスなどに抽出したデータを下流の分析ツールやデータベースで利用できる形式に変換・加工する処理を行います。 dbtはこの工程で役に立つ様々な機能を提供してくれます。 ref\nOfficial https://www.getdbt.com/ https://docs.getdbt.com/ https://github.com/dbt-labs/ Community 🇯🇵 dbt Developer Group(DDG) Tokyo - Japan dbt User Group https://dbt-ug.tokyo/ dbt-tokyoはdbtの日本におけるdbtの普及と、 dbtプロダクトへの貢献を目的に活動をしています。 Production [dbt] └ 1.[dbt-core] \u0026lt;-Free └ 2.[dbt Cloud] └ 1.[Developer] \u0026lt;-Free └ 2.[Team] └ 3.[Enterprise] dbt-core dbt Cloud WebUIを用いて開発ができる！ cf. dbt Pricing Plans dbt Chapter 02 dbt Cloudのセットアップ Zenn Hands On（dbt-core） 詳細なとは参考に乗っているので、この辺を読んで手を動かした部分を掻い摘む。 dbt 入門 Zenn dbtで始めるデータパイプライン構築〜入門から実践〜 Zenn dbtハンズオン Zenn 導入から実行まで [STEP1] ✴️ dbt 導入\n# 作業場用意 $ mkdir -p sandbox/dbt_training | cd dbt_training # venv仮想環境を用意 $ python3 -m venv venv $ source venv/bin/activate # 仮想環境実行 (venv)$ pip install --upgrade pip # pip更新 (venv)$ pip install dbt-postgres # dbtインストール (venv)$ deactivate # 仮想環境停止 $ # ローカル環境に戻る # dbtに必要なものを準備 $ source venv/bin/activate # 再度仮想環境実行 (venv)$ dbt --version # dbt環境が手元にあるか確認 (venv)$ mkdir \\ # 必要なディレクトリを準備 models \\ analysis \\ tests \\ seeds \\ macros \\ snapshots \\ target　(venv)$ touch dbt_project.yml　# dbt設定ファイルを作成（＊１: 後述のYAMLファイル） *1 --- name: \u0026#39;dbt_training\u0026#39; config-version: 2 version: \u0026#39;1.0.0\u0026#39; profile: \u0026#39;dbt_training_dw\u0026#39; model-paths: [\u0026#34;models\u0026#34;] analysis-paths: [\u0026#34;analysis\u0026#34;] test-paths: [\u0026#34;tests\u0026#34;] seed-paths: [\u0026#34;seeds\u0026#34;] macro-paths: [\u0026#34;macros\u0026#34;] snapshot-paths: [\u0026#34;snapshots\u0026#34;] target-path: \u0026#34;target\u0026#34; clean-targets: [target, dbt_packages] models: dbt_training: example: YAMLファイルの中身の詳細説明はここがわかりやすい. # 上位階層に不可視ディレクトリ作成 $ mkdir .dbt | cd .dbt $ touch profiles.yml # DWH接続ファイルを作成（＊２: 後述のYAMLファイル） *2 --- dbt_training_dw: target: dev outputs: dev: type: postgres host: localhost user: admin password: admin port: 5432 dbname: postgres schema: public threads: 1 keepalives_idle: 0 connect_timeout: 10 上記は「PostgreSQL」に接続させる設定ファイル（cf.公式）。 dbt は DWH の接続設定を ~/.dbt/profiles.yml に書く。 ~/.dbt/profiles.yml は各DWH毎にプロファイルを書く。 選定するDWHの種類(PostgreSQL, BigQuery, Snowflake …etc.) 毎にアダプターがあり、プロファイルはアダプターごとに設定の書き方が異なる. [STEP2] 🐘 データベース（PostgreSQL）設定\n# 再び仮装環境内に入ってDB（今回はPostgreSQL）を用意 (venv)$ touch docker-compose.yml | vi docker-compose.yml --- version: \u0026#39;3\u0026#39; services: postgres: image: postgres:latest restart: always ports: - 5432:5432 environment: POSTGRES_USER: admin POSTGRES_PASSWORD: admin volumes: - ./postgres:/var/lib/postgresql/data # Dockerインストールされているか確認 (venv)$　docker --version # gemを新規で導入するときには、まず以下のコマンドを実行 # cf. https://qiita.com/KenAra/items/f1976caa69468323c29d Qiita (venv)$ docker-compose build (venv)$ docker-compose up -d d # Docker起動 (venv)$ docker-compose stop # Docker停止 もしここでDockerでつまいづいたら🐋 コマンド参照 [STEP3] ✴️ dbt 実行\n(venv)$ dbt run - ※モデル作成していないから, 「`WARNING`」出ているが気にせず◎ - 逆に実行後 `logs/` ディレクトリができていることを確認。 モデリング [Step1] 🐘 データソースの準備\n# PostgreSQLログイン (venv)$ psql -U postgres postgres=# # データベース作成 postgres=# \\d # DB一覧確認 postgres=# CREATE DATABASE dbt_training; # # テーブル作成 postgres=# \\cd dbt_training # 作成したDBに移動 dbt_training=# CREATE SCHEMA raw; # スキーマ作成 dbt_training=# \\du # ロール一覧確認 List of schemas　Name | Owner ------------+---------- public | postgres raw | postgres (2 rows) # 各種テーブル作成 dbt_training=# -- 従業員テーブル CREATE TABLE \u0026#34;dbt_training\u0026#34;.\u0026#34;raw\u0026#34;.\u0026#34;employees\u0026#34; ( \u0026#34;employee_id\u0026#34; varchar(256), \u0026#34;first_name\u0026#34; varchar(256), \u0026#34;last_name\u0026#34; varchar(256), \u0026#34;email\u0026#34; varchar(256), \u0026#34;job_id\u0026#34; varchar(256), \u0026#34;loaded_at\u0026#34; timestamp ); dbt_training=# -- お仕事テーブル CREATE TABLE \u0026#34;dbt_training\u0026#34;.\u0026#34;raw\u0026#34;.\u0026#34;jobs\u0026#34; ( \u0026#34;job_id\u0026#34; varchar(256), \u0026#34;job_title\u0026#34; varchar(256), \u0026#34;min_salary\u0026#34; INTEGER, \u0026#34;max_salary\u0026#34; INTEGER, \u0026#34;loaded_at\u0026#34; timestamp ); dbt_training=# \\dt myschema.* # スキーマ内テーブル一覧確認 # データ格納 dbt_training=# -- 従業員テーブルへデータをINSERT INSERT INTO \u0026#34;dbt_training\u0026#34;.\u0026#34;raw\u0026#34;.\u0026#34;employees\u0026#34; VALUES (\u0026#39;101\u0026#39;,\u0026#39;taro\u0026#39;,\u0026#39;yamada\u0026#39;,\u0026#39;yamada@example.com\u0026#39;,\u0026#39;11\u0026#39;,\u0026#39;2022-03-16\u0026#39;), (\u0026#39;102\u0026#39;,\u0026#39;ziro\u0026#39;,\u0026#39;sato\u0026#39;,\u0026#39;satou@example.com\u0026#39;,\u0026#39;11\u0026#39;,\u0026#39;2022-03-16\u0026#39;) ; dbt_training=# -- お仕事テーブルへデータをINSERT INSERT INTO \u0026#34;dbt_training\u0026#34;.\u0026#34;raw\u0026#34;.\u0026#34;jobs\u0026#34; VALUES (\u0026#39;11\u0026#39;,\u0026#39;datascientist\u0026#39;,6000000,12000000,\u0026#39;2022-03-16\u0026#39;), (\u0026#39;12\u0026#39;,\u0026#39;dataengineer\u0026#39;,5000000,10000000,\u0026#39;2022-03-16\u0026#39;) ; # PostgreSQLログアウト dbt_training=# \\q postgres=# \\q cf. 🐘 PostgreSQL(コマンドリファレンス) [Step2] ✴️ dbt モデル作成\n(venv)$ touch models/employee_names.sql select \u0026#34;employee_id\u0026#34; , concat(\u0026#34;first_name\u0026#34;, \u0026#39; \u0026#39;, \u0026#34;last_name\u0026#34;) as full_name from \u0026#34;dbt_training\u0026#34;.\u0026#34;raw\u0026#34;.\u0026#34;employees\u0026#34; cf. SQL models dbt # モデルを作成後に、dbt実行 (venv)$ dbt run …省略… 1 of 1 OK created view model public.employee_names.............................. [CREATE VIEW in 0.08s] 最終的に 1 of 1 OK created view model と表示されればOK。 ココでは、デフォのviewのモデルが作成される。 PostgreSQL の中に employee_names ビューが生成されているはず。 [Step3] ✴️ dbt モデル作成（様々なモデル：マテリアライゼーション編）\nMaterializations dbt\n種類 モデル データ規模 説明 view Viewで作成 - 未指定だとデフォ。高速モデル構築（データ移動が発生しないから）。 でも view \u0026lt; table でクエリが遅い。初期構築に向いている。 table Tableで作成 小規模 実行($ dbt run)の度にデータ入れ直す. その為小規模データに向いている（大規模データだと都度入れ直しはコストや処理時間面で問題）。参照回数の多い、アドホックな分析や集計後のモデリングに適している( view \u0026lt; tablea でクエリ速い為) …","date":1722274255,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1751615720,"objectID":"714d8f52995c28b001f2244e090bdbe5","permalink":"https://kithub.f5.si/post/dbt_seeking_basic_understanding/","publishdate":"2024-07-30T02:30:55+09:00","relpermalink":"/post/dbt_seeking_basic_understanding/","section":"post","summary":"✴️ Wanna deepen my understanding by using my hands and referring to reference books.","tags":["dbt","SQL","YAML","Python","ETL/ELT"],"title":"[dbt] Seeking out basic understanding","type":"post"},{"authors":null,"categories":["Hands-on","Data Analytics Engineer","Google"],"content":" 「データサイエンス100本ノック（構造化データ加工編）」 数年前に遊んだノックを再校兼ねて...⚾️ 当時のものだからクエリがいけてないけど良しなに 一般社団法人データサイエンティスト協会 https://github.com/The-Japan-DataScientist-Society/100knocks-preprocess cf. データサイエンス初学者のための実践的な学習環境 「データサイエンス100本ノック（構造化データ加工編）」をGitHubに無料公開 Digital PR Platform 構築イメージ cf. Digital PR Platform 使用テーブルの確認 select　*　from( select \u0026#39;category\u0026#39; as tb_name , (select count(*) from `project.100knocks.category`) as reco , count(*) as col from `project.100knocks.INFORMATION_SCHEMA.COLUMNS` where table_name = \u0026#39;category\u0026#39; union all select \u0026#39;customer\u0026#39; as tb_name , (select count(*) from `project.100knocks.customer`) as reco , count(*) as col from `project.100knocks.INFORMATION_SCHEMA.COLUMNS` where table_name = \u0026#39;customer\u0026#39; union all select \u0026#39;geocode\u0026#39; as tb_name , (select count(*) from `project.100knocks.geocode`) as reco , count(*) as col from `project.100knocks.INFORMATION_SCHEMA.COLUMNS` where table_name = \u0026#39;geocode\u0026#39; union all select \u0026#39;product\u0026#39; as tb_name , (select count(*) from `project.100knocks.product`) as reco , count(*) as col from `project.100knocks.INFORMATION_SCHEMA.COLUMNS` where table_name = \u0026#39;product\u0026#39; union all select \u0026#39;receipt\u0026#39; as tb_name , (select count(*) from `project.100knocks.receipt`) as reco , count(*) as col from `project.100knocks.INFORMATION_SCHEMA.COLUMNS` where table_name = \u0026#39;receipt\u0026#39; union all select \u0026#39;store\u0026#39; as tb_name , (select count(*) from `project.100knocks.store`) as reco , count(*) as col from `project.100knocks.INFORMATION_SCHEMA.COLUMNS` where table_name = \u0026#39;store\u0026#39; ) order by reco desc ; 100本ノック(SQL編) S-001 ★ レシート明細テーブル(receipt)から全項目を10件抽出し、どのようなデータを保有して いるか目視で確認せよ。\nselect * from `100knocks.receipt` limit 10; S-002 ★ レシート明細のテーブル(receipt)から売上日(sales_ymd)、顧客ID (customer_id)、商品コード(product_cd)、売上金額(amount)の順に列を指定し、 10件表示させよ。\nselect sales_ymd , customer_id , product_cd , amount from `100knocks.receipt` limit 10 ; S-003 ★ レシート明細のテーブル(receipt)から売上日(sales_ymd)、顧客ID (customer_id)、商品コード(product_cd)、売上金額(amount)の順に列を指定し、10件表示させよ。ただし、sales_ymdはsales_dateに項目名を変更しながら抽出するこ と。\nselect sales_ymd as sales_date , customer_id , product_cd , amount from `100knocks.receipt` limit 10 ; S-004 ★ レシート明細のテーブル(receipt)から売上日(sales_ymd)、顧客ID (customer_id)、商品コード(product_cd)、売上金額(amount)の順に列を指定し、 以下の条件を満たすデータを抽出せよ。\n顧客ID（customer_id）が\u0026#34;CS018205000001\u0026#34; select sales_ymd , customer_id , product_cd , amount from `100knocks.receipt` where customer_id = \u0026#39;CS018205000001\u0026#39; ; S-005 ★ レシート明細のテーブル(receipt)から売上日(sales_ymd)、顧客ID (customer_id)、商品コード(product_cd)、売上金額(amount)の順に列を指定し、 以下の条件を満たすデータを抽出せよ。\n顧客ID（customer_id）が\u0026#34;CS018205000001\u0026#34; 売上金額（amount）が1,000以上 select sales_ymd , customer_id , product_cd , amount from `100knocks.receipt` where customer_id = \u0026#39;CS018205000001\u0026#39; and 1000 \u0026lt;= amount ; S-006 ★ レシート明細テーブル(receipt)から売上日(sales_ymd)、顧客ID(customer_id)、 商品コード(product_cd)、売上数量(quantity)、売上金額(amount)の順に列を指定 し、以下の条件を満たすデータを抽出せよ。\n顧客ID（customer_id）が\u0026#34;CS018205000001\u0026#34; 売上金額（amount）が1,000以上または売上数量（quantity）が5以上 select sales_ymd , customer_id , product_cd , quantity , amount from `100knocks.receipt` where customer_id = \u0026#39;CS018205000001\u0026#39; and (1000 \u0026lt;= amount or 5 \u0026lt;= quantity) ; S-007 ★ レシート明細のテーブル(receipt)から売上日(sales_ymd)、顧客ID (customer_id)、商品コード(product_cd)、売上金額(amount)の順に列を指定し、以下の条件を満たすデータを抽出せよ。\n顧客ID（customer_id）が\u0026#34;CS018205000001\u0026#34; 売上金額（amount）が1,000以上2,000以下 select sales_ymd , customer_id , product_cd , amount from `100knocks.receipt` where customer_id = \u0026#39;CS018205000001\u0026#39; and amount between 1000 and 2000 ; S-008 ★ レシート明細テーブル(receipt)から売上日(sales_ymd)、顧客ID(customer_id)、 商品コード(product_cd)、売上金額(amount)の順に列を指定し、以下の条件を満た すデータを抽出せよ。\n顧客ID（customer_id）が\u0026#34;CS018205000001\u0026#34; 商品コード（product_cd）が\u0026#34;P071401019\u0026#34;以外 select sales_ymd , customer_id , product_cd , amount from `100knocks.receipt` where customer_id = \u0026#39;CS018205000001\u0026#39; and product_cd \u0026lt;\u0026gt; \u0026#39;P071401019\u0026#39; ; S-009 ★ 以下の処理において、出力結果を変えずにORをANDに書き換えよ。\nselect * from store where not (prefecture_cd = \u0026#39;13\u0026#39; or floor_area \u0026gt; 900)\nselect * from `100knocks.store` where prefecture_cd \u0026lt;\u0026gt; 13 and floor_area \u0026lt; 900 ; S-010 ★ 店舗テーブル(store)から、店舗コード(store_cd)が\u0026#34;S14\u0026#34;で始まるものだけ全項目抽 出し、10件だけ表示せよ。\nselect * from `100knocks.store` where store_cd like \u0026#39;S14%\u0026#39; -- where regexp_contains(store_cd, r\u0026#39;^S14\u0026#39;) #別解 limit 10 ; S-011 ★ 顧客テーブル(customer)から顧客ID(customer_id)の末尾が1のものだけ全項目抽出 し、10件だけ表示せよ。\nselect * from `100knocks.customer` where regexp_contains(customer_id, r\u0026#39;1$\u0026#39;) limit 10 ; S-012 ★ 店舗テーブル(store)から横浜市の店舗だけ全項目表示せよ。\nselect * from `100knocks.store` where address like \u0026#39;%横浜市%\u0026#39; ; S-013 ★★ 顧客テーブル(customer)から、ステータスコード(status_cd)の先頭がアルファベッ トのA〜Fで始まるデータを全項目抽出し、10件だけ表示せよ。\nselect * from `100knocks.customer` where regexp_contains(customer_id, r\u0026#39;^(A|B|C|D|E|F)\u0026#39;) -- regexp_contains(customer_id, r\u0026#39;^(A-F)\u0026#39;) -- 別解 -- customer_id like \u0026#39;A%\u0026#39; -- or customer_id like \u0026#39;B%\u0026#39; -- or customer_id like \u0026#39;C%\u0026#39; -- or customer_id like \u0026#39;D%\u0026#39; -- or customer_id like \u0026#39;E%\u0026#39; -- or customer_id like \u0026#39;F%\u0026#39; -- 別解 limit 10 ; Cf. BigQueryでLIKE文の複数条件指定をORから正規表現に直す Qiita\nS-014 ★★ 顧客テーブル(customer)から、ステータスコード(status_cd)の末尾が数字の1〜9で 終わるデータを全項目抽出し、10件だけ表示せよ。\nselect * from `100knocks.customer` where regexp_contains(status_cd, …","date":1721935418,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1751615720,"objectID":"653721b897ba0218675e911dc00ad03e","permalink":"https://kithub.f5.si/post/bigquery_structured_data_processing_100/","publishdate":"2024-07-26T04:23:38+09:00","relpermalink":"/post/bigquery_structured_data_processing_100/","section":"post","summary":"🔍 Tried using SQL to learn the basics of data science.","tags":["BigQuery","SQL"],"title":"[BigQuery] Structured data processing 100's","type":"post"},{"authors":null,"categories":["Hands-on","Data Reliability Engineer"],"content":" Why SSIA\nWhat Terraform https://developer.hashicorp.com/terraform/ | doc Provider GoogleCloud | hashicorp/terraform-provider-google | doc AWS | hashicorp/terraform-provider-aws | doc Terraformはインフラストラクチャのプロビジョニングと管理を自動化するためのツール。 管理を効率化し、エラーの削減と一貫性の向上に寄与します。\nインフラストラクチャのコード化（Infrastructure as Code, IaC） コード化: インフラストラクチャをコードとして記述できるため、設定が明確で再利用可能。 バージョン管理: Gitなどのバージョン管理システムでインフラのコードを管理でき、変更履歴を追跡できる。 プロバイダの多様性 マルチクラウド対応: AWS、GCP、Azureなど、複数のクラウドプロバイダをサポートしており、異なるプロバイダ間のインフラを一元管理できる。 プラグイン: プラグインを使って、新しいサービスやAPIを簡単に追加できる。 宣言的な構成 宣言的構成: 望む最終状態を宣言するだけで、Terraformがその状態を実現するために必要な手順を自動的に計算して実行。 依存関係の管理 依存関係の自動解決: リソース間の依存関係を自動的に解析し、正しい順序でリソースを作成・削除。 一貫性と再現性 再現可能な環境: 同じコードを使えば、異なる環境でも一貫したインフラを構築できる。 DRY原則: 一度記述したインフラ構成を複数の環境で再利用することができる。 プラン機能 事前確認: terraform plan コマンドを使用して、変更がインフラに与える影響を事前に確認できるため、意図しない変更を防ぐことができる。 コミュニティとエコシステム 豊富なリソース: 豊富なドキュメントと活発なコミュニティがあり、問題解決やベストプラクティスの学習が容易。 モジュール: 既成のモジュールを利用して、インフラの構築を迅速化。 ごちゃごちゃ書いてあるけど小学生にも分かるようにChatGPTﾆｷに要約依頼… ChatGPTﾆｷ もちろん、小学生でも分かりやすいように簡単に説明しますね！ ### Terraformの便利なポイント 1. おもちゃの説明書みたい - インフラの設計図を作れるから、何をどう作るか一目でわかるよ。 2. いろんなお店で使える - AWSとかGCPとかAzureっていうお店（クラウドサービス）で使えるよ。どこのお店でも同じ道具（Terraform）が使えるんだ。 3. 最終形を決めるだけでOK - 何を作りたいか（ゴール）を言うだけで、Terraformがどうやって作るか考えてくれるよ。 4. 作る順番を守ってくれる - 家を作る時に壁を先に作らずに土台から作るみたいに、ちゃんと順番を考えて作ってくれるよ。 5. 何度でも同じものが作れる - 1回作った設計図を使えば、同じものが何度でも作れるよ。 6. 何が変わるか教えてくれる - 実際に作る前に、何がどう変わるか教えてくれるから安心だよ。 7. みんなで使える - たくさんの人が使っていて、困った時に助けてくれる情報がいっぱいあるよ。 このように、Terraformはインフラを作るのをすごく簡単にしてくれる道具なんだ。 Hands On 記事参考に自分でも手を動かしてみた。 詳細はREFにまとめてある。 Terraform × GKE (k8s) cf. 【GKE】GKE × Terraformで基本的なk8s環境を構築する KIYONO Engineer Blog Terraform × Docker Compose cf. Docker ComposeでTerraformを使用しGoogle Cloud開発環境を構築 KIYONO Engineer Blog Terraform × Cloud Build cf. Cloud BuildでTerraform実行を自動化する G-gen Ref. 【初学者向け】Terraformの基本 Qiita イチから学べるTerraform～基礎から実践を紹介するウェビナー動画を無料公開～ LAC WATCH ","date":1721932924,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1751615720,"objectID":"a4074a97f4fd7d13930ee9a090e8065f","permalink":"https://kithub.f5.si/post/terraform_trying_to_build_the_environment/","publishdate":"2024-07-26T03:42:04+09:00","relpermalink":"/post/terraform_trying_to_build_the_environment/","section":"post","summary":"✝️ I wll try setting up an environment to gain an initial understanding of IaC (Infrastructure as Code) provisioning.","tags":["Terraform","IaC","GKE","Kubernetes","Docker"],"title":"[Terraform] Trying to build the environment","type":"post"},{"authors":null,"categories":["Know-how"],"content":" Why SSIA\nWhat [GitHub] Identicon GitHub からの最初の贈り物 (…emoi XD) eg. https://github.com/octocat ie. 自分のIdenticonにアクセス https://github.com/identicons/\u0026lt;username\u0026gt;.png cf. GitHubのIdenticonの生成と取得 Qiita Identicons! GitHub Blog Identicon Wikipedia [GitHub] Octocat https://github.com/octocat https://myoctocat.com/ cf. オリジナルOctocatを作ろう！ [GitHub] emoji https://github-emoji-picker.rickstaa.dev/ eg. :octocat: :atom: :electron: https://emojipedia.org/github https://emojis.sh/ cf.\nEmojis Gist [GitHub] Dummy account $ git commit をするとコミットログに自分のアドレスが見れてしまう… 秘匿性の高いアドレスに変更することができる -\u0026gt; USERNAME@users.noreply.github.com cf. コミットメールアドレスを設定する GitHub Doc GitHub でダミーのメールアドレスを使用する Qiita 【Git】メールアドレスをnoreplyに設定する Qiita [GitHub] Achievements cf.\nGitHub Achievements Cheatsheet https://github.com/drknzz/GitHub-Achievements How to earn every GitHub Achievement (complete list) Reddit GitHubのAchievementsのQuickdrawの取得条件を調べてみた。ついでにAchievements一覧も Qiita [Powerful README] 色々おしゃれにしちゃおう。\n[Offline Event] cf.\nGitHub Universe GitHub GitHub Universe 2023 現地会場!! GitHub カンファレンスレポート at サンフランシスコ Qiitai 【週刊オルターブース】『GitHub Universe 2023』現地参加のブログ祭り編 週間オルターブース [Festival] 直接的ではないが、GitHubに係るその他のお祭り cf.\nHacktoberfest プログラマのためのお祭り、”ハックトーバーフェスト”が開催中！ Medium [TextEditor/IDE] 駆け出しエンジニアの時にお世話になった製品 m(_)m Atom (GitHub社製 2023/5/4: ｻ終) https://atom-editor.cc/ Community https://atom-community.github.io/ https://github.com/atom-community cf. Atom (text editor) Wikipedia GitHub 製エディタ Atom リファレンス Qiita Atomエディタが開発終了へ！ 他のエディタに移行するならどれがオススメか？ Qbook Pulsar (融資が開発したAtom後継機) https://pulsar-edit.dev/ Package https://github.com/pulsar-edit/ppm Install $ brew install --cask pulsar cf. Atom後継のPulsarを使う Zenn Codespaces (GitHub Online IDE?) Secure development made simple IDE https://github.com/codespaces cf. The tools you need to build what you want. GitHub [Dangit, Git!?!（処方箋）] https://dangitgit.com/ja https://x.com/jalva_dev/status/1821742876080124039?s=46\u0026amp;t=RSRnjQg5szbGQ19iUHwMlg [🧾レシート] https://gitreceipt.vercel.app/\n[Uithub] URL置換だけでドキュメント自動生成 https://uithub.com/ https://uithub.com/openapi.html e.g. ｺﾚを https://github.com/gcpug/nouhau ｺｳ https://uithub.com/gcpug/nouhau [DeepWiki] URL置換だけで公開リポジトリを対話型自動Wikiを生成 Devin社\nhttps://deepwiki.org/\ne.g.\nｺﾚを https://github.com/gcpug/nouhau ｺｳ https://deepwiki.com/gcpug/nouhau 個人開発でも同名の似たサービスがある\ncf. 話題のDeepWikiをセルフホスト？Open DeepWikiを試してみた Zenn [🧜‍♀️ Mermaid] README.md で図示したいときに！\nOfficial https://mermaid.js.org/ Editor https://mermaid.live/edit cf.\nMermaidでAWSアーキテクチャ図を描いてみた！ Zenn mermaidでフローチャートを描く Zenn GitHub、Markdown構文でフローチャートやクラス図、ガントチャートなどのダイアグラムを表示できる「Mermaid」をサポート開始 [GitHub Markdown] https://www.markdownguide.org/\n\u0026gt; [!NOTE] \u0026gt; Highlights information that users should take into account, even when skimming. \u0026gt; [!TIP] \u0026gt; Optional information to help a user be more successful. \u0026gt; [!IMPORTANT] \u0026gt; Crucial information necessary for users to succeed. \u0026gt; [!WARNING] \u0026gt; Critical content demanding immediate user attention due to potential risks. \u0026gt; [!CAUTION] \u0026gt; Negative potential consequences of an action. cf.\nGitHub 上での執筆とフォーマットについて GitHub Doc [Markdown] An option to highlight a “Note” and “Warning” using blockquote (Beta) #16925 GitHub Repo GitHub.com の Markdown でアラート記法が増えた話 Qiita [Gist] コードスニペット的な cf.\n【入門】Github Gistとは？概要や基本的な使い方をわかりやすく解説 gistリポジトリに一発で移動したい Zenn [GitHub] アカウントお引越し cf.\nGitHubのアカウントを統合する 複数のアカウントの管理 複数の個人アカウントのマージ [GitHub] セキュリティ cf.\nGitHub の削除されたリポジトリや非公開のリポジトリに誰でもアクセスできるの？ Zenn [GitHub] Marketplace Marketplaceとは : GitHub Marketplaceは、プロジェクト管理からコードレビューまで、開発プロセス全般で使えるツールを販売するウェブサイト。 cf. https://japan.cnet.com/article/35101578/\ncf. Marketplace アプリの GitHub Marketplace について 自作GitHub ActionsをGitHub Marketplaceに公開する手順 Qiita [GitHub]　GitDiagram https://gitdiagram.com/ eg.\nhttps://github.com/skills/hello-github-actions https://gitdiagram.com/skills/hello-github-actions ","date":1721801293,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1751615720,"objectID":"8041b52974a18b052075034402489b71","permalink":"https://kithub.f5.si/post/github_this_and_that/","publishdate":"2024-07-24T15:08:13+09:00","relpermalink":"/post/github_this_and_that/","section":"post","summary":"📍 Various things to make your GitHub life more enjoyable.","tags":["GitHub"],"title":"[GitHub] This \u0026 That","type":"post"},{"authors":null,"categories":[""],"content":" Why SSIA\nWhat [Slack] マークアップ ChatGPTﾆｷ曰く\nSlack のメッセージ書式は、Markdown と似ていますが、独自の仕様を持っています。\nまた、GFM（GitHub Flavored Markdown）とは異なります。GFM は GitHub が使用する Markdown 拡張であり、標準的な Markdown に幾つかの追加機能があります。\nしたがって Slack の書式設定は独自のものであり、Markdown や GFM とは異なります。しかし、基本的なアイデア（太字、斜体、引用、コードブロックなど）は共通しています。Markdown や GFM で可能な高度な機能（例えば、表やチェックリストなど）は Slack ではサポートされていません。\n書式一覧 Slack（Markdown） 用途 *太字* _イタリック_ ~取り消し~ \u0026gt; 引用 \u0026gt;\u0026gt;\u0026gt; 引用ブロック \u0026#39;コード\u0026#39; ※[`]バッククオート ```コードブロック``` 1. 手順 - リスト + リスト * リスト ※ネストはtab :emoji: e.g. :bow: [リンク](url) @username メンション指定 #channelname チェンネル指定 cf.\nテキストコミュニケーションで心掛けること Slack Slack の書式設定でメッセージをもっと伝わりやすく Slack Slack のメッセージ記法と Markdown を比較してみる Qiita Slackの文章が劇的に綺麗になる! markdown記法を利用しよう samurai [Slack] 内部検索 検索コマンド 目的 from:@人物名 in:#チャンネル名\nin:@人物名 on:日付 before:日付 after:日付 during:日付 has:pin, has:link, has:絵文字コード is:saved ブックマーク内検索 ”キーワード” -キーワード 除外 cf.\nSlack 内で検索する Slack どこよりも分かりやすい、slack内メッセージ検索のやり方 KOHIMOTOLABO Slack上の膨大な情報から目当てのものを見つけ出す方法 ASCII [Slack] 絵文字 cf.\nSlackmojis 絵文字 ジェネレーター MEGAMOJI カスタム絵文字をサクッと作れる🐱 [Slack] Slack API Block Kit\ncf.\nSlack で UI を構築するためのフレームワーク「Block Kit」の基本を確認してみた classmethod [Slack] 外部機能 Google Chrome でBrowser使用もできる。 その場合Chrome Webstore で Extensions を使ってカスタマイズもできる。\ncf.\nSlackUtils Slack Channels Grouping テクニカルライティング Slack に限った話ではないが、ドキュメンとコミュニケーションをする上で知っておくとお得なお作法 cf.\nGoogle社のテクニカルライティングの基礎教育資料がとても良かったので紹介したい Qiita 想定ケース\nSlack等のコミュニケーションツールで Notion等の社内ナレッジで GitHubでのREADME.mdで プログラミング時のコメント残しの時 LLM用のプロンプト構成、作成で …etc. ","date":1721726237,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1751615720,"objectID":"b2ed8385d3df92ad21a5c3dd39742cd4","permalink":"https://kithub.f5.si/post/slack_writing_etiquette/","publishdate":"2024-07-23T18:17:17+09:00","relpermalink":"/post/slack_writing_etiquette/","section":"post","summary":"🔔 Learn writing etiquette using Slack's unique notation.","tags":["Slack","Markdown","GFM"],"title":"[Slack] Writing etiquette","type":"post"},{"authors":null,"categories":["Know-how"],"content":" Why 個人や、グループ開発で様々なシーン 差分確認 の場面がある。 その中で、 いち早く問題特定 ができるようになりたい。 バグ修正時やペアプログラミングの際、はたまたコードレビュー時に鋭くかつ適切に理解ができる効果を期待したいため。\nHow [App] SourceGraph https://github.com/sourcegraph\nいち早く問題特定するためには、何といってもコード理解が必須である。 いちいち差分を手作業で見たり、そもそも修正箇所を特定してから差分比較するというときにはやはり、リポジトリ内での検索力が問われる。\nそこで、「SourceGraph」なるツールがあることを知った。 ref\ncf. Global Code Time Report Software [WebUI] GitHub Code Search (β版) https://github.com/features/code-search/ Try it now https://github.com/search?type=code\u0026amp;auto_enroll=true Watch video https://www.youtube.com/watch?v=ujVY8xqkflQ 検索窓に「repo:{your/repository/name} {your/file.name}」 会社の凄い人は、こんな風に検索をかけて調べていた。 ※最近では専らコレばかり使用している。\n[WebUI] URL URLを加工してBrowser画面上で比較（※便宜的にURLノード別に改行している）\nhttps://github.com /[ユーザー（組織）名] /[リポジトリ名] /compare /[Old]...[New] コミット間 GitHubのリポジトリ頁へ 「🕐 Commits」ボタンをクリック コミット一覧より比較したい世代のコミットIDを選択 https://github.com/[ユーザー（組織）名]/[リポジトリ名]/compare/コミットハッシュ値old...コミットハッシュ値new e.g. https://github.com/numpy/numpy/compare/b01a473...4d29079 ブランチ間 リポジトリのトップ頁から「Compare」ボタンをクリック ブランチの比較頁へ遷移 e.g. https://github.com/numpy/numpy/compare/master...maintenance/1.1.x 日付間 n日前 e.g. https://github.com/numpy/numpy/compare/master@{7.day.ago}...master 指定日 e.g. https://github.com/numpy/numpy/compare/master@{2020-12-01}...master [WebUI] Link Share GitHubのWebUI上で、コードやページに対して共有リンクを発行することができる。\n修正したい箇所や共有したい箇所に対して、右クリックで発行してシェアすることができる。\n[CLI] $ git diff $ git diff コミット間 今回コミットと直前 $ git diff HEAD^ 指定コミット同士 $ git diff 変更前のSHA..変更後のSHA ブランチ間 指定ブランチ間 $ git diff ブランチA..ブランチB [CLI] $ git grep 第13話 どのコミットでバグが入ったかgit blameで調べてみよう【連載】マンガでわかるGit ～コマンド編～\n[CLI] $ git blame コードを書いたのが誰かわかる超便利コマンド「git blame」の使い方を解説\n[CLI] shell # GitHub CLI や Git コマンドで対象レポをローカルにクローン $ gh clone https://github.com/username/repository.git | cd repository $ git clone https://github.com/username/repository.git | cd repository $ grep -r \u0026#34;特定の文字列\u0026#34; . Ref. コミットを比較する GitHub Doc GitHubの差分確認方法(ブランチ、コミット、日付) Qiita GitHub – コミット間・ブランチ間の差分(Diff)の確認方法・見方 Howpon 忘れやすい人のための git diff チートシート Qiita gitのdiff, status, logを極限までコンパクト化＋便利化する Qiita ソースコードgrep地獄を助けるSourceGraph Qiita GitHubのプルリクエストで差分が更新されない時 Qiita gitでmergeしても差分が反映されない Stackoverflow 実務未経験者の人に読んでほしいGitHubの実務tips Qiita ","date":1721675601,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1751615720,"objectID":"76b002aadcb652cf215443af13cdad72","permalink":"https://kithub.f5.si/post/github_how_to_check_difference/","publishdate":"2024-07-23T04:13:21+09:00","relpermalink":"/post/github_how_to_check_difference/","section":"post","summary":"📍 Methods and tools to capture code fixes faster and more accurately.","tags":["GitHub","Git","SourceGraph"],"title":"[GitHub] How to check difference code","type":"post"},{"authors":null,"categories":["Data Analytics Engineer","Know-how","Google"],"content":" Why SSIA\nWhat || CTEの定義 文脈によって若干の違いがあるのものの `CTE` と `CTEs` は基本的に同じ概念を指している（細かいｶﾖ!）。 CTE（Common Table Expression） 定義: 一時的に名前を付けて結果セットを定義し、それを後続のクエリで参照するSQL構文。 単一のCTEを使用する場合、単数形のCTEという用語が使用される。 -- 単一のCTE with SingleCTE as ( select column1, column2 from `dataset.table` where condition ) select column1, count(*) as count from SingleCTE group by column1 ; CTEs（Common Table Expressions） 定義: 複数のCTEを組み合わせて使用する場合、それらを総称してCTEsと言う。 -- 複数のCTE（CTEs） with FirstCTE as ( select column1, column2 from `dataset.table` where condition1 ) , SecondCTE as ( select column1, sum(column3) as sum_column3 from FirstCTE where condition2 group by column1 ) , ThirdCTE as ( select column1, sum_column3 from SecondCTE where condition3 ) select column1, count(*) as count from ThirdCTE group by column1 ; || DataAnalyticsにおけるCTEs ※ 一般論ではなく、業務を通じて個人的に心掛けたことをまとめる。 | 避けたい記述 CTEs書くと記述がばらつく問題がある… 「十人十色、多様性の時代ではあるが記述を統一したいよなぁ〜」と悩み出す。 個人的に気を遣って書いている事をまとめてみる。\n「072ｸｴﾘ」は避けたい 意味：自分だけしか解読できないようなクエリのこと（属人化の危惧） 対処： 誰が読んでも分かるような記述を心掛ける。(認知コストを奪うことはしたくない意図) eg. サブクエリのネストは２段まで…etc. 「毒親ｸｴﾘ」は避けたい 意味：過保護すぎるコメント記述でゴチャつくクエリのこと 対処： クエリを読んで理解できる自明に対しては説明を控える。 一方、where pref_flag = \u0026#39;3\u0026#39; --3:福岡県のようなクエリに対してはコメントする。 | 集計・分析の為の記述構成 SQLを用いての集計・分析を行うと、クエリだけで軽く数百行を超えることがしばしばある。 おおまかに、どのエリアにどのテーブル（整形したサブクエリ）が存在しているのかを予め決めておくことで、 容易にクエリを記述・読み返すことができる。\n#standardSQL /* (memo) * e.g. 分析依頼内容や、依頼者の要望クエリを要約しておく。 */ /* UDF */ --チームで共有するUDFはココに書いておく。 /* PERIOD */ --チャンク/パーティショニング（eg. 期間 ...etc.) declear FROM TO_ datetime; set STARAT_ = ; set END_ = ; with /* IMPORT */ HOGE_RAW as (select from \u0026#39;prj.ds.hoge_*\u0026#39; where _table_suffix between STARAT_ and END_) , FUGA_RAW as (select from \u0026#39;prj.ds.fuga_*\u0026#39; where _table_suffix \u0026lt;= END_) , … /* PREPROCESS */ , HOGE_PREP as (select * from HOGE_RAW) , FUGA_PREP as (select * from FUGA_RAW where fuga_key in (1, 2, 3, 4)) , … /* AGGREGATION */ , HOGE_CALC as ( select * , count(distinct hoge) as CNT_HOGE from HOGE_PREP group by 1 ) , HOGE_FUGA_MARGE as ( select * from HOGE_CALC left join FUGA_PREP using(ID) ) , … /* REPORT */ , REPORT as (select * from HOGE_FUGA_MARGE) /* EDA */ , EDA as (select \u0026#39;-- eda --\u0026#39; # 探索的データ分析 ) /* OUTPUT */ select * REPORT ; | アウトラインの「/**/」の説明 必須 エリア名 エリア用途 説明 ＊ PERIOD 集計区間/期間指定 declareや、set等のBigQuery scrptingを用いて、定義するとより汎用的 ＊ IMPORT 使用テーブル 使用するテーブルをまとめておく(Pythonの import を意識) ＊ PREPROCESS 前処理（準備） 分析に使用する特徴量（カラム）を指定や（カラムの）名寄せ等の準備 ＊ AGGREGATION 前処理（分析） 準備したテーブル通しを結合したり、集計分析関数（ウィンドウ関数）等を用いて実に分析 ＊ REPORT 提出用整形 分析を終えたデータを、クライアントの要望や意図に合うように整形する EDA 探索的データ分析 上述のサブクエリ等を組み上げるうえでのロジック確認したり、アドホックの補填用に記述 ＊ OUTPUT 最終出力 select {column} from REPORT;で受け取れるようにしておく | コメントアウトの振る舞い コメント記述 用途 用途（行数別） 備考 /* コメント */ アウトライン記述時 複数行 - -- コメント 行の説明時 単一行 - # コメント 行の説明時 単一行（or複数行） Python記述で慣れている方がチームに居るならコッチの方が優しいかも ※ クエリ文を読んで、自明に対してはコメントしない（何でもかんでもコメントは入れなくて◎） ※ コメントする場合は、 『意図』 を記述する（どうしてこうしたのか？） | 特徴量（カラム）の振る舞い DQLの SELECT句 でデータ抽出する際は、極力「 ＊ （=wild card）」を避ける。\n#standardSQL /* NG */ --会員データ MEMBER as ( select * from `pj.ds.table` ) #standardSQL /* OK */ --会員データ MEMBER as ( select member_id , gender , age , pref , created_at from `pj.ds.table` ) ※ 手間ではあるが、使用カラムを一つ一つ記述 ※ この書き方をしていると、後々に皆幸せになれる（騙されたと思って記述） Ref. CTEs(Common Table Expressions)について学び直す Zenn\nサブクエリと CTE の作成 GoogleCloudSkillBoost\nBigQuery 集計・分析の為の記述構成\n再帰 CTE を使用する GoogleCloud\nBigQuery で WITH RECURSIVE 句が GA になりました！ Zenn\ndbtでのSQLモデル記述時に利用推奨されている『共通テーブル式(CTE/Common Table Expression)』について DevelopersIO\n","date":1721459622,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1751615720,"objectID":"db37d153d6bac169f9e2cc85a46191ab","permalink":"https://kithub.f5.si/post/bigquery_data_analysis_on_cte/","publishdate":"2024-07-20T16:13:42+09:00","relpermalink":"/post/bigquery_data_analysis_on_cte/","section":"post","summary":"🔍 Things to keep in mind when analyzing data using SQL.","tags":["BigQuery","SQL"],"title":"[BigQuery] CTEs in data analyisis","type":"post"},{"authors":null,"categories":["Know-how"],"content":" Why SSIA\nWhat Issues とは GitHubのIssues は、様々なシーンで使用される便利なツール。\nプロジェクトのバグ報告 機能リクエスト タスクのトラッキング ディスカッション 正しい使い方を理解することで、プロジェクトの管理やコラボレーションがスムーズになる。 共同開発と個人開発でのユースケースを例を入れてメモする。 共同開発でのIssuesの使い方 Issuesの作成： タイトル: 問題やリクエストの要点を簡潔に表現する。 ラベル: Issueの種類（バグ、機能リクエスト、質問など）や優先度を示すためにラベルを追加する。 アサイン: 担当者を指定する。 説明: 問題の詳細、再現手順、期待される結果、スクリーンショットなどを含める。 例1：バグ報告 タイトル: “アプリが特定の操作でクラッシュする” ラベル: bug, high priority アサイン: @developer1 説明: ### 概要 アプリがユーザーが「設定」メニューを開くとクラッシュします。 ### 再現手順 1. アプリを開く 2. 「設定」メニューをクリックする 3. アプリがクラッシュする ### 期待される結果 設定メニューが正常に表示されること。 ### 実際の結果 アプリがクラッシュし、エラーメッセージが表示される。 ### 環境 - OS: macOS 11.4 - アプリバージョン: 1.0.0 - デバイス: MacBook Pro 2020 例2：機能リクエスト タイトル: “ダークモードのサポートを追加する” ラベル: enhancement, feature request アサイン: @designer1, @developer2 説明: ### 概要 アプリにダークモードを追加することで、夜間や暗い環境での使用時の目の負担を軽減します。 ### 詳細 - ユーザーが「設定」からダークモードを選択できるようにする。 - ダークモードを選択すると、アプリの全てのUIが暗いテーマに変更される。 ### 利点 - 目の負担が軽減される。 - ユーザーエクスペリエンスが向上する。 Issuesの管理： ステータス更新： Issueが進行中の場合は、「In Progress」などのラベルを追加。 進捗状況をコメントで共有。 関連Issueのリンク： 関連するIssueやPull Requestをリンクして、情報を統合。 e.g.) Related to #123（Issue番号123に関連） クローズ： 問題が解決したら、Issueをクローズ。 e.g.) Fixed by #456（Pull Request番号456で解決） 具体例の流れ Issueの作成： ユーザーがアプリのクラッシュを報告するためにIssueを作成。 コメントでのディスカッション： 開発者が再現手順や追加情報を確認するためにコメントを追加。 修正作業： 開発者が修正に取り組み、進捗をコメントで報告。 @developer1が修正完了後にプルリクエスト（PR）を作成。 レビューとマージ： 他の開発者がPRをレビューし、承認。 PRがマージされ、Issueがクローズされる。 まとめ GitHubのIssuesを正しく使うことで、プロジェクトの管理が効率化され、チームメンバー間のコミュニケーションが向上。 明確なタイトル と 詳細な説明 、 適切なラベル とアサインを行うことで、誰が何をするべきかが明確になり、プロジェクトの進行がスムーズになる。 個人開発でのIssuesの使い方 タスク管理： 自分のやるべきタスクをIssueとして登録することで、To-Doリストとして機能する。 各タスクの進捗状況や詳細を追跡できる。 e.g.) タイトル: “Vimの設定を見直す” ラベル: enhancement, task 説明: ### 概要 現在のVim設定を見直し、より効率的な設定に変更する。 ### タスク - プラグインの整理 - キーバインドの見直し - パフォーマンスの向上 ### 期日 2024-08-01 アイデアの記録： 新しい機能や改善点のアイデアを忘れないようにIssueに記録する。 時間があるときに取り組むべきアイデアのリストとして活用する。 e.g.) タイトル: “zshのテーマをカスタマイズする” ラベル: idea, enhancement 説明: ### 概要 zshのテーマを自分好みにカスタマイズすることで、ターミナルの見た目を改善する。 ### アイデア - プロンプトにGitステータスを表示 - 配色を変更 - カスタムプロンプトを追加 バグの追跡： 自分で発見したバグや問題をIssueに記録し、後で修正するためのリマインダーとして使う。 e.g.) タイトル: “dotfilesのシンボリックリンクが正しく作成されない” ラベル: bug, urgent 説明: ### 概要 新しい環境にdotfilesをデプロイする際、シンボリックリンクが正しく作成されない問題が発生。 ### 再現手順 1. リポジトリをクローン 2. セットアップスクリプトを実行 3. シンボリックリンクが正しい場所に作成されていない ### 期待される結果 シンボリックリンクが正しい場所に作成されること。 ### 実際の結果 シンボリックリンクが間違った場所に作成される。 まとめ 個人開発でのIssueの利用は、プロジェクトの進捗管理やアイデアの整理、問題の追跡に非常に役立つ。 定期的にIssuesを見直すことで、自分の作業の進捗や次に取り組むべきタスクが明確になり、効率的な開発が可能になる。 個人プロジェクトであっても、気にする御作法は共同開発の場合と同様で、明確にしてくことを意識すると良い。 Ref. GitHub Issues のドキュメント GitHub Docs Issueについて GitHub Docs 【Github初心者向け】issueの使い方をマスターしよう Zenn githubでissueのテンプレートを作成する方法 Qiita チーム開発を変える「GitHub」とは？〜Issuesの使い方〜【連載第3回】 SELECK GitHubのissueを活用した個人アプリの開発手順を書いてみた Qiita GitHubのIssueでは、Milestoneが役に立つという話 excite GitHubのissueについて Zenn ","date":1721295001,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1751615720,"objectID":"1705664b3410586784a7dbce478db046","permalink":"https://kithub.f5.si/post/github_issues_how_to_use/","publishdate":"2024-07-18T18:30:01+09:00","relpermalink":"/post/github_issues_how_to_use/","section":"post","summary":"📍 How to effectively use 'Issues' in GitHub repository.","tags":["GitHub"],"title":"[GitHub] How to use issues","type":"post"}]